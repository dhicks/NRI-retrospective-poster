"AwardNumber","Title","NSFOrganization","Program(s)","StartDate","LastAmendmentDate","PrincipalInvestigator","State","Organization","AwardInstrument","ProgramManager","EndDate","AwardedAmountToDate","Co-PIName(s)","PIEmailAddress","OrganizationStreet","OrganizationCity","OrganizationState","OrganizationZip","OrganizationPhone","NSFDirectorate","ProgramElementCode(s)","ProgramReferenceCode(s)","ARRAAmount","Abstract"
"1317978","NRI: Small: Reducing Trunk Musculoskeletal Forces During Manual Work","IIS","National Robotics Initiative","09/01/2013","05/18/2015","Homayoon Kazerooni","CA","University of California-Berkeley","Standard Grant","Jordan Berg","08/31/2016","$1,017,136.00","David Rempel","kazerooni@berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","8013","116E, 7923, 8086, 9178, 9231, 9251","$0.00","The research objective of this award is to develop a set of advanced engineering principles for design of the exoskeleton systems that reduce the internal musculoskeletal forces and oxygen consumption in a person during a variety of laborious tasks in various workplaces. The hypothesis is that these systems will decrease the severity and number of work-related back injuries while enhancing worker safety in automobile assembly plants and distribution centers. The research approach progresses from the exploration of the human-machine interaction when both the flow of power and information between the device and human dictate the overall functionality and performance of the device. Employing the dynamic models of various elements in this human-machine interaction, passive impedances and actuators will be developed for the exoskeletons. A set of experiments will be conducted to quantify the effect of the exoskeleton system on human trunk in sagittal plane, in frontal plane, and rotation around the spine. <br/><br/>If successful, the technologies proposed here will manifest in the development of broad classes of exoskeleton devices for workers who repeatedly move objects in factories, warehouses and distribution centers. This project will yield a set of engineering principles that decrease the risk of injuries due to repetitive maneuvers in distribution centers and in auto assembly plants. This project further will increase the availability of affordable assist systems for workers and improve the quality of life for workers. The educational impact of this research is derived from the proposed concerted effort to integrate education with research. Graduate and undergraduate engineering students will benefit through involvement in the research. The prototypes developed in this research project will enrich educational platforms for the study of design, modeling, and control of assist devices interacting with humans."
"1317744","NRI: Small: Integrated modeling and manufacturing framework for soft fluidic robotics","IIS","National Robotics Initiative","09/15/2013","09/08/2013","Conor Walsh","MA","Harvard University","Standard Grant","Jordan Berg","08/31/2017","$750,088.00","George Whitesides","walsh@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385366","6174955501","CSE","8013","7923, 8086","$0.00","The research objective of this award is to leverage recent advances in 3D-printing technology and in the robotics communities: open-source tools to enable rapid, on-site design and manufacture of soft, human-friendly robots. These soft robots can be customized to the geometry of a particular task by creating 3D molds from the object geometry where embedded chambers for fluid can be molded into the elastomer. Actuation of the devices can be achieved by pressurizing the chambers, leading to induced strain in certain parts of the elastomeric material. The research will enable 3D simulations of soft robot designs to be performed via a finite element approach that takes into account the complex geometry and non-linear material properties. Deliverables include a catalog of design and fabrication rules for soft robots, guidelines for simulating fluid-structure interaction with finite element modeling, demonstration and validation via hardware, documentation of research results, engineering student education, and the development of an open-source platform for sharing the research with the broader educational and research communities. <br/><br/>If successful, the results of this research will provide an opportunity to create custom soft robots that are inherently low-cost and safe for interacting with humans and fragile objects. This will lead the way for a transition from conventional machines to a new generation of versatile, multifunctional soft systems customized for specification applications where safe interaction with humans and the environment is critical. Example applications include soft robotic braces that can be customized for patients for physical rehabilitation and soft robotic grippers for that can be customized to particular objects on manufacturing assembly lines. For dissemination, an open-source platform will be developed that will consist of 3D CAD models, simulation files, fabrication instructions that can all be adapted for other applications. This information will enable high-school students, undergraduate design teams, and academic and industrial researchers to create their own soft robots."
"1355716","NRI: Small: Rapid exploration of robotic ankle exoskeleton control strategies","IIS","National Robotics Initiative","09/15/2013","09/11/2013","Steve Collins","PA","Carnegie-Mellon University","Standard Grant","Jordan Berg","08/31/2018","$800,001.00","","stevecollins@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","7923, 8086","$0.00","This project compares different techniques for assisting individuals with stroke-related mobility impairments using a robotic ankle orthosis. Several promising assistance techniques have been developed for robotic prostheses and rehabilitation platforms, which have been extended to exoskeletons - worn at the ankle joint, and adapted for individuals with stroke. An ankle exoskeleton test bed is used to emulate each assistance technique, allowing comparisons of efficiency and effectiveness within the same platform. Each technique is first programmed and verified in pilot tests with this emulator, followed by multi-dimensional parameter studies, conducted first on subjects without neurological impairment and then on subjects with hemiparesis following stroke. The results for each technique are used to identify ideal parameters and their settings, which facilitates across-technique comparisons and development of a standardized set of quantitative performance metrics, including measures of effort, preferred speed, and stability. These studies will contribute to a scientific foundation for the design and prescription of robotic ankle-foot orthoses that will benefit the millions of impaired individuals."
"1344222","INSPIRE Track 1: Programming Digital Materials: Additive Assembly of Integrated Electronics","CMMI","SPECIAL STUDIES AND ANALYSES, SPECIAL PROJECTS - CISE, IIS SPECIAL PROJECTS, ENG INTERDISC RES (IDR), National Robotics Initiative, INSPIRE","01/01/2014","06/30/2014","Neil Gershenfeld","MA","Massachusetts Institute of Technology","Continuing grant","Bruce M. Kramer","12/31/2016","$798,977.00","","neil.gershenfeld@cba.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","ENG","1385, 1714, 7484, 7951, 8013, 8078","024E, 067E, 082E, 152E, 7951, 8086, 8653","$0.00","This INSPIRE award is partially funded by the Interdisciplinary Research Program in the Division of Civil, Mechanical and Manufacturing Innovation in the Directorate for Engineering, the Computer systems Research Program in the Division of Computer and Network Systems in the Directorate for Computer and Information Science and Engineering, and the Robust Intelligence Program in the Division of Information and Intelligent Systems in the Directorate for Computer and Information Science and Engineering.<br/><br/>Integrated circuits are produced in billion-dollar chip fabs, which require many months of processing to go from a design to a chip. The goal of this proposal is to accomplish that in an afternoon, with a table-top process. Rather than etching or depositing electronic materials, as is done today, it is based on assembling digital materials. These use a discrete set of components, reversibly joined in a discrete set of relative positions and orientations. Those attributes allow positions to be determined by the parts, errors in their placement to be detected and corrected, dissimilar materials to be joined, and them to be disassembled rather than disposed. A conducting and insulating part type will be used to replace multilayer printed circuit boards, connectors and cabling for three-dimensional interconnect, inductors and capacitors, striplines and antennas. A resistive part type will be added for producing passive components, semiconducting part types will be added for active components, and magnetic and flexural part types for electromechanical components.<br/><br/>This project will develop prototypes of the parts, the processes to produce them, the assemblers to place them, and the software tools to design with them. The research will progress in stages of size and complexity, reproducing the history of integrated electronics. First will be the equivalent of small-scale integration, using tens of parts with a 100 micron feature size. A test case at this level of integration will be assembling a radiofrequency matching network. Then will come medium-scale integration, using hundreds of parts with a 10 micron feature size. A goal here will be assembling a ring oscillator and binary counter. Finally, large-scale integration will use thousands of parts with a 1 micron feature size, with a goal of assembling a microprocessor. Computer-controlled manufacturing has progressed from subtractive to additive processes; this research roadmap will introduce the discrete assembly and disassembly of functional digital materials, to code the construction of complete systems in an integrated process."
"1638163","NRI: A Variable Stiffness Artificial Muscle Material for Dexterous Manipulation","CMMI","National Robotics Initiative","09/01/2016","08/10/2016","Qibing Pei","CA","University of California-Los Angeles","Standard Grant","Jordan Berg","08/31/2019","$474,501.00","","qpei@seas.ucla.edu","11000 Kinross Avenue, Suite 211","LOS ANGELES","CA","900952000","3107940102","ENG","8013","8086","$0.00","The goal of this project is creation of a material combining innovative actuation with integrated sensing. The actuation principle is based on a bistable electroactive polymer. This comprises an electrically insulating polymer sheet sandwiched between two flexible conductive layers. The polymer sheet exhibits a phase transition, with a large increase in compliance as temperature exceeds a transition value. In order to actuate the material, it is first heated above transition, then a potential is applied across the polymer. Electrostatic forces pull the conductive layers together, increasing the area of the sheet, possibly by several hundred percent. These high strains, combined with boundary constraints, cause large displacements of the actuator. Then the material is allowed to cool and transition back to the stiff form, locking in the new actuator position and allowing the electric potential to be removed. Thin film sensors measuring pressure or touch are integrated with the actuator sheets. The resulting high-strain, variable-compliance, self-sensing, device is well-suited for robotic manipulators with muscle-like dexterity. Such manipulators would have application to a wide variety of robotic systems, including prosthetic hands, patient rehabilitative equipment, compliant surgical instruments, artificial organs, and humanoid robots for assisted living. This project will provide summer research intern opportunities for minority high school students. Undergraduate and graduate students will also participate in the project, to gain hands-on research experience, and analytical, communication, and inter-personal skills.<br/><br/>This project investigates an artificial muscle material combining variable stiffness, large-strain actuation and sensing, and explores the application of the material for dexterous manipulation that is critically needed in a wide range of robotic applications. It differs from traditional robotics in that the manipulation is object-centered. The objects can have various different shapes, stiffness, surface texture, and weight. Artificial muscles combining sensing, actuation, and variable stiffness are desired to produce dexterous manipulations from gentle touch to firm gripping, with local controllability. Electroactive polymers have shown promise for reproducing both the active and structural properties of human muscles. Among these ""artificial muscle"" materials, dielectric elastomers exhibit low stiffness, high actuation strain and force output. Bistable electroactive polymers have stiffness variable up to 1000 times. In the softened state, the bistable polymer behaves like an elastomer and can be actuated like a dielectric elastomer. The combination of variable stiffness and large strain actuation will enable a new generation of artificial muscles for bioinspired robotic applications. A 6-finger manipulator will be demonstrated to grip and lift a variety of objects including eggs, golf balls, smartphones, and to squeeze a specified length of toothpaste out of the tube."
"1637446","NRI: Vine Robots: Achieving Locomotion and Construction by Growth","CMMI","National Robotics Initiative","08/01/2016","08/10/2016","Allison Okamura","CA","Stanford University","Standard Grant","Jordan Berg","07/31/2019","$1,749,999.00","Jonathan Fan, Sean Follmer","aokamura@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","ENG","8013","8086, 9102","$0.00","In contrast to legged robots inspired by locomotion in animals, this project explores robotic locomotion inspired by plant growth. Specifically, the project creates the foundation for classes of robotic systems that grow in a manner similar to vines. Within its accessible region, a vine robot provides not only sensing, but also a physical conduit -- such as a water hose that grows to a fire, or an oxygen tube that grows to a trapped disaster victim. The project will demonstrate vine-like robots able to configure or weave themselves into three-dimensional objects and structures such as ladders, antennae for communication, and shelters. These novel co-robots aim to improve human safety, health, and well-being at a lower cost than conventional robots achieving similar outcomes. Because of their low cost, vine robots offer exceptional educational opportunities; the project will include creation and testing of inexpensive educational modules for K-12 students.<br/><br/>This work broadens the concept of bio-inspired robots from animals to plants, the concept of locomotion from point-to-point movement to growth. In contrast to traditional terrestrial moving robots that tend to be based on the animal modality of repeated intermittent contacts with a surface, the vine modality begins with a root, harboring power and logic, and extends using growth, increasing permanent contacts throughout the process. This project will demonstrate a soft robot capable of growing over 100 times in length, withstanding being stepped on, extending through gaps a quarter of its height, climbing stairs and vertical walls, and navigating over rough, slippery, sticky and aquatic terrain. The design adopts a bio-inspired strategy of moving material through the core to the tip, allowing the established part of the robotic vine to remain stationary with respect to the environment. A thin-walled tube fills with air as it grows, allowing the vine robot to be initially stored in a small volume at its base, and to extend very large distances when controllably deployed. Mechanical modeling and new design tools will enable the development of task-specific vine robots for search and rescue, reconfigurable communication antennas, and construction. The paradigm of achieving movement and construction through growth will produce new technologies for integrated actuation, sensing, planning, and control; novel principles and software tools for robot design; and humanitarian applications that push the boundaries of collaborative robotics."
"1637704","NRI: Decentralized Feedback Control Design for Cooperative Robotic Walking with Application to Powered Prosthetic Legs","CMMI","National Robotics Initiative","09/01/2016","08/08/2016","Kaveh Akbari Hamed","CA","San Diego State University Foundation","Standard Grant","Jordan Berg","08/31/2019","$612,213.00","Robert Gregg","kakbarihamed@mail.sdsu.edu","5250 Campanile Drive","San Diego","CA","921822190","6195945731","ENG","8013","8086","$0.00","This project addresses the creation of innovative decentralized controllers for legged locomotion. Decentralized controllers require only local information to accomplish their function. In the case of legged locomotion, decentralization is desirable for several reasons. For prosthetics, where the purpose is to replace a lost natural limb, it is impractical to wire the user with a profusion of sensors. Therefore the prosthetic device must primarily rely on its own built-in measurements. Another advantage of decentralization is the management of complexity. As robots become more sophisticated, the number of variables that must be monitored for a complete description of the system status becomes so large that top-down controllers are costly or infeasible to implement. The challenge of decentralized control is made substantially more difficult because walking and running are hybrid dynamic behaviors, that is, the dynamics follow a completely different set of rules when, for example, a foot is planted on the ground, compared to when it is swinging in the air. This project will address the substantial analytical difficulties caused by these features. This project will advance the state of the art in advanced lower limb prosthetics, as well as in locomotion for the next generation of legged robots. <br/><br/>This project will investigate the systematic design of decentralized feedback controllers that coordinate low-dimensional subsystems to achieve robust legged locomotion, overcoming the curse of dimensionality in legged robots and enabling cooperative human-machine walking with powered prosthetic legs. The project draws upon robotics, optimization, and feedback control theory to advance two key innovations: (1) creating algorithms to systematically design robust stabilizing decentralized controllers for cooperative subsystems; and (2) transferring the decentralized control framework into practice with an experimental quadruped and a powered prosthetic leg. The problem of creating decentralized nonlinear controllers for robust dynamic walking with interconnected subsystems, coordinated only by a common gait cycle phasing variable, will be formulated in the context linear and bilinear matrix inequalities. The theoretical significance of these algorithms include: (1) they are powerful tools for the design of general nonlinear decentralized feedback control schemes; (2) they explicitly account for underactuation to account for walking motions that are not flat-footed; (3) they provide cooperation between subsystems of complex walking models with high dimensionality and strong interactions; and (4) they provably stabilize full-dimensional hybrid dynamical models of walking robots rather than simplified models. This decentralized control framework is technologically significant because it can be readily transferred into practical high-DOF legged robots, as well as wearable robots for physical rehabilitation."
"1208233","NRI-Small: Robust, highly-mobile MEMS micro-robots based on integration of piezoelectric and polymer materials","IIS","National Robotics Initiative","09/01/2012","08/01/2012","Kenn Oldham","MI","University of Michigan Ann Arbor","Standard Grant","Jordan Berg","08/31/2014","$195,195.00","","oldham@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8013","7923, 8086","$0.00","The research objective of this award is to integrate polymer and piezoelectric micro-structures to create robust, sub-centimeter terrestrial micro-robots, and to use a combination of modeling and experimentation to evaluate leg dynamics of such robots. Specifically, high-aspect ratio parylene flexural mechanisms will be integrated with thin-film lead-zirconate-titanate (PZT) actuators in complex, multi-degree-of-freedom micro-robotic leg joints. Experimental measurements of parylene structure response to integrated thin-film PZT actuation or external bulk PZT ceramic or load cell actuation will be used to characterize parylene stiffness and damping characteristics at varying strain rates, relative to high-strain rate piezoelectric actuation. Adhesion between bulk-micromachined silicon trench surfaces and PZT/metal stack layers will also be evaluated. Measured parylene properties will then be incorporated into existing micro-robotic foot-terrain models developed by the PI and students to produce simulation models of PZT-polymer robots that can be validated against experimental robot prototypes.<br/><br/>Successful completion of this work would dramatically improve the ability of walking millimeter-scale micro-robots to move over uneven terrain, thus increasing the range of possible interactions between human operators and engineered or natural systems. The target user community for millimeter-scale autonomous robots includes disaster response teams, infrastructure maintenance and monitoring workers, and national security organizations. The framework to be deployed would be a technique to embed piezoelectric microactuators in resilient micro-robotic appendages, producing sample walking micro-robot platforms. Results from this research would be coupled into both undergraduate and graduate curriculum and secondary school education. The latter effort will consist of interactive hands-on and web-based projects developed by the PI for use in science education for the local Ypsilanti, Michigan school district and the broader community of interested citizens."
"1208499","NRI-Small: Cooperative Underwater Robotic Networks for Discovery & Rescue","CMMI","CONTROL SYSTEMS, National Robotics Initiative","09/01/2012","12/23/2015","Chengyu Cao","CT","University of Connecticut","Standard Grant","Jordan Berg","08/31/2015","$567,719.00","Jun-Hong (June) Cui, Kazem Kazerounian","ccao@engr.uconn.edu","438 Whitney Road Ext.","Storrs","CT","062691133","8604863622","ENG","1632, 8013","030E, 031E, 034E, 116E, 7923, 8086, 9178, 9251","$0.00","The goal of this project is to develop a cooperative underwater robotic network for underwater discovery and rescue. With technological advancements, human involvement in underwater activities has increased. Vulnerable oceanic engineering systems, however, can have drastically negative environmental and economic consequences. The oil spill in the Gulf of Mexico in 2010 was a case in point for the impact of underwater infrastructure has on marine ecosystem and local economy. Compared to manned systems, underwater robots, also called autonomous underwater vehicles (AUVs), have inherent advantages by eliminating the need of life support systems and potential risk of human life. Further, an AUV network (with a swam of AUVs) can offer more benefits in efficiency and cost in underwater exploration, discovery and rescue. In this project, innovative algorithms, methods and techniques in autonomous underwater vehicle (AUV) design, cooperative control and underwater acoustic communication networks are proposed to ensure the AUV network's performance in highly uncertain environments. Deliverables for this project include key enabling technologies for the AUV networks and the demonstration of a network including several prototype AUVs.<br/><br/>A high performance, energy efficient and autonomous AUV network is significant to science, economy and society. It will have significant impact to underwater infrastructure inspection, wildlife and habitat monitoring, and search and rescue missions. It can also be leveraged for oceanography data collection and water pathway monitoring. Beyond the research significance, this project has important impact on education and outreach by supporting undergraduates, women and other under-represented groups, as well as promoting multi-disciplinary collaboration across departments, campuses, and institutions."
"1317952","NRI: Small: EEG and EMG Human Model-Based Adaptive Control of a Dexterous Artificial Hand","IIS","National Robotics Initiative","09/15/2013","09/05/2013","Erik Engeberg","OH","University of Akron","Standard Grant","Jordan Berg","06/30/2015","$250,000.00","","eengeberg@fau.edu","302 Buchtel Common","Akron","OH","443250001","3309727666","CSE","8013","7923, 8086","$0.00","The research goal of this award is to explore different methods for upper limb amputees to control dexterous artificial hands with brain waves. Biomedical signal processing techniques will be developed to enable this with a single, small recording electrode placed noninvasively on the amputees' heads. The recorded brain waves will be wirelessly transmitted to the hand in real time. A top-level controller will be developed to interpret the intent of the amputees while a low-level controller will be used to synchronize the dexterous grasp motions of the artificial hand. Algorithms will also be developed using tactile feedback from the fingertips to automatically prevent grasped objects from being accidentally dropped when they are transported or disturbed. Amputees will participate in a study to compare the newly developed artificial hand control techniques with brain waves to conventional control techniques with muscle signals during common tasks of daily life.<br/><br/><br/>If successful, this research will result in a noninvasive and economic method for amputees to control a dexterous artificial hand with brain waves. This could substantially improve the functionality of prosthetic hands for many amputees. The use of minimal hardware will facilitate the clinical adoption of this technique and the autonomous low-level control algorithms will produce a brain machine interface that places a low cognitive burden on the operator. This research can also be readily applied to benefit many disabled people including stroke victims and quadriplegics and can positively impact other areas of robotics such as improvised explosive disarmament, underwater and space exploration, and rescue robotics. Underrepresented engineering students will benefit from being included in this research plan. Additionally, undergraduate and graduate engineering students will benefit from newly developed classes and laboratory exercises resulting from this research."
"1317788","NRI: Small: Collaborative Research: Active Sensing for Robotic Cameramen","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","09/15/2013","04/29/2015","Ibrahim Isler","MN","University of Minnesota-Twin Cities","Standard Grant","Jie Yang","08/31/2017","$300,000.00","","isler@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7495, 8013","7923, 8086, 9251","$0.00","With advances in camera technologies, and as cloud storage, network bandwidth and protocols become available, visual media are becoming ubiquitous. Video recording became de facto universal means of instruction for a wide range of applications such as physical exercise, technology, assembly, or cooking. This project addresses the scientific and technological challenges of video shooting in terms of coverage and optimal views planning while leaving high level aspects including creativity to the video editing and post-production stages. <br/><br/>Camera placement and novel view selection challenges are modeled as optimization problems that minimize the uncertainty in the location of actors and objects, maximize coverage and effective appearance resolution, and optimize object detection for the sake of semantic annotation of the scene. New probabilistic models capture long range correlations when the trajectories of actors are only partially observable. Quality of potential novel views is modeled in terms of resolution that is optimized by maximizing the coverage of a 3D orientation histogram while an active view selection process for object detection minimizes a dynamic programming objective function capturing the loss due to classification error as well as the resources spent for each view.<br/><br/>The project advances active sensing and perception and provides the technology for further automation on video capturing. Such technology has broader impact on the production of education videos for online courses as well as in telepresence applications. Research results are integrated into robotics and digital media programs addressing K-12 students."
"1426338","NRI/Collaborative Research: Improving the Safety and Agility of Robotic Flight with Bat-Inspired Flexible-Winged Robots","CMMI","National Robotics Initiative","08/01/2014","05/17/2016","Kenneth Breuer","RI","Brown University","Standard Grant","Jordan Berg","07/31/2017","$708,000.00","Sharon Swartz","kbreuer@brown.edu","BOX 1929","Providence","RI","029129002","4018632777","ENG","8013","116E, 8086, 9150, 9178, 9231, 9251","$0.00","Bat flight, perhaps the most advanced and efficient form of animal flight, has long been a source of inspiration for roboticists and biologists alike. This National Robotics Initiative (NRI) collaborative research award supports research aimed at understanding and reproducing the unparalleled agility and resilience of bat flight. Biological studies of bats (their structure, muscle movement, and flight dynamics) will drive the engineering development of mathematical models of robotic flight and the eventual design and implementation of a prototype 30-80cm bat-like robot. The physical flight capabilities of the robot will be augmented with perception and reasoning abilities, with the aim of providing support for construction site activities such as site monitoring, inspection, and general surveillance of the work site to provide image data to enhance situational awareness of human workers. The research involves several disciplines, including biology, aerodynamics, robotics, control systems engineering, and construction engineering.<br/><br/>Aerial robots have nowhere near the agility and efficiency of animal flight, especially in complex, constrained environments. This is not surprising since even the simplest winged robots have complex flight dynamics that pose significant challenges for modeling, design, and control. In the case of bat-inspired robots, these difficulties are exacerbated by the use of under-actuated mechanisms driving wings constructed from flexible membranes. This project will combine biological and engineering research to address these problems. Biological research on the kinematics of bats and their flight will provide a basis for mechanical designs. To control the robot, agile motion planning and flight control algorithms will employ motion primitives that are derived from biological investigation of the dynamics of bat flight. Conversely, models obtained from biological studies will be validated by experimental investigations using the prototype robot, enabling iterative refinement of reduced-order models and control algorithms. Ultimately, the robots will be equipped with sensing systems and planning algorithms, to facilitate localization, mapping, inspection and surveillance at construction sites."
"1426799","NRI/Collaborative Research: Models and Instruments for Integrating Effective Human-Robot Teams into Manufacturing","CMMI","National Robotics Initiative","09/01/2014","08/01/2014","Julie Shah","MA","Massachusetts Institute of Technology","Standard Grant","Bruce M. Kramer","08/31/2017","$300,000.00","","arnoldj@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","ENG","8013","082E, 092E, 1786, 8086, 9102, MANU","$0.00","Robots for application in collaborative manufacturing must perform manual work side-by-side with people. Such robots offer the flexibility to work on many different tasks and promise to transform manufacturing by improving the quality and efficiency of manual processes in small shops and in facilitates that manufacture highly customized products. However, in order to meet this promise, robots must be effectively integrated into existing manufacturing teams and practices. To enable this integration, this National Robotics Initiative (NRI) award supports fundamental research on the methods and instruments that manufacturing engineers will need to form effective human-robot teams based on task requirements and worker skills. These methods will also enable robots to adapt to changes in workflow to maximize safety and efficiency. The effective integration of collaborative robots into manufacturing promises improvements in many industries that have not yet benefited from robotic technology. Therefore, results from this research will contribute to the competitiveness of U.S. manufacturing and benefit the U.S. economy and society. The research will involve contributions from multiple disciplines, including robotics, human factors, computer science, and manufacturing, and by academic and industry collaborators. These collaborations will help the dissemination of research results into manufacturing organizations and the integration of research into undergraduate and graduate curriculum in engineering.<br/><br/>Advancements in robotics promise the use of collaborative robots that perform interdependent work with people in order to improve quality, efficiency, and safety in industrial manufacturing. However, integrating collaborative robots into these processes and ensuring their efficient operation pose significant research challenges, including the optimal allocation of work based on task requirements and constraints, the formation of human-robot teams, and the dynamic adaptation of teamwork to workflow changes. This research will address these research challenges, enabling the seamless integration of collaborative robots into these processes and achieving efficient and safe collaboration between human and robot workers. The research team will create novel methods for optimal allocation of tasks to human and robot workers based on task constraints and worker skills, design new tools that utilize these methods to facilitate workflow design for human-robot teams, and develop novel mechanisms that enable robots to more efficiently and safely collaborate with human workers in the planned manufacturing operations. These methods and instruments will be validated in real-world manufacturing operations and disseminated through industry workshops, engineering curricula, and a public outreach program."
"1427050","NRI: Collaborative Research: Efficient Algorithms for Contact-Aware State Estimation","IIS","National Robotics Initiative","08/15/2014","08/11/2014","Russell Tedrake","MA","Massachusetts Institute of Technology","Standard Grant","Jie Yang","07/31/2017","$874,928.00","Alberto Rodriguez Garcia","russt@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","8086","$0.00","This project addresses the difficult theoretical, computational, and applied challenges required to exploit a deep mathematical relationship between recent advances in machine perception/estimation algorithms and recent advances in algorithms for planning/controlling systems undergoing frictional contact. It explores the immediate applications of these algorithms to perception, robotic object manipulation and parts assembly, and humanoid robots performing complex, multi-contact, whole-body maneuvers. In order to showcase the generality of approach, and simultaneously reach out to the important under-represented minority population, the research team employs a new hands-on short-course curriculum in which students apply the proposed algorithms to predict the outcome of games using visual tracking. The course is being developed in partnership with the MIT Office of Minority Education (OME).<br/><br/>The project brings together expertise in simultaneous localization and mapping (SLAM), robot manipulation, robotic automation, legged robots, and optimization and nonlinear control, leading to a cross-fertilization of ideas and techniques. The research team exploits sparsity in the complementarity formulations of contact in Lagrangian dynamics. The project explores a new algebraic approach to nonlinear estimator design. The project produces new theorems, new algorithms, and experimental results on real robots. The project also represents a new partnership with our industrial collaborator, ABB Robotics. The developed algorithms facilitate a broad range of new applications in which perception and control systems monitor and manipulate physical interactions with the world. From palm-sized smart devices to environmental monitoring, sensors are becoming ubiquitous; to reach their full potential these sensor networks must be able to reason about contact - the basic building block of physical interaction."
"1427255","NRI: Human Cognition Assisted Control of Industrial Robots for Manufacturing","CMMI","National Robotics Initiative","08/01/2014","07/22/2014","T. Kesavadas","NY","SUNY at Buffalo","Standard Grant","Bruce M. Kramer","12/31/2014","$558,527.00","","kesh@illinois.edu","520 Lee Entrance","Amherst","NY","142282567","7166452634","ENG","8013","087E, 092E, 6840, 8086, 8091, MANU","$0.00","Advanced manufacturing, driven by industrial robots, is playing an increasing role in US economy. Robots are being used to carry out assembly, welding, material handling and fabrication. Even as such interactions are becoming more common in every phase of manufacturing, a perfect symbiotic relationship between machines and human beings is still very far away. Because of this, a majority of the robotic applications in manufacturing are currently limited to areas where a relatively low level of skill is required. This has restricted the full potential of robotics to augment human operators and improve productivity and quality of life. With recent advances in cognitive neuroscience and brain interface technologies, connecting the human cognitive thought process directly to robots and machines is possible, resulting in direct control of real world applications. By collecting the brain signals using sensors and analyzing the thought processes, many activities that take place inside the brain when humans take specific actions or think of actions can be identified and matched to known signals using fast computation. This new human-robot communication paradigm will be demonstrated by developing three manufacturing scenarios. The project will also have broad applicability in the design of robotic systems in fields outside manufacturing, including telesurgery, rehabilitation and space exploration. Results from this multidisciplinary research, which combines manufacturing, computer science and robotics, have the potential to improve the productivity of future manufacturing plants and can lead to new commercial ventures, which will help the US maintain global leadership in robotics and manufacturing, broaden participation of underrepresented groups in research, and positively impact engineering education.<br/><br/>Significant future challenges in the development of a new human-robot communication system, which allows operators to perform complex high skilled tasks, will be addressed. The postulated paradigm will be explored by meeting the following intellectual challenges: (i) researching a novel methodology for communicating motion commands to a robot by imagining simple actions using a grammar called ""actemes,"" (ii) new brain-computer mode and algorithms to classify these actemes and, (iii) an intent-based system that auto-completes robotic actions based on most likely sequence of events that human operators are planning to complete. Three robotic manufacturing scenarios will be explored to demonstrate the human cognition based interactions in manufacturing environment: assembly, direct control, and quality control through object recognition. Finally, by using a non-invasive brain-computer interface a wide range of day-to-day applications of robotics will be demonstrated."
"1637838","NRI: Achieving Selective Kinematics and Stiffness in Flexible Robotics","CMMI","National Robotics Initiative","12/01/2016","08/20/2016","Robert Howe","MA","Harvard University","Standard Grant","Jordan Berg","11/30/2019","$547,508.00","Katia Bertoldi","howe@seas.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385366","6174955501","ENG","8013","8086","$0.00","This research project will employ an effect called laminar jamming to create robotic structures that controllably transition between highly compliant and nearly rigid. Precise position control typically requires rigidity, while safe and comfortable interactions with humans requires compliance. The need to incorporate both characteristics arises, for example, in assistive applications such as tremor suppression in Parkinson's disease. It is also necessary for robots working collaboratively with humans in manufacturing settings. Laminar jamming structures are composed of multiple parallel layers that ordinarily slide easily over each other. However, when the laminar structure is squeezed the layers become locked together by friction forces. This project will derive computationally tractable models of laminar structures under external forcing, to enable parametric design and real-time control. Fabrication of laminar jamming devices is simple and inexpensive, lowering barriers to widespread use, both in commercial applications and in educational settings. To ensure dissemination of the results, reference designs, configurations of jamming elements, application prototypes, and testing and performance data will be posted on a popular soft robotics website.<br/><br/>The combination of laminar jamming and soft robotics opens an entirely new range of robot designs and behaviors. Because the bending stiffness of a beam is proportional to the third power of its thickness, even a few laminae can produce dramatic increases in stiffness when jammed. The goal of this project is to define the capabilities of the technology and derive and validate the fundamental underlying principles. The project consists of three research subtasks: actuator design and testing, computational and analytical modeling, and implementation of key applications. The results of the project will provide a rich set of building blocks for robots that combine the advantageous features of both soft and rigid robots."
"1329620","CPS: Synergy: Collaborative Research: High-Level Perception and Control for Autonomous Reconfigurable Modular Robots","CNS","INFORMATION TECHNOLOGY RESEARC, SPECIAL PROJECTS - CISE, CYBER-PHYSICAL SYSTEMS (CPS), National Robotics Initiative","10/01/2013","05/15/2014","Mark Yim","PA","University of Pennsylvania","Standard Grant","Bruce M. Kramer","09/30/2016","$416,000.00","","yim@grasp.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","1640, 1714, 7918, 8013","6840, 7918, 8086, 9102, 9178, 9251","$0.00","The goal of the project is the development of the theory, hardware and computational infrastructure that will enable automatically transforming user-defined, high-level tasks such as inspection of hazardous environments and object retrieval, into provably-correct control for modular robots. Modular robots are composed of simple individual modules; while a single module has limited capabilities, connecting multiple modules in different configurations allows the system to perform complex actions such as climbing, manipulating objects, traveling in unstructured environments and self-reconfiguring (breaking into multiple independent robots and reassembling into larger structures). The project includes (i) defining and populating a large library of perception and actuation building blocks both manually through educational activities and automatically through novel algorithms, (ii) creating automated tools to assign values to probabilistic metrics associated with the performance of library components, (iii) developing a grammar and automated tools for control synthesis that sequence different components of the library to accomplish higher level tasks, if possible, or provide feedback to the user if the task cannot be accomplished and (iv) designing and building a novel modular robot platform capable of rapid and robust self-reconfiguration.<br/><br/>This research will have several outcomes. First, it will lay the foundations for making modular robots easily controlled by anyone. This will enrich the robotic industry with new types of robots with unique capabilities. Second, the research will create novel algorithms that tightly combine perception, control and hardware capabilities. Finally, this project will create an open-source infrastructure that will allow the public to contribute basic controllers to the library thus promoting general research and social interest in robotics and engineering."
"1634431","NRI: A Proactive Approach to Managing Contingencies during Human Robot Collaboration in Manufacturing","CMMI","National Robotics Initiative","08/01/2016","05/02/2016","Satyandra Gupta","CA","University of Southern California","Standard Grant","Bruce M. Kramer","11/30/2018","$639,076.00","","guptask@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","ENG","8013","030E, 071E, 082E, 6840, 8086","$0.00","Recent advances in human-safe industrial robots present an opportunity for creating hybrid work cells, where humans and robots can collaborate in close physical proximities. This capability enables realizing systems that utilize complementary strengths of humans and robots. Several new low-cost robots have been introduced in the market over the last few years, making them attractive in many new manufacturing applications. This makes the idea of hybrid cells economically viable for small and medium manufacturers, which represent a very important segment of the manufacturing sector in the United States. Both humans and robots can make errors in a hybrid cell, hence creating contingency situations. Unless handled promptly, a contingency situation may lead to significant operational inefficiencies. Hence, hybrid cells are not likely to cost-effective unless contingency situations are proactively detected and managed. This National Robotics Initiative (NRI) award supports fundamental research to enable proactive management of contingencies arising during human-robot collaboration in hybrid work cells in small production volume manufacturing operations. Results from this research will enable introduction of industrial robots in small production volume operations and are expected to make US manufacturing industry cost competitive in the global market. The integration of the research with graduate and undergraduate courses will enhance the robotics and manufacturing curricula and enrich learning experiences of the participating students. Outreach activities will educate and inform K-12 students about career opportunities in robotics and manufacturing. <br/><br/>Fundamental advances are needed in automated planning, cell monitoring, replanning, and human-robot interaction to endow hybrid cells with effective contingency handling capabilities. This research will investigate computational foundations for the design of task planning and resource allocation algorithms that explicitly account for managing contingencies. Algorithms will be developed for real-time monitoring of the task progress to ensure that tasks are completed in a safe and efficient manner. Real-time replanning algorithms will be designed to handle contingencies and refine plans based on the observed task execution performance. This research will also explore and characterize methods for effective information exchange between humans and robots to deal with contingencies."
"1317477","NRI: Small: Magnetic Mobile Micro-Robotic Swarms using Smart Magnetic Composites","CBET","National Robotics Initiative","09/15/2013","09/09/2013","Metin Sitti","PA","Carnegie-Mellon University","Standard Grant","Jordan Berg","08/31/2017","$800,000.00","","sitti@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","ENG","8013","7923, 8086","$0.00","The research objective of this award is to develop methods for modeling, design and control of a large number of magnetic mobile micro-robots for a given specific application. Here, micro-robots are proposed to be made of novel smart magnetic composites for remote, parallel magnetic actuation and control in two dimensions and three dimensions in small confined spaces such as inside the human body, a micro-fluidic chip, and small desktop workspaces. The research will result in methods which are general enough to apply across all magnetic micro-robotic systems and that can be exploited in diverse applications. Deliverables include a catalog of fundamental modeling, analysis, design and fabrication tools, demonstration and validation via hardware, documentation of research results, and engineering student education.<br/><br/>If successful, the results of this research will provide an opportunity to create new micro-robotic systems for use in health-care, microfluidic bio-devices, tissue engineering, programmable matter, and desktop micro-manufacturing applications. The results will be disseminated to allow the creation of commercial micro-robotic devices for medical, bioengineering and micro-manufacturing applications. Graduate and undergraduate engineering students will benefit through classroom instruction and involvement in the research. The research results of the project will be presented to children, K-12 students, K-12 teachers, IEEE student members, and college students through public lectures. The achievements of the project will be also exhibited in science museums for educating the public and children on micro-robotics."
"1317981","NRI: Small: Reflex approximation of optimal control for an energy-efficient bipedal walking platform","CMMI","National Robotics Initiative","09/01/2013","08/24/2013","Andy Ruina","NY","Cornell University","Standard Grant","Jordan Berg","08/31/2017","$999,912.00","","ruina@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","ENG","8013","7923, 8086","$0.00","The broad research goal of this project is to better understand the mechanics, balance, and energetics of bipedal walking. The first specific goal is to design, build and test a two-legged walking 'platform', a waist-down robot, and its computer controller. One can think of this platform as a legged Segway, for use by others putting on robotic upper bodies with arms and heads. The goal is to match the robustness of existent bipedal robots but with no need for electrical cords or hydraulic tubes and with approximately a 90 percent reduction in energy use. This will be achieved by the development and use of a reflex-based controller that minimizes the needed sensor bandwidth and on-board computation. The second goal is to advance the theories of walking balance and energetics including, especially, basic models for understanding the tradeoffs between robustness, versatility, energy efficiency, sensor accuracy and motor size. Deliverable outcomes include a demonstrable robot with a new 'reflex-based' controller, public access to complete design and control information, basic design formulas for the fundamental design trade-offs, student design and research experience, and inspiring demonstrations for public viewing.<br/><br/>If successful, robots based on this platform design could replace wheel and tread based systems used for, and proposed for use for, service robotics. The advantages of legs over wheels include a more anthropomorphic look, for human interaction, and, ultimately with further development, the ability to function on rough terrain. The basic theories of balance and energetics developed in this work will add insight concerning human walking, ultimately helping with the diagnosis and remediation of human disabilities, including aiding the design of assistive and prosthetic devices. The hands-on development work will educate and inspire engineering students working on the project. The demonstrations and demonstration videos will serve as an inspiration to those considering engineering careers."
"1427122","NRI: Flexible Multi-Leg Robots for Safe Interaction and Surgical Dexterity","CMMI","National Robotics Initiative","09/01/2014","07/21/2014","Daniel Rucker","TN","University of Tennessee Knoxville","Standard Grant","Jordan Berg","08/31/2017","$372,794.00","","drucker6@utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","ENG","8013","8086, 9150","$0.00","Robotic devices can allow doctors to intuitively command precise motions of surgical tools within the body while requiring much smaller incisions and less damage to healthy tissue than conventional surgery. Today, the use of such surgical robots has reduced patient recovery times, trauma, and costs in an increasing number of procedures. However, the impact of robotics in surgery remains hindered by the limited dexterity, low strength, and relatively large size of current robotic tools, especially for procedures which require operation within highly confined spaces that are difficult to access. This award supports fundamental robotics research that will improve public health by creating smaller, stronger, and more dexterous robotic tools for minimally invasive surgery. This will benefit broad patient populations by enabling new procedures to be performed robotically and existing robotic procedures to be performed more efficiently. Beyond these medical benefits, the research will also serve the manufacturing and service industries by creating robots that can work alongside humans with inherent safety due to their lightweight, compliant structure. Educational activities are integrated with the research which will help broaden the participation of underrepresented groups in engineering research through hands-on robot design challenges for high school students.<br/><br/>The project will address the primary robotics questions of design synthesis, modeling, and control for parallel continuum manipulators comprised of multiple flexible legs which each bend independently. Successful completion of the project will include (1) design and construction of two testbed system prototypes, (2) an experimentally validated, mechanics-based modeling framework for manipulator kinematics and statics, (3) a set of design guidelines driven by model-based analysis of workspace, dexterity, and stiffness, and (4) a kinematic control approach demonstrated by teleoperation to complete dexterous tasks with safe human interaction. This research will unify two previously separate fields of robotics, namely parallel manipulators and continuum manipulators. It will advance knowledge in both fields by realizing a hybrid class of manipulators and providing a model-based framework for understanding their behavior, designing them, and controlling them. The project will also represent the first application of large-deflection Cosserat-rod theory (which has only recently been introduced to the robotics community) to parallel robots. By answering the fundamental robotics questions in this new area, a body of useful knowledge and tools will be made available for researchers to make further advances in minimally invasive robotic surgery and human-robot interaction."
"1527220","NRI: A Proactive Approach to Managing Contingencies during Human Robot Collaboration in Manufacturing","CMMI","National Robotics Initiative","09/01/2015","06/21/2015","Satyandra Gupta","MD","University of Maryland College Park","Standard Grant","Bruce M. Kramer","06/30/2016","$639,076.00","","guptask@usc.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","ENG","8013","030E, 071E, 082E, 6840, 8086","$0.00","Recent advances in human-safe industrial robots present an opportunity for creating hybrid work cells, where humans and robots can collaborate in close physical proximities. This capability enables realizing systems that utilize complementary strengths of humans and robots. Several new low-cost robots have been introduced in the market over the last few years, making them attractive in many new manufacturing applications. This makes the idea of hybrid cells economically viable for small and medium manufacturers, which represent a very important segment of the manufacturing sector in the United States. Both humans and robots can make errors in a hybrid cell, hence creating contingency situations. Unless handled promptly, a contingency situation may lead to significant operational inefficiencies. Hence, hybrid cells are not likely to cost-effective unless contingency situations are proactively detected and managed. This National Robotics Initiative (NRI) award supports fundamental research to enable proactive management of contingencies arising during human-robot collaboration in hybrid work cells in small production volume manufacturing operations. Results from this research will enable introduction of industrial robots in small production volume operations and are expected to make US manufacturing industry cost competitive in the global market. The integration of the research with graduate and undergraduate courses will enhance the robotics and manufacturing curricula and enrich learning experiences of the participating students. Outreach activities will educate and inform K-12 students about career opportunities in robotics and manufacturing. <br/><br/>Fundamental advances are needed in automated planning, cell monitoring, replanning, and human-robot interaction to endow hybrid cells with effective contingency handling capabilities. This research will investigate computational foundations for the design of task planning and resource allocation algorithms that explicitly account for managing contingencies. Algorithms will be developed for real-time monitoring of the task progress to ensure that tasks are completed in a safe and efficient manner. Real-time replanning algorithms will be designed to handle contingencies and refine plans based on the observed task execution performance. This research will also explore and characterize methods for effective information exchange between humans and robots to deal with contingencies."
"1525972","NRI: A Compliant Lower-Body Exoskeleton to Enable Balanced Walking for Patients with Spinal Cord Injuries","IIS","National Robotics Initiative","09/01/2015","06/21/2016","A. Wicks","VA","Virginia Polytechnic Institute and State University","Standard Grant","Jordan Berg","08/31/2018","$749,598.00","Tomonari Furukawa","awicks@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8013","030E, 031E, 034E, 116E, 8086, 9178, 9231, 9251","$0.00","Spinal cord injuries can lead to complete leg paralysis, drastically limiting mobility and reducing quality of life. Significant progress has been made towards wearable exoskeletons that provide some ability to walk, but these require the use of crutches, which create unnatural loads on the wearer, as well as restricting use of the hands and arms. The objective of this research is to develop a lower body exoskeleton able to balance and walk without the use of crutches, thus allowing the wearers to interact with their environment and maintain proper posture. The resulting platform will improve the overall quality of life for those with spinal cord injury by granting them a level of mobility that is currently unachievable. <br/><br/>This work will apply methods advanced in the field of humanoid robots to the design of exoskeletons for functionally impaired persons. Specifically, the project will apply a method called whole-body control, which finds motions that resolve multiple motion objectives at once, to a lower body exoskeleton with 12 degrees of freedom, providing a range of movement similar to natural human locomotion, with the exception of ankle yaw. High fidelity torque control will be achieved using series elastic actuators to enable compliant locomotion of the wearer."
"1464737","NRI: Human Cognition Assisted Control of Industrial Robots for Manufacturing","CMMI","National Robotics Initiative","12/01/2014","11/03/2014","T. Kesavadas","IL","University of Illinois at Urbana-Champaign","Standard Grant","Bruce M. Kramer","07/31/2017","$558,527.00","","kesh@illinois.edu","SUITE A","CHAMPAIGN","IL","618207473","2173332187","ENG","8013","087E, 092E, 6840, 8086, 8091, MANU","$0.00","Advanced manufacturing, driven by industrial robots, is playing an increasing role in US economy. Robots are being used to carry out assembly, welding, material handling and fabrication. Even as such interactions are becoming more common in every phase of manufacturing, a perfect symbiotic relationship between machines and human beings is still very far away. Because of this, a majority of the robotic applications in manufacturing are currently limited to areas where a relatively low level of skill is required. This has restricted the full potential of robotics to augment human operators and improve productivity and quality of life. With recent advances in cognitive neuroscience and brain interface technologies, connecting the human cognitive thought process directly to robots and machines is possible, resulting in direct control of real world applications. By collecting the brain signals using sensors and analyzing the thought processes, many activities that take place inside the brain when humans take specific actions or think of actions can be identified and matched to known signals using fast computation. This new human-robot communication paradigm will be demonstrated by developing three manufacturing scenarios. The project will also have broad applicability in the design of robotic systems in fields outside manufacturing, including telesurgery, rehabilitation and space exploration. Results from this multidisciplinary research, which combines manufacturing, computer science and robotics, have the potential to improve the productivity of future manufacturing plants and can lead to new commercial ventures, which will help the US maintain global leadership in robotics and manufacturing, broaden participation of underrepresented groups in research, and positively impact engineering education.<br/><br/>Significant future challenges in the development of a new human-robot communication system, which allows operators to perform complex high skilled tasks, will be addressed. The postulated paradigm will be explored by meeting the following intellectual challenges: (i) researching a novel methodology for communicating motion commands to a robot by imagining simple actions using a grammar called ""actemes,"" (ii) new brain-computer mode and algorithms to classify these actemes and, (iii) an intent-based system that auto-completes robotic actions based on most likely sequence of events that human operators are planning to complete. Three robotic manufacturing scenarios will be explored to demonstrate the human cognition based interactions in manufacturing environment: assembly, direct control, and quality control through object recognition. Finally, by using a non-invasive brain-computer interface a wide range of day-to-day applications of robotics will be demonstrated."
"1527016","NRI: Collaborative Research: Dynamic Robot Guides for Emergency Evacuations","CMMI","National Robotics Initiative","09/01/2015","08/03/2015","Yi Guo","NJ","Stevens Institute of Technology","Standard Grant","Jordan Berg","08/31/2018","$315,274.00","","yguo1@stevens.edu","CASTLE POINT ON HUDSON","HOBOKEN","NJ","070305991","2012168762","ENG","8013","030E, 6840, 8024, 8086, 9102","$0.00","Crowd stampede is one of the most harmful collective human behaviors. In incidents throughout history, panic due, for example, to the outbreak of fire or the unexpected discharge of firearms has been a greater hazard than the original triggering events. This project supports fundamental research on the influence of human-robot interaction on crowd dynamics, towards the design of dynamic robot control algorithms to assist humans and prevent panic in emergency situations. The ultimate goal of this research will be reconfigurable robot guides that can respond to a variety of needs. These include different types of emergency evacuation, as well as non-emergency situations involving mass movement of crowds, such as at parades, concerts, or other large public events. The project integrates research with educational activities through robot-centric education and short course development. To engage the younger generation with science and technology, the project will partner with a university educational center and a community college for various outreach activities.<br/><br/>The objective of the project is to investigate human-robot interaction in crowd dynamics, develop optimal feedback control to regulate human flow distribution, and design robot-assisted emergency evacuation algorithms. The research will advance the state-of-the-art in human-robot interaction, and fill a gap in robotics research by experimentally validating and measuring the interaction forces governing human-robot interaction in crowd dynamics. The proposed robot motion primitive design leads to new approaches for learning-based robot motion planning to efficiently engage humans. The project validates the use of dynamic robot guides in real human-robot interaction experiments in indoor environments. Simulation validation in benchmark environments such as shopping-malls and campus buildings will also be performed, and the efficiency of alternative robot-assisted evacuation strategies will be evaluated. While primarily for intelligent robots, the research results are anticipated to be cross-cutting and applicable to other areas such as transportation, communication, and control."
"1526835","NRI: Collaborative Research: Dynamic Robot Guides for Emergency Evacuations","CMMI","National Robotics Initiative","09/01/2015","08/03/2015","Haibo He","RI","University of Rhode Island","Standard Grant","Jordan Berg","08/31/2018","$282,112.00","","he@ele.uri.edu","RESEARCH OFFICE","KINGSTON","RI","028811967","4018742635","ENG","8013","030E, 6840, 8024, 8086, 9150","$0.00","Crowd stampede is one of the most harmful collective human behaviors. In incidents throughout history, panic due, for example, to the outbreak of fire or the unexpected discharge of firearms has been a greater hazard than the original triggering events. This project supports fundamental research on the influence of human-robot interaction on crowd dynamics, towards the design of dynamic robot control algorithms to assist humans and prevent panic in emergency situations. The ultimate goal of this research will be reconfigurable robot guides that can respond to a variety of needs. These include different types of emergency evacuation, as well as non-emergency situations involving mass movement of crowds, such as at parades, concerts, or other large public events. The project integrates research with educational activities through robot-centric education and short course development. To engage the younger generation with science and technology, the project will partner with a university educational center and a community college for various outreach activities.<br/><br/>The objective of the project is to investigate human-robot interaction in crowd dynamics, develop optimal feedback control to regulate human flow distribution, and design robot-assisted emergency evacuation algorithms. The research will advance the state-of-the-art in human-robot interaction, and fill a gap in robotics research by experimentally validating and measuring the interaction forces governing human-robot interaction in crowd dynamics. The proposed robot motion primitive design leads to new approaches for learning-based robot motion planning to efficiently engage humans. The project validates the use of dynamic robot guides in real human-robot interaction experiments in indoor environments. Simulation validation in benchmark environments such as shopping-malls and campus buildings will also be performed, and the efficiency of alternative robot-assisted evacuation strategies will be evaluated. While primarily for intelligent robots, the research results are anticipated to be cross-cutting and applicable to other areas such as transportation, communication, and control."
"1528110","NRI: Development of Autonomous Sub-Gram Flapping-Wing Artificial Flyers Using Novel Combustion-Driven SMA-Based Actuators","CMMI","National Robotics Initiative","09/01/2015","07/31/2015","Nestor Perez-Arancibia","CA","University of Southern California","Standard Grant","Jordan Berg","08/31/2018","$750,000.00","Paul Ronney","perezara@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","ENG","8013","030E, 031E, 032E, 033E, 034E, 6840, 8024, 8086, 9102","$0.00","This project addresses power and control questions central to achieving maneuverable, autonomous, untethered, insect-scale, flying robots. The amount of energy stored per unit mass in even the best small batteries is low enough that flight times would be limited to at most a few minutes. Furthermore, when the high rate at which the battery must supply energy is considered, it is questionable whether a winged microrobot could even lift its own weight. Instead, this project emulates flying insects in nature, which use animal fat as fuel, with an energy density about 100 times greater than state-of-the-art batteries. Accordingly, this project will demonstrate liquid-fuel catalytic combustion engines expected to be capable of over 90 minutes of powered flight. This result will be achieved through innovative integrated modeling, analysis, design, fabrication, and control of insect-scale aerodynamics, combustion, and flight. Broader impacts will arise from application of insect-scale flying robots to, for example, artificial pollination, search-and-rescue operations, and field biological research. Indirectly, this project will produce new methods for energy conversion, novel algorithms for control synthesis, and fabrication techniques, applicable to a wide gamut of wheeled, winged, and legged microrobots.<br/><br/>To accomplish the project goals, the research advances knowledge in three specific areas: (i) biologically inspired design and fabrication of aerodynamically efficient flapping-wing microflyers, where principles from nature are translated into robotic designs, employing a systems-and-control conceptual framework. In this framework, the interaction between aerodynamics, power, design, and controls is analyzed using tools such as input-output modeling and system identification; (ii) mechanical actuation using fuel-powered shape-memory-alloys-based mechanisms, where flameless catalytic combustion generates the heat required to induce material phase transitions, necessary for the production of mechanical work; (iii) control, which emerges naturally from the first two areas as new aerodynamically efficient designs require the invention of novel control strategies for stable flight and new techniques for controller synthesis. Similarly, new actuation technologies require the invention of new low-level, physically implementable controllers. In this case, the dynamics of the combustion-driven actuators and flapping mechanisms are nonlinear and time-varying, reason for which a significant part of the research effort is dedicated to the development and real-time implementation of novel robustly stable nonlinear and adaptive controllers."
"1637899","NRI: Receding Horizon Integrity-A New Navigation Safety Methodology for Co-Robotic Passenger Vehicles","CMMI","National Robotics Initiative","09/01/2016","08/08/2016","Matthew Spenko","IL","Illinois Institute of Technology","Standard Grant","Jordan Berg","08/31/2019","$899,926.00","Mathieu Joerger","mspenko@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","ENG","8013","8086","$0.00","The objective of this research is to ensure the integrity of vehicle position, heading, and velocity estimates that are used by self-driving cars as the basis for life-critical decisions such as the initiation and execution of hazard-avoidance maneuvers. Integrity, which is a measure of trust in a sensor's information, has been successfully implemented in commercial aircraft to guarantee the safety of maneuvers such as landing. This project addresses several obstacles in translating integrity from aviation applications to self-driving cars, including integrating the disparate sensor types used by ground vehicles; meeting the stringent demands of routine autonomous driving; accounting for the number, proximity, and high relative velocity of other vehicles on the road; and evaluating multiple, distinct, and mutually exclusive courses of action in a timely manner. Project subtasks include characterization of integrity for representative sensors, construction of appropriate models for uncertainty propagation, and experimental validation of the resulting integrity framework. The project will advance the larger research effort to realize the potential of self-driving cars for relieving congestion, reducing emissions, and saving lives. The work includes public outreach efforts on autonomous navigation for self-driving cars, which will build upon an ongoing relationship with Chicago's Museum of Science and Industry, including a hands-on demonstration during National Robotics Week to illustrate how safety can be ensured despite uncertainties related to sensor readings, vehicle dynamics, and the driving environment.<br/><br/>Specifically, this research will provide new experimental and analytical methods to quantify and prove self-driving car safety. The results of this work will create a high-level, sensor-independent, quantifiable metric that can be used to compare, evaluate, and certify safety across self-driving car manufacturers. Knowledge will be advanced in several previously-unexplored areas, including first-ever demonstrations of: 1) high-integrity sensor measurement error and fault models for non-GPS sensors, 2) analytical methods to quantify the safety risk of feature extraction and data association algorithms required in lidar, radar, and camera-based localization, 3) multi-sensor pose estimators and integrity monitors designed to evaluate the impact of undetected sensor faults on safety risk, and 4) rigorously derived and experimentally validated integrity risk prediction methods in dynamic environments."
"1208420","NRI-Small: Virtualized Welding: A New Paradigm for Intelligent Welding Robots in Unstructured Environment","CMMI","National Robotics Initiative","09/01/2012","08/29/2012","Ruigang Yang","KY","University of Kentucky Research Foundation","Standard Grant","Jordan Berg","08/31/2017","$800,000.00","Catherine Carswell, YuMing Zhang","ryang@cs.uky.edu","109 Kinkead Hall","Lexington","KY","405260001","8592579420","ENG","8013","7923, 8086, 9150","$0.00","This project is to develop a new robotic platform with novel 3D modeling and visualization algorithms. An existing ""dumb"" welding robot will be augmented with sensors to observe the work piece, as well as its surroundings. New algorithms will be developed to record and reconstruct in 3D the welding process. The reconstructed data are transmitted to a control room and visualized with augmented reality techniques: A skilled welder can look at the welding process from different angles, as if he/she was right next to the actual welding system. Welding parameters can be adjusted by the human (with intelligence) and executed by the robot (with precision). More importantly, the adjustment, together with the reconstructed welding process, will be recorded and analyzed. System modeling techniques will be developed to correlate the human adjustment with the 3D reconstruction of the welding process. In this way, a welding robot can ""learn by examples"" the knowledge and experiences of a human welder and make similar intelligent adjustments by itself in the future. <br/><br/>The primary use for this new technology is in manufacturing. Successful completion of the proposed project paves the foundation for intelligent welding robots with closed-loop intelligent control. Such a robotic system can perform high-speed and high-precision welding while allowing more variations in the work pieces and environments. In addition, virtualized welding can be integrated with a mobile platform to allow welding in places that are hazardous or unsuitable for human welders. The proposed welding extension platform will significantly expand the use of welding robots as well as reduce manufacturing costs. Under-represented students will be recruited to participate in the research through exiting institutional programs. Additional funding and industrial collaboration to transfer technology from research labs to industry will also be pursued."
"1207975","NRI-Small: Multifunctional Electroactive Polymers for Muscle-Like Actuation","IIS","National Robotics Initiative","09/01/2012","07/31/2012","Qibing Pei","CA","University of California-Los Angeles","Standard Grant","Jordan Berg","12/31/2015","$379,999.00","","qpei@seas.ucla.edu","11000 Kinross Avenue, Suite 211","LOS ANGELES","CA","900952000","3107940102","CSE","8013","7923, 8086","$0.00","This project aims to develop a new, bistable electroactive polymer that combines large actuation strain and energy density with variable stiffness and bistable deformation. The technical approach of the project involves: (1) synthesizing new polymers comprising interpenetrating polymers network to achieve stable, high-strain actuation; (2) investigating ultrathin carbon nanotube coatings for fault tolerance and enhanced operation reliability; (3) reducing the driving voltages of electroactive polymers to around 200 V by synthesizing new polymers with high dielectric permittivity as well as by developing processing techniques to produce high-quality polymer thin films; and (4) fabricating compact modular actuators that can be readily integrated into robotic systems. The potential transformative technical impact of this project is a radically new actuator material that can reproduce the structural, actuation, and sensing functions of muscles, and can be inserted into a broad range of robotic systems for locomotion and manipulation. <br/><br/>This project will develop a new actuator material based on a bistable electroactive polymer that behaves like an artificial muscle, and offers a combination of attributes that future robotic systems demand including power output that outperforms human skeletal muscle, flexibility, quietness, and biocompatibility. Actuators based on the new polymer material enable the design of robotic systems that interact with people, such as assistive prosthesis or assistive devices for people with disability, humanoid robots for elderly in-home care, and surgical robots to save lives. The material can also be used for industrial automation for increased production efficiency. This project includes significant outreach and educational activities. It will provide summer research intern opportunities for under-represented minority high school students each year. Undergraduate and graduate students will participate in the proposed research to gain hands-on research experience, as well as analytical, communication, and inter-personal skills."
"1527208","NRI: Collaborative Research: Task Dependent Semantic Modeling for Robot Perception","IIS","National Robotics Initiative","09/01/2015","08/12/2015","Jana Kosecka","VA","George Mason University","Standard Grant","Jie Yang","08/31/2018","$267,486.00","","kosecka@gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","8013","8086","$0.00","The research in this project enables robots to better deal with the complex cluttered environments around us, ranging from open scenes to cluttered table-top settings and to perform the basic mapping, navigation, object search so as to enable fetch and delivery tasks most commonly required in service co-robotics applications. The key contribution of the project is to develop visual perception systems for robots that can understand the semantic labels of the visual world at multiple levels of specificity as required by particular robot tasks or human-robot interaction. In addition, the project enables robot perception systems to better understand new, previously unseen, environments through automatically adapting existing learned models, and by actively choosing how to best explore and recognize novel visual spaces and objects. The datasets and benchmarks, as well as the developed models, form basis for more rapid progress on semantic visual perception for robotics.<br/><br/>The development of methodologies for learning compositional representations which enable active learning and efficient inference is a long standing problem in computer vision and robot perception. Guided by the constraints of indoors and outdoors environments, we plan to exploit large amounts of data, strong geometric and semantic priors and develop novel representations of objects and scenes. The developed representations are captured by compositional structured probabilistic models including deep convolutional networks. Doing this rapidly is required to support active visual exploration to improve semantic parsing of a space. Furthermore the project team collects and disseminates a large dataset of densely sampled RGBD imagery to support offline evaluation and benchmarking of active vision for semantic parsing. The project can result in advances in active hierarchical semantic vision for robot tasks including exploration, search, manipulation, programming by example, and generally for human-robot interaction."
"1527087","NRI: Collaborative Research: Dynamic Braces for Quantification and Treatment of Abnormal Curves in the Human Spine","CMMI","National Robotics Initiative","09/01/2015","08/11/2015","Sunil Agrawal","NY","Columbia University","Standard Grant","Jordan Berg","08/31/2018","$861,996.00","David Roye","Sunil.Agrawal@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","ENG","8013","030E, 031E, 8086, 032E, 033E, 034E","$0.00","Idiopathic scoliosis is a condition in which the spine develops a strong left/right curvature, forming a C- or S-shape instead of a straight line. Approximately 2% to 3% of adolescents suffer from the disorder, with about 1 in 500 required to wear corrective braces until skeletal maturity, and about 1 in 5,000 requiring spinal surgery. A typical scoliosis brace is worn around the trunk and hips, and completely immobilizes the upper body, which substantially degrades quality of life. This project will demonstrate a hybrid dynamic brace for correcting scoliosis, while minimally affecting the activities of daily living. Compliant passive braces tailored to the treatment needs of individual wearers allow greater freedom of movement, but cannot respond to changes in posture or more gradual evolution of the wearer's condition. Active braces provide dynamically responsive corrective forces, but require power-hungry motors, and greatly increase weight and complexity. This project will demonstrate a hybrid approach, providing freedom of movement and dynamic response, but without the weight and power requirements of fully active designs. The result is essentially a wearable robot that continually monitors and responds to the needs of the user.<br/><br/>This project will lay the scientific foundation for the design of dynamic brace co-robots, and the evaluation of their effectiveness for both quantification and treatment of the abnormal spine. These dynamic braces will be designed to modulate the corrective forces on the spine in desired directions while still allowing the users to perform typical activities of daily life. The project will investigate the hypothesis that dynamic braces have the potential to transform treatment in this field, as these can provide effective control of corrective forces on the spine both spatially and temporally. The scientific studies will characterize the spatial stiffness of the spine in a specific pose and during different functions. The studies will target treatment outcomes in subjects with abnormal spine. Furthermore, this project will train students in interdisciplinary research and will result in future workshops and courses appealing to engineers, clinicians, medical caregivers, and high school students, motivating careers in STEM."
"1527133","NRI: Collaborative Research: Dynamic Braces for Quantification and Treatment of Abnormal Curves in the Human Spine","CMMI","National Robotics Initiative","09/01/2015","08/11/2015","Charles Kim","PA","Bucknell University","Standard Grant","Jordan Berg","08/31/2018","$141,993.00","","cjk019@bucknell.edu","One Dent Drive","LEWISBURG","PA","178372111","5705773855","ENG","8013","030E, 031E, 032E, 033E, 034E, 8086","$0.00","Idiopathic scoliosis is a condition in which the spine develops a strong left/right curvature, forming a C- or S-shape instead of a straight line. Approximately 2% to 3% of adolescents suffer from the disorder, with about 1 in 500 required to wear corrective braces until skeletal maturity, and about 1 in 5,000 requiring spinal surgery. A typical scoliosis brace is worn around the trunk and hips, and completely immobilizes the upper body, which substantially degrades quality of life. This project will demonstrate a hybrid dynamic brace for correcting scoliosis, while minimally affecting the activities of daily living. Compliant passive braces tailored to the treatment needs of individual wearers allow greater freedom of movement, but cannot respond to changes in posture or more gradual evolution of the wearer's condition. Active braces provide dynamically responsive corrective forces, but require power-hungry motors, and greatly increase weight and complexity. This project will demonstrate a hybrid approach, providing freedom of movement and dynamic response, but without the weight and power requirements of fully active designs. The result is essentially a wearable robot that continually monitors and responds to the needs of the user.<br/><br/>This project will lay the scientific foundation for the design of dynamic brace co-robots, and the evaluation of their effectiveness for both quantification and treatment of the abnormal spine. These dynamic braces will be designed to modulate the corrective forces on the spine in desired directions while still allowing the users to perform typical activities of daily life. The project will investigate the hypothesis that dynamic braces have the potential to transform treatment in this field, as these can provide effective control of corrective forces on the spine both spatially and temporally. The scientific studies will characterize the spatial stiffness of the spine in a specific pose and during different functions. The studies will target treatment outcomes in subjects with abnormal spine. Furthermore, this project will train students in interdisciplinary research and will result in future workshops and courses appealing to engineers, clinicians, medical caregivers, and high school students, motivating careers in STEM."
"1208598","NRI-Small: The Intelligent Workcell - Enabling Robots and People to Work Together Safely in Manufacturing Environments","CMMI","National Robotics Initiative","10/01/2012","03/27/2013","Paul Rybski","PA","Carnegie-Mellon University","Standard Grant","Jordan Berg","09/30/2017","$690,000.00","Daniel Huber, Paul Rybski","prybski@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","ENG","8013","7923, 8086","$0.00","The research objective of this award is to investigate methods to enable people and industrial robots to work safely within the same workspace. Current robotic manufacturing practice requires the physical separation of people and robots, which ensures safety, but is inefficient in terms of time and resources, and limits the tasks suitable for robotic manufacturing. This research will develop an ""Intelligent Workcell,"" which augments the traditional robotic workcell with perception systems that observe workers within the workspace. Methods to explicitly track workers and estimate their body pose will enable dynamically adaptive safety zones surrounding the robot, thereby preventing the robot from injuring workers. Algorithms will be developed to recognize the activities that workers are performing. These algorithms will learn a task-independent vocabulary of fundamental action components, which will form the building blocks for a hierarchical activity recognition framework. Finally, mechanisms for providing feedback to workers about the robot's intended actions will be studied.<br/><br/>This research is expected to provide new capabilities in robotic workcell safety and monitoring, allowing people and industrial robots to work safely and effectively in the same environment. Such capabilities would improve the efficiency of existing robotic workcells, since the robot would not be required to stop whenever a person enters the workspace (as is current practice). Furthermore, new manufacturing processes that involve robots and people working together on a single task would be enabled. Students at the graduate and undergraduate level will benefit from using the prototype Intelligent Workcell in project courses, and grade-school students will participate in short courses and workshops designed to ignite interest in STEM activities related to industrial robotics and computer vision."
"1317913","NRI: Small: Compliant Multifunctional Robotic Structures for Safety and Communication by Touch","CMMI","National Robotics Initiative","09/01/2013","08/27/2013","Hugh Bruck","MD","University of Maryland College Park","Standard Grant","Jordan Berg","08/31/2017","$600,000.00","Miao Yu, Elisabeth Smela","bruck@eng.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","ENG","8013","7923, 8086, 9102","$0.00","The objective of this research is to enable better training of robots by enabling them to physically communicate via human touch using new compliant multifunctional structures. To achieve this, arrays of conducting polymers will be developed to form a system similar to the human nervous system that can sense shape and forces distributions. This sensor array will be integrated into composite foam structures using a scalable additive manufacturing process. To support development of models and to serve as proof-of-concept for these multifunctional structures on robotic platforms, simulated co-robotics experiments will be conducted using a robotic arm interacting with objects of varying compliance. Experimental details of the associated contact mechanics will be quantified in real-time using Digital Image Correlation and conventional video imaging. Output from the sensor array will then be related to shape and force distributions by solving the nonlinear inverse problem using a novel Singular Value Decomposition method. Research results will be documented and disseminated, and the experiments will be converted to STEM demonstrations targeted at educating young girls.<br/><br/>This research will lead to new compliant, scalable, sensing structures that simultaneously monitor in real-time both global and local shapes, as well as force distributions. Since compliant multifunctional sensing structures do not yet exist for robots, it is envisioned that the proposed work will enable realization of new bio-inspired control principles for training robots. This will significantly advance the ability to make safer interactions and decisions in co-robotics by differentiating robotic interactions with humans from other objects in their environment. The proposed integration of research and education will train new mechanical engineers to create multifunctional products that enable new products and new capabilities in existing products in critical areas such as healthcare. The new fabrication methods will enable these structures to be manufactured in the United States in a cost-competitive manner, increasing employment."
"1317961","NRI: Small: Additive Manufacturing of Soft Robot Components with Embedded Actuation and Sensing","CMMI","National Robotics Initiative","09/01/2013","08/24/2013","Adam Cohen","TX","Southern Methodist University","Standard Grant","Jordan Berg","08/31/2017","$640,000.00","Paul Krueger, Edmond Richer","alcohen@lyle.smu.edu","6425 BOAZ","Dallas","TX","752750302","2147682030","ENG","8013","030E, 031E, 7923, 8029, 8086, MANU","$0.00","The objective of this research is to develop a new 3D printing/additive manufacturing process that will allow the direct fabrication from digital designs of integrated, multi-material devices such as soft robots with embedded actuators, sensors, and circuitry. The focus of the research will be (1) to design, prototype, characterize, optimize, and demonstrate the new process, and (2) to design, model, fabricate, and test an electromagnetic actuator and capacitive sensor made using the process. A testbed ""robot printer"" will be developed, along with control software and process design files. The research will require a number of technical and scientific advancements to provide the desired capabilities, including co-deposition of multiple materials in multi-layered patterns and formation of low-resistance electrical junctions. <br/><br/><br/>If successful, the results of this research will lead to a new manufacturing process for soft robots, an emerging class of devices promising greater safety, better manipulation of delicate and irregular objects, the ability to squeeze through small openings in search and rescue operations, etc. The challenge of manufacturing soft robotic components that include a large number of distributed actuators, sensors, and associated circuitry can be economically approached by concurrent 3-D printing with two types of materials: one that is conductive for interconnects, and the other insulating and flexible. Today's 3-D printing works with one type or the other, but not both. Overcoming this limitation would be transformative and enable a wide range of sophisticated, active structures to be readily fabricated. Beyond robotics, applications of the research include active prosthetics, minimally-invasive surgical instruments, and smart implants, as well as stretchable and wearable electronics. Undergraduate and graduate students will be heavily involved, and a partnership with the Perot Museum of Nature and Science (Dallas, TX) to develop a semi-permanent display and live demonstrations will expose a wide public audience to 3-D printing and robotics."
"1426824","NRI/Collaborative Research: Models and Instruments for Integrating Effective Human-Robot Teams into Manufacturing","CMMI","National Robotics Initiative","09/01/2014","08/01/2014","Bilge Mutlu","WI","University of Wisconsin-Madison","Standard Grant","Bruce M. Kramer","08/31/2017","$589,959.00","","bilge@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","ENG","8013","082E, 092E, 1786, 8086, MANU","$0.00","Robots for application in collaborative manufacturing must perform manual work side-by-side with people. Such robots offer the flexibility to work on many different tasks and promise to transform manufacturing by improving the quality and efficiency of manual processes in small shops and in facilitates that manufacture highly customized products. However, in order to meet this promise, robots must be effectively integrated into existing manufacturing teams and practices. To enable this integration, this National Robotics Initiative (NRI) award supports fundamental research on the methods and instruments that manufacturing engineers will need to form effective human-robot teams based on task requirements and worker skills. These methods will also enable robots to adapt to changes in workflow to maximize safety and efficiency. The effective integration of collaborative robots into manufacturing promises improvements in many industries that have not yet benefited from robotic technology. Therefore, results from this research will contribute to the competitiveness of U.S. manufacturing and benefit the U.S. economy and society. The research will involve contributions from multiple disciplines, including robotics, human factors, computer science, and manufacturing, and by academic and industry collaborators. These collaborations will help the dissemination of research results into manufacturing organizations and the integration of research into undergraduate and graduate curriculum in engineering.<br/><br/>Advancements in robotics promise the use of collaborative robots that perform interdependent work with people in order to improve quality, efficiency, and safety in industrial manufacturing. However, integrating collaborative robots into these processes and ensuring their efficient operation pose significant research challenges, including the optimal allocation of work based on task requirements and constraints, the formation of human-robot teams, and the dynamic adaptation of teamwork to workflow changes. This research will address these research challenges, enabling the seamless integration of collaborative robots into these processes and achieving efficient and safe collaboration between human and robot workers. The research team will create novel methods for optimal allocation of tasks to human and robot workers based on task constraints and worker skills, design new tools that utilize these methods to facilitate workflow design for human-robot teams, and develop novel mechanisms that enable robots to more efficiently and safely collaborate with human workers in the planned manufacturing operations. These methods and instruments will be validated in real-world manufacturing operations and disseminated through industry workshops, engineering curricula, and a public outreach program."
"1427096","NRI: Using Multi-Robot Enabled Dexterous Locomotion to Search for Victims in Disaster Areas","CMMI","National Robotics Initiative","11/01/2014","07/18/2014","Ronald Fearing","CA","University of California-Berkeley","Standard Grant","Jordan Berg","10/31/2017","$1,200,000.00","Avideh Zakhor","ronf@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","ENG","8013","8086","$0.00","After a disaster, such as an earthquake or hurricane, victims are often trapped in voids that are survivable but too difficult or dangerous for rescuers to reach. Small robots that can crawl through narrow crevices could provide rescuers with valuable information about where to focus their efforts. Although individual small-scale robots have previously lacked the intelligence and mobility of larger robots, recent breakthroughs in robot locomotion, and compact computers (from cell-phone technology) will enable the development of distributed robotic teams of dozens of very low-cost, small-scale mobile robots coordinating with each other to overcome obstacles while rapidly making a map of an irregular environment. This National Robotics Initiative (NRI) award supports fundamental research in understanding how to design systems of multiple robots which can cooperate with each other to identify and then move over obstacles, while providing information to, and receiving overall guidance from, human operators. Success in this research project will bring society closer to having teams of mobile, disposable, search and rescue robots which can robustly locomote through uncertain environments and find survivors in disaster situations while removing risks posed to human and animal rescuers. In addition to graduate students, undergraduate students will gain firsthand experience in the design and control of millirobotic platforms, and robot designs will be made publically available for construction and simulation. To increase youth interest in science, technology, engineering and math, middle school students will be engaged in robot design and build activities.<br/><br/>This research project aims to understand how to combine enhanced mobility through dynamic robot cooperation with distributed low-cost mapping of irregular environments. To reduce cost and complexity, millirobots are typically minimally actuated and have limited mobility in complex terrain. Through dynamic robot cooperation, multiple robots can jointly traverse terrain features which can block single robots. Algorithms will be developed to flexibly surmount obstacles by combining forces from multiple robots and using robot body surfaces as temporary bridging or stepping elements, which should enable teams of millirobots to cooperatively locomote and explore areas efficiently, for example by finding, or if necessary, creating low-energy ingress paths. The research team will develop new techniques for joint 3D mapping using multiple small and medium robots size equipped with lightweight sensors. A research challenge is using compact (sub-5 gram) steerable lasers and image sensors to identify reference points, and then using low-bandwidth communication of the most salient features to higher level computation nodes to create a 3D global map. Exterior surface maps will be fused with interior maps to guide search planning towards the regions with highest probability of unexplored volumes. Human searchers can carry lightweight mapping gear (on the order of 100 grams) while working on exterior surfaces, and provide information regarding the search regions. The robot ensemble will be tested at a local urban search and rescue training facility."
"1427111","NRI/Collaborative Research: Improving the Safety and Agility of Robotic Flight with Bat-Inspired Flexible-Winged Robots","CMMI","National Robotics Initiative","08/01/2014","07/25/2014","Seth Hutchinson","IL","University of Illinois at Urbana-Champaign","Standard Grant","Jordan Berg","07/31/2017","$1,500,000.00","Soon-Jo Chung, Timothy Bretl, Mani Golparvar-Fard","seth@uiuc.edu","SUITE A","CHAMPAIGN","IL","618207473","2173332187","ENG","8013","8086","$0.00","Bat flight, perhaps the most advanced and efficient form of animal flight, has long been a source of inspiration for roboticists and biologists alike. This National Robotics Initiative (NRI) collaborative research award supports research aimed at understanding and reproducing the unparalleled agility and resilience of bat flight. Biological studies of bats (their structure, muscle movement, and flight dynamics) will drive the engineering development of mathematical models of robotic flight and the eventual design and implementation of a prototype 30-80cm bat-like robot. The physical flight capabilities of the robot will be augmented with perception and reasoning abilities, with the aim of providing support for construction site activities such as site monitoring, inspection, and general surveillance of the work site to provide image data to enhance situational awareness of human workers. The research involves several disciplines, including biology, aerodynamics, robotics, control systems engineering, and construction engineering.<br/><br/>Aerial robots have nowhere near the agility and efficiency of animal flight, especially in complex, constrained environments. This is not surprising since even the simplest winged robots have complex flight dynamics that pose significant challenges for modeling, design, and control. In the case of bat-inspired robots, these difficulties are exacerbated by the use of under-actuated mechanisms driving wings constructed from flexible membranes. This project will combine biological and engineering research to address these problems. Biological research on the kinematics of bats and their flight will provide a basis for mechanical designs. To control the robot, agile motion planning and flight control algorithms will employ motion primitives that are derived from biological investigation of the dynamics of bat flight. Conversely, models obtained from biological studies will be validated by experimental investigations using the prototype robot, enabling iterative refinement of reduced-order models and control algorithms. Ultimately, the robots will be equipped with sensing systems and planning algorithms, to facilitate localization, mapping, inspection and surveillance at construction sites."
"1637656","NRI: Shape Morphing Arm Robotic (SMART) Manipulators for Simultaneous Safe Human-Robot Interaction and High Performance in Manufacturing","CMMI","National Robotics Initiative","09/01/2016","08/09/2016","Hai-Jun Su","OH","Ohio State University","Standard Grant","Bruce M. Kramer","08/31/2019","$977,778.00","Marcelo Dapino, Junmin Wang","su.298@osu.edu","Office of Sponsored Programs","Columbus","OH","432101016","6146888735","ENG","8013","082E, 6840, 7632, 8086, MANU","$0.00","Co-robots are robotic devices that work in collaboration with human partners. The current solutions for human-safe co-robots fail to offer the safety required in many manufacturing tasks. The shape morphing robotic manipulators are designed to be inherently safe by making the links flexible during robot motion. The upper and lower arms of the manipulators can change their stiffness in real-time by simple smart material actuators. The arms are relatively stiff at low speeds for maximum performance and highly flexible at high speeds for maximum safety. When a collision occurs, the flexible link deflects to limit the impact to the human operator. At low speeds, the flexible link is morphed to the high stiffness mode for maximum positioning accuracy. This research will significantly improve the design of safe co-robotic systems, which can benefit numerous fields, including the health care, automotive, construction and military sectors. It will help to reduce injuries in manufacturing industries and home/hospital nursing and improve efficiency of housekeeping. <br/><br/>Safety concerns with industrial robots present a serious technical barrier to practical co-robot applications. To address these safety challenges, this research offers a comprehensive solution by integrating complementary expertise in three areas: (i) shape morphing and design optimization of compliant mechanisms, (ii) electrically-controlled stiffness modulation with smart materials and (iii) performance maximization by optimal motion control. The shape morphing robotic manipulators can adapt their stiffness to the traveling speed by morphing their shape. They are designed for safe operation while maintaining their performance via shape morphing control and trajectory motion control. The shape morphing robotic manipulator offers several advantages over existing techniques in co-robots. It has the potential to create a paradigm shift towards ""safe by design, performance by control"" for human-safe co-robots."
"1536136","NRI: Small: EEG and EMG Human Model-Based Adaptive Control of a Dexterous Artificial Hand","CMMI","National Robotics Initiative","04/01/2015","05/18/2015","Erik Engeberg","FL","Florida Atlantic University","Standard Grant","Jordan Berg","08/31/2017","$161,782.00","","eengeberg@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","ENG","8013","7923, 8086","$0.00","The research goal of this award is to explore different methods for upper limb amputees to control dexterous artificial hands with brain waves. Biomedical signal processing techniques will be developed to enable this with a single, small recording electrode placed noninvasively on the amputees' heads. The recorded brain waves will be wirelessly transmitted to the hand in real time. A top-level controller will be developed to interpret the intent of the amputees while a low-level controller will be used to synchronize the dexterous grasp motions of the artificial hand. Algorithms will also be developed using tactile feedback from the fingertips to automatically prevent grasped objects from being accidentally dropped when they are transported or disturbed. Amputees will participate in a study to compare the newly developed artificial hand control techniques with brain waves to conventional control techniques with muscle signals during common tasks of daily life.<br/><br/><br/>If successful, this research will result in a noninvasive and economic method for amputees to control a dexterous artificial hand with brain waves. This could substantially improve the functionality of prosthetic hands for many amputees. The use of minimal hardware will facilitate the clinical adoption of this technique and the autonomous low-level control algorithms will produce a brain machine interface that places a low cognitive burden on the operator. This research can also be readily applied to benefit many disabled people including stroke victims and quadriplegics and can positively impact other areas of robotics such as improvised explosive disarmament, underwater and space exploration, and rescue robotics. Underrepresented engineering students will benefit from being included in this research plan. Additionally, undergraduate and graduate engineering students will benefit from newly developed classes and laboratory exercises resulting from this research."
"1526367","NRI: Collaborative Research: Task Dependent Semantic Modeling for Robot Perception","IIS","National Robotics Initiative","09/01/2015","08/12/2015","Alexander Berg","NC","University of North Carolina at Chapel Hill","Standard Grant","Jie Yang","08/31/2018","$269,480.00","","aberg@cs.unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","8013","8086","$0.00","The research in this project enables robots to better deal with the complex cluttered environments around us, ranging from open scenes to cluttered table-top settings and to perform the basic mapping, navigation, object search so as to enable fetch and delivery tasks most commonly required in service co-robotics applications. The key contribution of the project is to develop visual perception systems for robots that can understand the semantic labels of the visual world at multiple levels of specificity as required by particular robot tasks or human-robot interaction. In addition, the project enables robot perception systems to better understand new, previously unseen, environments through automatically adapting existing learned models, and by actively choosing how to best explore and recognize novel visual spaces and objects. The datasets and benchmarks, as well as the developed models, form basis for more rapid progress on semantic visual perception for robotics.<br/><br/>The development of methodologies for learning compositional representations which enable active learning and efficient inference is a long standing problem in computer vision and robot perception. Guided by the constraints of indoors and outdoors environments, we plan to exploit large amounts of data, strong geometric and semantic priors and develop novel representations of objects and scenes. The developed representations are captured by compositional structured probabilistic models including deep convolutional networks. Doing this rapidly is required to support active visual exploration to improve semantic parsing of a space. Furthermore the project team collects and disseminates a large dataset of densely sampled RGBD imagery to support offline evaluation and benchmarking of active vision for semantic parsing. The project can result in advances in active hierarchical semantic vision for robot tasks including exploration, search, manipulation, programming by example, and generally for human-robot interaction."
"1208468","NRI-Small: Collaborative Research: A Dynamic Bayesian Approach to Real-Time Estimation and Filtering in Grasp Acquisition and Other Contact Tasks","IIS","National Robotics Initiative","09/01/2012","11/05/2013","Jeffrey Trinkle","NY","Rensselaer Polytechnic Institute","Standard Grant","Ephraim P. Glinert","12/31/2016","$367,703.00","","jtrinkle@nsf.gov","110 8TH ST","Troy","NY","121803522","5182766000","CSE","8013","7923, 8086, 9251","$0.00","Robots cannot currently grasp objects or perform other contact tasks in unstructured environments with speed or reliability. This project is developing techniques for accurate real-time perception in support of contact tasks. In the proposed method, sensor data tracks the continuous motions of manipulated objects, while models of the objects are simultaneously updated. Particle filtering, a kind of Monte-Carlo simulation, ensures consistency of this tracking and updating.<br/><br/>The strongest impact of this work will be in robotic grasping and manipulation. Because of the synthesis of modeling and probabilistic inference, further impacts can be expected, for example in real-time haptics for telepresence."
"1317462","NRI: Small: Collaborative Research: Adaptive Motion Planning and Decision-Making for Human-Robot Collaboration in Manufacturing","IIS","COLLABORATIVE RESEARCH, IIS SPECIAL PROJECTS, National Robotics Initiative","09/15/2013","04/30/2015","Dmitry Berenson","MA","Worcester Polytechnic Institute","Standard Grant","Reid Simmons","10/31/2016","$337,878.00","","berenson@eecs.umich.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","7298, 7484, 8013","5918, 7923, 8086, 9251","$0.00","This project addresses manufacturing tasks that cannot be fully automated because of either the limitations of current algorithms or prohibitive cost and set-up time. Such tasks generally require workers to collaborate in close proximity and adapt to each other's decisions and motions. This project explores accomplishing these tasks through human-robot collaboration. Recent hardware developments in robotics have made human-robot collaboration physically possible, but robots still require new algorithms to ensure safety, efficiency, and fluency when working with people. Creating such algorithms is difficult because there can be high uncertainty in what a person is going to do and how they are going to do it. This project explores the integration of reasoning about how a person moves and how he or she makes decisions into a robot motion planning and decision-making framework. The research centers on the development of new algorithmic frameworks for modeling, simulating, and planning for human-robot collaboration, which requires advances in robot training, task modeling, human motion understanding, high-dimensional motion planning with uncertainty, and metrics to assess human-robot joint action. The results of this project have the potential to signi&#64257;cantly improve American competitiveness in manufacturing; especially for small-batch manufacturing and burst production, where the cost and set-up time of fully-autonomous solutions is prohibitive. The work will be disseminated in research papers and integrated into curricula. The project is guided by an advisory board from the manufacturing industry, which provides another avenue for dissemination."
"1451427","Valkyrie Robot Prized Build","IIS","National Robotics Initiative","08/01/2014","09/15/2016","Reginald Berka","TX","NASA Johnson Space Center","Interagency Agreement","Reid Simmons","09/30/2016","$2,000,000.00","","reginald.b.berka@nasa.gov","NASA JSC","Houston","TX","770583696","2812445561","CSE","8013","170E, 8086","$0.00",""
"1658635","NRI: Small: Collaborative Research: Adaptive Motion Planning and Decision-Making for Human-Robot Collaboration in Manufacturing","IIS","COLLABORATIVE RESEARCH, IIS SPECIAL PROJECTS, National Robotics Initiative","07/01/2016","09/15/2016","Dmitry Berenson","MI","University of Michigan Ann Arbor","Standard Grant","Reid Simmons","08/31/2017","$167,016.00","","berenson@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7298, 7484, 8013","5918, 7923, 8086","$0.00","This project addresses manufacturing tasks that cannot be fully automated because of either the limitations of current algorithms or prohibitive cost and set-up time. Such tasks generally require workers to collaborate in close proximity and adapt to each other's decisions and motions. This project explores accomplishing these tasks through human-robot collaboration. Recent hardware developments in robotics have made human-robot collaboration physically possible, but robots still require new algorithms to ensure safety, efficiency, and fluency when working with people. Creating such algorithms is difficult because there can be high uncertainty in what a person is going to do and how they are going to do it. This project explores the integration of reasoning about how a person moves and how he or she makes decisions into a robot motion planning and decision-making framework. The research centers on the development of new algorithmic frameworks for modeling, simulating, and planning for human-robot collaboration, which requires advances in robot training, task modeling, human motion understanding, high-dimensional motion planning with uncertainty, and metrics to assess human-robot joint action. The results of this project have the potential to signi&#64257;cantly improve American competitiveness in manufacturing; especially for small-batch manufacturing and burst production, where the cost and set-up time of fully-autonomous solutions is prohibitive. The work will be disseminated in research papers and integrated into curricula. The project is guided by an advisory board from the manufacturing industry, which provides another avenue for dissemination."
"1638070","NRI: Liquid Handling Robots - A New Paradigm for STEM Education","IIS","ITEST, National Robotics Initiative","10/01/2016","09/13/2016","Hans Riedel-Kruse","CA","Stanford University","Standard Grant","David Haury","09/30/2019","$885,230.00","Mark Miller, Paulo Blikstein","ingmar@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","7227, 8013","8086","$0.00","This National Robotics Initiative project will test a series of liquid handling robots in school biology and chemistry classes to determine the range of learning opportunities that can be supported through the instructional use of collaborative robots. Low-cost robots using the Lego Mindstorms platform will be used to implement classroom activities ranging from artistic expression using colorful arrangements of liquids to performing experiments using dilution series, density gradients, and spectral measurements. The aim is to make biotechnology more tangible and relevant to students while supporting interdisciplinary learning as recommended by the Next Generation Science Standards (NGSS). This innovative approach to engaging students in biology and chemistry will be tested with teachers and students in grades 6-12, with a range of user studies being employed to examine learning outcomes and guide the development process. The goal is to integrate liquid handling into educational robotics to enhance current science curricula by enabling deeper inquiry, more variety in learning experiences, and increased attention to interdisciplinary and project-based education.<br/><br/>The research of this project will focus on two themes: students' sense making as they engage in inquiry activities using the platform, and the pedagogical and infrastructural support needed for sustainable deployment and implementation. Multimodal data collected from students running experiments will be combined with traditional qualitative and quantitative methods to answer three primary questions related to the two research themes: 1) How effectively does the system capture the practices and inquiry activities of real scientists using similar tools? 2) How do the affordances identified by the first question map onto the learning goals of engaging in extended inquiry cycles within the context of limited amounts of time available to 6-12 grade science teachers? And 3) What implementation challenges are associated with our proposed curricular distribution model which relies on software and instructions being downloaded and fabricated in local Makerspaces or Lego/Arduino kits being used in schools? The research plan for the project will progress over three years from Microgenetic design and testing during the first year to controlled study of in-class effectiveness during year 2. In the final year of the project, teacher-led in-class effectiveness will be examined."
"1427030","NRI: Collaborative Research: Modeling and Verification of Language-based Interaction","IIS","National Robotics Initiative","08/15/2014","08/18/2014","Mark Campbell","NY","Cornell University","Standard Grant","Reid Simmons","07/31/2017","$700,000.00","Hadas Kress Gazit","mc288@cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","8013","8086","$0.00","Many autonomous systems today, such as personal or service robots, are designed primarily to perform tasks independently and in isolation. Integrating these robots with human partners can often result in poor performance, as the robot does not know how to interpret human interaction, and cannot merge information from this interaction with a model that guarantees robot performance. This research brings together key elements that are just now reaching a sufficient level of maturity for integration: firstly, natural language processing and probabilistic modeling to capture human input, and secondly probabilistic synthesis and verification of the combined human-robot systems to ensure correct performance. The outcome will be theory and software to enable correct, effective and natural interactions between robots and humans to be realized. This research will impact most future autonomous systems which require interactions with humans, including service, personal and planetary robots. <br/><br/>The goal of this research is to develop models and algorithms for synthesizing and verifying an integrated human-plus-robot system based on natural language interaction. Algorithms are being developed for probabilistic modeling and inference of natural language, including the grounding of the constituents of the language into the physical world and the human's expectations. These models will enable the development of a distribution over specifications for control synthesis, which will in turn enable the development and verification of correct-by-construction controllers to a particular level of probability. The out years will consider interactive human-robot dialogue to resolve conflicts, and ""open world"" scenarios to enable on-line learning of new models over time. It is expected that this research will enable high reliability and performance in many autonomous systems because of the inherent interaction with humans. Outcomes include open source data and software; community workshops; and undergraduate and graduate student education in the unique area of language, modeling and verification for robotics."
"1317702","NRI: Small: Dynamic Locomotion: From Humans to Robots via Optimal Control","IIS","National Robotics Initiative","09/01/2013","08/26/2013","Emanuel Todorov","WA","University of Washington","Standard Grant","Jie Yang","08/31/2017","$1,217,726.00","Zoran Popovic","todorov@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8013","7923, 8086, 1653","$0.00","The objective of this research is to develop algorithms that can make robots and simulated characters move like humans. A range of dynamic locomotion tasks including walking, running, getting up and climbing, as well as task variations such as walking backwards, and concurrent tasks such as holding a cup of water while walking, will be studied. The approach is based on optimal control theory. Human movements will be analyzed, and the performance criteria with respect to which they are optimal will be identified. Algorithms that optimize the same performance criteria will then be developed.<br/><br/>Intellectual merit: Movement analysis will be based on a new mathematical framework where inference of performance criteria from observed movements becomes a convex optimization problem. Control synthesis will exploit new algorithms for real-time optimization which are able to plan long movement sequences involving multiple contact events. These algorithms rely on novel formulations of the physics of contact which are more amenable to numerical optimization, as well as a new physics simulator which exploits advances in parallel processing.<br/><br/>Broader impact: This research will change how robots and simulated characters move. Currently many robotic control systems with the appearance of dynamic movements are controlled in open loop, or are designed to execute one specific task. This work will enable robots to express more natural and versatile movements, as well as make robot programming more automated. The resulting controllers will also serve as models for human motor control."
"1317849","NRI: Small: Collaborative Research: Rethinking Motion Generation for Robots Operating in Human Workspaces","IIS","National Robotics Initiative","09/01/2013","06/10/2016","Lydia Kavraki","TX","William Marsh Rice University","Standard Grant","Reid Simmons","08/31/2017","$915,140.00","Mark Moll","kavraki@cs.rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","8013","7923, 8086, 9251","$0.00","This proposal develops a new motion planning paradigm for enabling robots to work in the presence of humans or cooperatively with humans as co-workers. The paradigm is intended to be fast, reactive, and responsive to the requirements that arise from human-robot interaction. It involves the definition and subsequent implementation of constraints that encode properties of human-aware paths and can be translated to cost functions characterizing path quality. New motion planners are proposed. The operation of these planners is guided by constraints and their corresponding cost functions. The planners produce paths compatible with a given set of constraints. A mechanism to incorporate user feedback on the relative importance of constraints is provided and semi-autonomous operation of the robots is considered. Importantly, the planners are embedded in a novel hybrid-systems framework that allows a robot to automatically switch among planners, and hence behaviors, in order to take into account different constraints and user preferences that arise in the context of semi-autonomous operation. Besides the actual implementation of the planners on specific platforms, this project also disseminates all developed libraries and planners. Compatibility will the Robot Operating System (ROS) is provided for wide adoption, while tutorials at major conferences are planned. The training of graduate students and female undergraduate students are a central focus of this proposal."
"1638073","NRI: Guiding with Touch: Haptic Cueing of Surgical Techniques on Virtual and Robotic Platforms","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","01/01/2017","08/30/2016","Marcia O'Malley","TX","William Marsh Rice University","Standard Grant","Reid Simmons","12/31/2019","$1,000,000.00","Michael Byrne","omalleym@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","7495, 8013","8086","$0.00","Virtual reality has enabled surgeons to train on procedures without risk to patients. However, the feedback surgical trainees receive is delayed, subjective, and qualitative, thereby lacking support for rapid acquisition of skill. This project will enhance surgical performance and training by providing performance feedback using touch-based cues that convey movement quality and strategies rather than task outcomes like procedure time. This will allow trainees to get feedback that is immediate and quantitative, and result in improved performance in difficult-to-train motor skills. Should this research prove successful, there is an opportunity to make meaningful changes in how surgeons are trained. <br/><br/>Co-robots in the endovascular surgical domain can take several forms, from a virtual reality simulator that mimics patient-specific procedures, enabling high-fidelity procedural rehearsal, to telesurgical systems that are used to navigate flexible robotic tools to anatomical locations. Such systems require significant skill to use and, in turn, a rigorous training protocol before certification to operate with the robot is granted. To date, training protocols rely on practice and subjective feedback by a skilled observer, and acquisition of skill on these systems can be inefficient. Furthermore, there is a lack of objective metrics of skill acquisition. This project will investigate the use of haptic feedback to trainees based on motion-based performance metrics, and will evaluate the potential role of this feedback modality during performance and training for simulated and robotic endovascular surgical tasks. In particular, the project will evaluate the benefit of providing specific performance feedback that highlights task strategies in the motion space, and the effect of providing such directed feedback on learning and retention of the task."
"1649302","Doctoral Consortium at the 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2016)","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","08/01/2016","08/11/2016","Marcia O'Malley","TX","William Marsh Rice University","Standard Grant","Reid Simmons","07/31/2017","$25,000.00","","omalleym@rice.edu","6100 MAIN ST","Houston","TX","770051827","7133484820","CSE","7495, 8013","7495, 7556, 8086","$0.00","This proposal will support U.S. Ph.D. students working in robotics the opportunity to share their knowledge and interact with each other and more senior researchers, to learn about different sub-fields within robotics, to meet potential employers, and to become apprised of new technology that will be demonstrated by vendors. This goal will be accomplished by partially supporting the travel costs for U.S. Ph.D. students to attend the International Conference on Intelligent Robots and Systems (IROS), which is one of two major international robotics conferences co-sponsored by the IEEE Robotics and Automation Society. The conference, which in 2016 will be held in Daejeon DCC, South Korea, attracts an international crowd that includes academics, industry workers, entrepreneurs, and funding agency leaders."
"1208522","NRI-Small: A Biologically Plausible Architecture for Robotic Vision","IIS","National Robotics Initiative","09/01/2012","08/27/2012","Nuno Vasconcelos","CA","University of California-San Diego","Standard Grant","Reid Simmons","08/31/2017","$1,150,000.00","","nuno@ece.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8013","1653, 7923, 8086","$0.00","The objective of this research is to develop a generic robotic vision architecture that is both biologically plausible and jointly optimal, in a decision theoretic sense, for attention, object tracking, object recognition, and action recognition, in both static and dynamic environments. The research is motivated by the observation that all these problems are solved by biological vision with very homogeneous neural computations. The approach is to exploit a mapping of accepted computational models of visual cortex into the elementary computations of statistical learning and inference in order to derive unified algorithms for all tasks. <br/><br/>Intellectual merit: the proposed unification of vision tasks is novel and of paramount importance for robotics, since it is computationally infeasible for a robot to implement a large set of disjoint vision algorithms. It will also exploit task synergies, producing algorithms that leverage the solution of one task to improve performance on another. This will likely enable overall better performance of vision systems. Finally, the project will produce novel insights on the structure of the visual world, and how it can be leveraged by robotic vision, by introducing new models for natural image statistics. <br/><br/>Broader impacts: The research has applicability in manufacturing, intelligent systems, health care, homeland security, etc. The expected theoretical insights are likely to be of wide application in statistics (models of feature dependence), neuroscience (models of neural computation), and computer vision (synergistic models). Educationally, the project provides an exciting opportunity for the involvement of undergraduates in research."
"1524544","NRI: Robotic Tool-Use for Cleaning","IIS","National Robotics Initiative","09/01/2015","07/16/2015","Maya Cakmak","WA","University of Washington","Standard Grant","Tatiana D. Korelsky","08/31/2018","$400,063.00","","mcakmak@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8013","8086","$0.00","Cleaning is as one of the most desired capabilities for personal robots, according to reported surveys and interviews conducted with potential users. However, successful solutions to robotic cleaning have so far been special-purpose robots designed for particular cleaning tasks, as with the vacuuming or mopping robots. Instead, this project aims to make general-purpose mobile manipulators perform a much wider range of cleaning tasks, including dusting, wiping, scrubbing, sweeping, or mopping, by enabling them to use human tools. Cleaning is a key activity of daily living and an important task in the maintenance of one's quality of life. Many older adults lose their independence when they can no longer carry out cleaning tasks. Similarly, many mobility-impaired individuals rely on others for cleaning. Robots taking on these tasks could therefore positively impact millions of individuals. General-purpose cleaning robots could also have significant economic impact, similar to those of special-purpose ones like the Roomba. Cleanliness is critical in many commercial facilities, such as hotels or department stores, which involve large surfaces that need to be regularly cleaned in specific ways that programmable cleaning robots developed in this project could handle.<br/><br/>General-purpose cleaning robots need to be able to use many different cleaning tools, in many different environments, based on many different user preferences. The diversity of possible situations makes it difficult to develop universal controllers or planners for arbitrary cleaning tasks. Furthermore, it is impossible to know the users' preferences in advance. To address these challenges, this project develops a Learning from Demonstration (LfD) technique that allows a robot to learn cleaning actions from human demonstrations. This technique exploits the common structure of cleaning tasks: they all involve moving a cleaning tool relative to a target surface with a certain pattern while applying a certain force on the surface. Although many LfD approaches already exist, none of them allow learning cleaning actions that generalize to arbitrary surfaces with arbitrary cleaning patterns. This project contributes new representations and learning algorithms that enables a mobile manipulator to clean surfaces of arbitrary size, shape, curvature, dirt distribution, and clutter, using different types of cleaning tools. The methods developed in the project are evaluated with real cleaning tasks through systematic experiments and user studies in the lab, as well as, in the real world through an identical robot deployed in a private home."
"1514395","RI: Medium: Collaborative Research: Novel microLIDAR Design and Sensing Algorithms for Flapping-Wing Micro-Aerial Vehicles","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","08/01/2015","06/20/2016","Karthik Dantu","NY","SUNY at Buffalo","Continuing grant","Jeffrey Trinkle","07/31/2018","$228,195.00","","kdantu@buffalo.edu","520 Lee Entrance","Amherst","NY","142282567","7166452634","CSE","7495, 8013","7495, 7924, 8086, 9251","$0.00","This project makes it possible for a tiny robotic bee to sense its distance to any nearby object. Such depth sensing for a small robot insect pushes the limits of sensor and algorithm design in terms size, weight, computing, and power. The key idea is joint design; every part of the robotic insect is optimized together, from wing design and optics to intelligent algorithms and efficient computation. This is possible by inter-disciplinary work across scientists and engineers from diverse backgrounds. The lessons learned through this project can be applied to transform other applications that involve small devices including medical sensors and endoscope imaging, smart homes and the internet of things, agricultural and industrial monitoring systems, and mobile vision for search and rescue.<br/><br/>Lidar sensing has enabled large robotic cars to navigate complex environments. This proposal introduces designs for ""micro-lidar"" that can be used on insect-scale aerial robots. Making micro-lidar work on small platforms involves four intertwined research thrusts. The first thrust uses MEMS mirrors and wide-angle optics to sense and modulate the laser pulses. The second thrust is adapting signal processing algorithms to estimate range data at this scale. The third thrust is developing novel perception and navigation algorithms to map the indoor environments using a micro-aerial vehicle. The fourth thrust is to improve robotic insect flight to allow novel manipulations that require knowledge of the surrounding range map. The utility of these sensors will be demonstrated on the robobee for novel maneuvers and building topo-feature maps of indoor environments."
"1328930","NRI: Large: Collaborative Research: Fast and Accurate Infrastructure Modeling and Inspection with Low-Flying Robots","IIS","National Robotics Initiative","09/15/2013","04/28/2016","Sanjiv Singh","PA","Carnegie-Mellon University","Continuing grant","Jie Yang","08/31/2017","$2,004,750.00","Daniel Huber, Sebastian Scherer, Burcu Akinci","ssingh@realearth.us","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","7925, 8086, 9251","$0.00","The goal of this project is to transform the efficiency, fidelity, and safety of current critical infrastructure inspection methods by combining human judgment with machine intelligence through the development of an autonomous robotic inspection assistant. The proposed work utilizes small aerial robots, coupled with three-dimensional imaging and the state-of-the-art in planning, modeling, and analysis to develop safe and efficient, high-precision assessment of structures. The key themes of the proposed work are: (1) rapid infrastructure modeling and analysis of large complex structures via a small autonomous aerial robot with 3D mapping capabilities; (2) immersive inspection and structural assessment to combine shape and appearance into an integrated representation amenable to structural health evaluation by an inspector; and (3) adaptive aerial vehicle motion plans that seek to learn from the experience of human inspectors and facilitate as autonomous inspection assistants. The proposed work is exploring the role of humans in the entire cycle from deployment of flying robots to registering data to the assessment. This project brings together members of participating communities and is developing curriculum to engage undergraduate and graduate students from robotics and civil engineering in the proposed research."
"1637443","NRI: Collaborative Research: Learning Deep Sensorimotor Policies for Shared Autonomy","IIS","National Robotics Initiative","09/01/2016","09/08/2016","Sergey Levine","WA","University of Washington","Standard Grant","Hector Munoz-Avila","08/31/2019","$500,000.00","","sergey.levine@gmail.com","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8013","8086","$0.00","Assistive robots have the potential to transform the lives of persons with upper extremity disabilities, by helping them perform basic daily activities, such as manipulating objects and feeding. However, human control of assistive robots presents substantial challenges. The high dimensionality of robotic arms means that joystick-like interfaces are unnatural hard to use intuitively, and motions resulting from direct teleoperation are often slow, imprecise, and severely limited in their dexterity. This research address these challenges by developing learning algorithms for shared autonomy, where the robot anticipates the user's intent and provides a degree of assistive autonomy to ensure fluid and successful motions. This research will also pave the way for future research that can bootstrap from teleoperation and build towards full robot autonomy. <br/><br/>The research proposes a hierarchical and multi-phased approach to shared autonomy, using techniques from deep learning and reinforcement learning. The system begins by using deep inverse reinforcement learning to quickly ascertain the user's high-level goal, such as whether the user wants to grasp a particular object or operate an appliance, from raw sensory inputs. This goal inference layer supplies objectives to the lower control layer, which consists of deep neural network control policies that can directly process raw sensory input about the environment and the user to make decisions. These policies choose low-level controls to satisfy the high-level objective while minimizing disagreement with the user's commands. The algorithms will be deployed and tested on a wheelchair-mounted robot arm with the potential to assist users with upper extremity disabilities to perform activities of daily living."
"1637748","NRI: Collaborative Research: Learning Deep Sensorimotor Policies for Shared Autonomy","IIS","National Robotics Initiative","09/01/2016","09/08/2016","Siddhartha Srinivasa","PA","Carnegie-Mellon University","Standard Grant","Hector Munoz-Avila","08/31/2019","$499,792.00","","siddh@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","8086","$0.00","Assistive robots have the potential to transform the lives of persons with upper extremity disabilities, by helping them perform basic daily activities, such as manipulating objects and feeding. However, human control of assistive robots presents substantial challenges. The high dimensionality of robotic arms means that joystick-like interfaces are unnatural hard to use intuitively, and motions resulting from direct teleoperation are often slow, imprecise, and severely limited in their dexterity. This research address these challenges by developing learning algorithms for shared autonomy, where the robot anticipates the user's intent and provides a degree of assistive autonomy to ensure fluid and successful motions. This research will also pave the way for future research that can bootstrap from teleoperation and build towards full robot autonomy. <br/><br/>The research proposes a hierarchical and multi-phased approach to shared autonomy, using techniques from deep learning and reinforcement learning. The system begins by using deep inverse reinforcement learning to quickly ascertain the user's high-level goal, such as whether the user wants to grasp a particular object or operate an appliance, from raw sensory inputs. This goal inference layer supplies objectives to the lower control layer, which consists of deep neural network control policies that can directly process raw sensory input about the environment and the user to make decisions. These policies choose low-level controls to satisfy the high-level objective while minimizing disagreement with the user's commands. The algorithms will be deployed and tested on a wheelchair-mounted robot arm with the potential to assist users with upper extremity disabilities to perform activities of daily living."
"1525251","NRI: Rich Task Perception for Programming by Demonstration","IIS","National Robotics Initiative","09/01/2015","08/17/2015","Dieter Fox","WA","University of Washington","Standard Grant","Jie Yang","08/31/2018","$1,200,000.00","Maya Cakmak, Luke Zettlemoyer","fox@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8013","8086","$0.00","Robots that can work alongside humans and take on repetitive, time-consuming tasks could greatly improve productivity and reliability in task-oriented environments such as laboratories, manufacturing facilities, or commercial kitchens. One of the key challenges in realizing this vision is that every combination of environment, user, and task presents unique requirements for the robot's behavior and it is impractical to employ traditional approaches for programming these robots. Instead, the PIs envision robots that are programmable by their end-users in their particular operation environment and for the particular tasks they are needed for. To overcome limitations of existing approaches, the PIs propose to develop a framework for rich task perception, which is able to extract detailed task descriptions from intuitive human demonstrations. Building on recent advances in depth camera sensing, GPU-optimized visual processing, and language understanding, the proposed framework will track all objects and people in a scene, recognize their goals and task context, and parse speech to extract higher-level task structure from a demonstration. The PIs will also introduce new programming by demonstration techniques that take full advantage of such rich task information and enable users to program robots by demonstrating their desired behavior. The proposed research has the potential to advance national health, prosperity and welfare by developing research and commercial robotic systems for use in factories, laboratories, and households. It will be an enabling technology for a new generation of highly flexible robots that can be programmed on-the-job to increase the productivity of task environments, such as laboratories or manufacturing facilities. The proposed work will also promote the progress of science by enabling reliable documentation and replication of experiments performed in scientific research wet-labs. Through a new undergraduate capstone course, this project will educate students to develop and program this next generation of robots. To motivate participation in STEM careers, the PIs will demonstrate their work at yearly public outreach events at the University of Washington, and will organize a summer camp for K-16 students through the UW DawgBytes program.<br/><br/>Co-robots that can take on repetitive, time-consuming tasks could greatly improve productivity and reliability in task-oriented environments currently occupied by human workers; such as laboratories, manufacturing facilities, or commercial kitchens. One of the key challenges in realizing this vision is that every combination of environment, user, and task presents unique requirements for the co-robot's behavior and it is impractical to employ traditional approaches for programming these robots. Instead, the PIs envision co-robots that are programmable by their end-users in their particular operation environment and for the particular tasks they are needed for. A popular end-user programming approach in robotics is Programming by Demonstration (PbD), which enables users to program robots by demonstrating their desired behavior. While state-of-the-art PbD techniques have generated impressive robotic behaviors, current approaches have limitations that prevent them from becoming practical and widely adopted. Many of these limitations are specifically related to perception, preventing robots from understanding the detailed context of human demonstrations. To overcome these limitations, The PIs propose to develop a framework for rich task perception, which is able to extract detailed task descriptions from intuitive human demonstrations. Building on recent advances in RGB-D camera sensing, GPU-optimized visual processing, and language grounding, the proposed framework will track all objects and people in a scene at a very fi ne granularity, and parse speech to extract higher-level task structure from a demonstration. The PIs will also introduce new PbD techniques that better take advantage of such rich task information both in the programming and execution of tasks."
"1208390","NRI-Small: Contextually Grounded Collaborative Discourse for Mediating Shared Basis in Situated Human Robot Dialogue","IIS","National Robotics Initiative","10/01/2012","02/22/2016","Joyce Chai","MI","Michigan State University","Standard Grant","Tatiana D. Korelsky","09/30/2017","$981,000.00","Ning Xi","jchai@cse.msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","8013","7923, 8086, 9251","$0.00","In human robot dialogue, although human partners and robots are co-present in a shared environment, they have completely mismatched capabilities in perceiving and reasoning about the environment. Their knowledge and representations of the shared world are drastically different. In addition, the shared environment is full of uncertainties and unexpected events. Humans and robots may have different capabilities in attending and responding to these uncertainties. All of these contribute to a misaligned perceptual basis between a human and a robot, which jeopardizes their collaborative activities and task performance. To enable situated human robot dialogue, a critical component is to develop techniques that will support mediating the shared perceptual basis for effective conversation and task completion. <br/><br/>The objective of this National Robotics Initiative project is to develop a novel framework that tightly integrates high level language and dialogue processing with low level sensing and control systems and contextually grounds the collaborative discourse to mediate shared perceptual basis. By capturing grounded symbolic representations as well as continuous representations of the internal configuration of a robotic system and continuous information sensed from the changing environment, the framework allows the robot to promptly modify its execution without interrupting the on-going tasks. It further enables collaborations between humans and robots to mediate a shared perceptual basis and support efficient interaction in a highly dynamic environment.<br/><br/>This project will provide insight as to how the misaligned perceptual basis between a human and a robot should be mediated through a collaborative process and how such a process should be integrated to produce intelligent and collaborative robot behaviors. The expected results will benefit many applications such as manufacturing, service, assistive technology, and search and rescue."
"1427872","MRI Development: Enabling Research in Natural Communication with Virtual Tutors, Therapists, and Robotic Companions","CNS","MAJOR RESEARCH INSTRUMENTATION, INFORMATION TECHNOLOGY RESEARC, SPECIAL PROJECTS - CISE, Cyber-Human Systems (CHS), ROBUST INTELLIGENCE, National Robotics Initiative","09/01/2014","06/07/2016","Mohammad Mahoor","CO","University of Denver","Standard Grant","Rita V. Rodriguez","08/31/2017","$1,332,000.00","Wayne Ward, Ronald Cole, Juan Wachs","mmahoor@du.edu","2199 S. University Blvd.","Denver","CO","802104711","3038712000","CSE","1189, 1640, 1714, 7367, 7495, 8013","1189, 8086, 9251","$0.00","This project, developing SocioBot-SDS (SocioBot-Spoken Dialog System), an instrument in the form of a robotic character with an emotional response, is expected to advance research involving next generation human-machine interactions. The robotic instrument will be used for therapeutic and educational purposes. Its development will specifically contribute to the research area of perceived speech and visual behaviors. The components of the instrument integrate a unique level of programmability and robustness to the display of human- and non-human-like emotive gestures with conventional orientation control through an articulated neck. The work is expected to accelerate research and development of social robots that can accurately model the dynamics of face-to-face communication with a sensitive and effective human tutor, clinician, or caregiver to a degree unachievable with current instrumentation. The robotic agent builds on advances in computer vision, spoken dialogue systems, character animation and effective computing to conduct dialogues that establish rapport with users producing rich, emotive facial gestures synchronized with prosodic speech generations in response to users' speech and emotions. The instrument represents a new level of integration of emotive capabilities that enable researchers to study socially emotive/robots/agents that can understand spoken language and show emotions and interact, speak, and communicate effectively with people in a natural way (as humans do).<br/><br/>The instrument provides an exciting platform for research and training supporting cross-discipline technology. Since the research community would have access to the instrument, research in how to optimize communication among people and avatars might be accelerated through the perception of speech patterns and visual behaviors of those interacting. The instrument represents a new level of integration of emotive capabilities and serves as a platform for designing a new generation of more immersive and effective intelligent tutoring and therapy systems, and robot-assisted therapeutic treatments for human disabilities that include infants at risk for sensory, attention and language delays, as well as adults with mental disabilities. From an educational perspective, the proposed activities will enhance inter-disciplinary education by involving students at all levels, during and beyond the development of the instrument. The resulting instrument will be freely distributed for researchers to investigate robotic behaviors that lead to immersive and effective applications across a variety of task domains such as teaching students to read, tutoring students in science, conducting speech and language therapy sessions, or providing companionship to elderly individuals in their homes. Moreover, the activities initiated by this development enhance interdisciplinary education, involving students at the undergraduate and graduate levels, during and beyond the development of the instrument."
"1405259","NRI-Small: Improved safety and reliability of robotic systems by faults/anomalies detection from uninterpreted signals of computation graphs","IIS","National Robotics Initiative","10/01/2013","11/18/2013","Andrea Censi","MA","Massachusetts Institute of Technology","Standard Grant","Ralph Wachter","09/30/2017","$672,570.00","","censi@MIT.EDU","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","7923, 8086","$0.00","One of the main challenges to designing robots that can operate around humans is to create systems that can guarantee safety and effectiveness, while being robust to the nuisances of unstructured environments, from hardware faults to software issues, erroneous calibration, and less predictable anomalies, such as tampering and sabotage. However, the fact that the streams of observations and commands possess coherence properties suggests that many of these disturbances could be detected and automatically mitigated with general methods that imply very low design efforts. Currently, robotic systems are developed as a set of components realizing a directed ""computation graph"". This project focuses on theoretical methods, applicable designs, and reference implementation of a faults/anomalies detection mechanism for low-level robotic sensorimotor signals. The system, without any prior information about the robot configuration, should learn a model of the robot and the environment by passive observations of the signals exposed in the computation graph, and, based on this model, instantiate faults/anomalies detection components in an augmented computation graph.<br/><br/>The project engages undergraduate and graduate students in advanced robotics design and development. It is expected the research results will have a significant impact on future robotic systems and machine learning."
"1637479","NRI: Collaborative Research: Experiential Learning for Robots: From Physics to Actions to Tasks","IIS","National Robotics Initiative","10/01/2016","08/17/2016","Dieter Fox","WA","University of Washington","Standard Grant","Hector Munoz-Avila","09/30/2019","$752,000.00","Ali Farhadi","fox@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8013","8086","$0.00","Recent advances in machine learning coupled with unprecedented archives of labeled data are advancing machine perception at a remarkable rate. However, applying these advances to robotics has not advanced as quickly because learning for robotics requires both active interaction with the physical world, and the ability to generalize over a variety of task contexts. This project addresses this knowledge gap through the development of new learning methods to produce experience-based models of physics. In this approach, an object or category specific model of physics is learned directly from perceptual data rather than deploying general-purpose physical simulation methods. These physical models will support both direct control of action - for example pouring a liquid into a container, and the learning of the physical effects of sequences of actions - for example planning to handle fluids in a laboratory. More generally, these methods will provide a means for robots to learn how to handle fluids, soft materials, and other complex physical phenomena.<br/><br/>The proposed experiential learning framework will build on recent advances in deep neural networks. The key problem is to learn the mappings between raw perceptual and control data via a low-dimensional implicit physics space representing a perception-based physical model of how an object acts in the environment. Three directions will be investigated: 1) the development of experiential physics models for object interaction and fluid flow that have strong predictive capabilities, 2) creating mappings directly from experiential models to control of actions such as pouring or moving an object, 3) the assembly of local experience-based controllers into complex tasks from interactive demonstration. Additionally, the project will develop unique data sets that include physical models, simulations, data components, and learned components that other groups can access and build on to enable comparative research similar to what has emerged in machine perception."
"1637444","NRI: Collaborative Research: Software Framework for Research in Semi-Autonomous Teleoperation","IIS","National Robotics Initiative","10/01/2016","08/18/2016","Blake Hannaford","WA","University of Washington","Standard Grant","Jeffrey Trinkle","09/30/2019","$479,998.00","","blake@ee.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8013","8086","$0.00","Telemanipulation systems consist of a human interacting with a mechanical device on the master side to operate a robot at the remote side. They provide natural opportunities for research in intelligent human/robot collaboration, but existing commercial systems, used in areas such as telesurgery, are not intelligent and therefore only replicate the actions of the human operator. These systems are also proprietary, expensive, and not available for modification by researchers. The goal of this NRI project is to provide an open-source software infrastructure that is designed to work with a broad range of hardware and simulated devices to enable a larger community to pursue research and education in intelligent telemanipulation at a lower cost.<br/><br/>The increasing pace of robotics research can be attributed, at least in part, to the increasing availability of software infrastructure, such as Robot Operating System (ROS), and open hardware platforms. This NRI project focuses on providing a software infrastructure for research in intelligent telemanipulation, leveraging infrastructure developed for the Raven II robot and the da Vinci Research Kit (dVRK) and continuing to extend it to other systems, including simulated robots. The three main tasks are to: (1) engage the community to guide development, (2) develop and implement a common API for the diverse hardware platforms, and (3) provide a set of high-level, platform-independent software modules. The goal is to support research towards semi-autonomous telerobotic systems that can more effectively combine the knowledge, reasoning, and decision-making capabilities of a human with the sensing and manipulation capabilities of a robot."
"1637535","NRI: Design of nanorobotics based on iron-palladium alloy nanohelicses for a new diagnosis and treatment of cancer","ECCS","National Robotics Initiative","10/01/2016","08/10/2016","Minoru Taya","WA","University of Washington","Standard Grant","Radhakisan S. Baheti","09/30/2019","$1,500,000.00","Donghoon Lee, Yasuo Kuga","tayam@u.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","ENG","8013","8086","$0.00","Nanohelix is considered a new and attractive building block element for designing a set of new synthetic nano-actuators and -sensors and combination of them, namely nanorobotics which has broader applications; biomedicine, nanomedicine, key catalyst for synthesis of pharmaceutical medicine, key electrodes for energy devices (battery, solar cells, etc), and proximity tactile sensor of soft-matter robotic hands. If the nanohelix is mechanically flexible and made of magnetically active material, which is controlled under applied magnetic field, such magnetically active nanohelix can be designed into a new robotics system for diagnosis and treatment of difficult-to-treat cancers. The proposed nanorobotics can have multi-functions; (i) swimming under magnetic guidance, thanks to the shape of ""helical spring"", (ii) mechanical vibrations of the nanorobotics with flexible nanohelix under applied magnetic field and gradient, thus, killing cancer cells due to mechanical stress loading, and (iii) magnetically active material for nanorobotics plays also as a magnetic resonance imaging enhancer, thus, accurate locations of the nanorobots if they are attached to cancer cell sites, can be identified by the magnetic resonance imaging. <br/><br/>We recently synthesized iron-palladium alloy nanohelices by using chemistry processing route; alumina-silica template and electroplating to make solid-state iron-palladium alloy nanohelices. This iron-palladium alloy nanohelix is down-sizing from our previous design of macro-iron-palladium alloy spring which exhibited the fast vibrations under applied magnetic gradient. The key scientific mechanism associated with the macro-iron-palladium alloy spring, which we discovered is a new actuation mechanism (hybrid mechanism), a set of chain-reactions; applied magnetic gradient, magnetic force, stress induced martensite phase from austensite phase, resulting in fast-actuation within a very short time. We recently made molecular dynamics modelling to simulate another actuator mechanism of iron-palladium alloy nanohelices under applied ""constant"" magnetic field. We also synthesized another nanorobot which is composed of iron-palladium alloy cylindrical head (head) and nanohelix where we can replace the iron-palladium alloy head by an iron head, thus, the nanorobot based on the combination of iron head and iron-palladium alloy helix may serve more effective nanorobot concept. <br/>The goals of the proposed NSF project are multi-fold: (1) to prove the hypothesis driven mechanical stress-induced apoptosis of cancer cells by using the nanorobots under magnetic field, (2) to establish the optimum navigation control of the magnetic nanorobots and (3) to demonstrate the effectiveness of the nanorobots for cancer diagnosis and treatment using in vitro experiment. To achieve the above goals, we propose the following five tasks over a three-year period:<br/><br/>Task-A: High-yield processing of magnetic nanohelices and their nanorobots (Taya)<br/>Task-B: Characterization of the nanostructure and properties of iron-palladium alloy nanohelices (Taya)<br/>Task-C: Modeling work (Kuga/Taya)<br/>Task-D: Production of nanorobots containing solution for apoptosis study (Takao/Taya)<br/>Task-E: In vitro experiment for magnetic nanorobots under applied magnetic field/gradient (Lee/Kuga)<br/><br/>The broader impact of this proposal is that the proposed nanorobots based on magnetic nanohelices, leading to opening up new applications discussed above. We plan to incorporate the results into education,i.e., into the existing graduate course on active and sensing materials and their integrated systems and educational summer program at University of Washington. <br/>The intellectual significances of this NSF project are: (i) to establish high-yield processing route for key building block element of nanorobots, i.e. iron-palladium nanohelices, and combined magnetic head and iron-palladium alloy nanohelix , (ii) to study if the hybrid mechanism of actuation in magnetic nanohelix is realized, (iii) to construct a cohesive model for an accurate control of nanorobots navigation, (iv) to test the hypothesis of mechanical stress loading-induced cell death and (v) to design Helmholtz coil system tailored for accurate navigation of nanorobots."
"1526519","NRI: Collaborative Research: Unified Feedback Control and Mechanical Design for Robotic, Prosthetic, and Exoskeleton Locomotion","IIS","National Robotics Initiative","09/01/2015","08/31/2015","Aaron Ames","GA","Georgia Tech Research Corporation","Standard Grant","Radhakisan S. Baheti","08/31/2018","$712,010.00","Curt Salisbury","aaron.ames@me.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","8086, 092E","$0.00","There is a pressing need for wearable robots, e.g., prostheses and exoskeletons, which improve the quality of life for individuals with limited mobility - devices that work symbiotically with human users to achieve stable, safe and efficient locomotion. At present, approximately 4.7 million people in the United States would benefit from an active lower-limb exoskeleton due to the effects of stroke, polio, multiple sclerosis, spinal cord injury, and cerebral palsy, and by 2050 an estimated 1.5 million people in the United States will be living with a major lower-limb amputation. Yet current wearable robotic devices do not address this growing population's needs since they are bulky, heavy, noisy, and require large batteries for even short duration use, while implementing predominately hierarchical control algorithms. Impeding innovation in this domain is the expensive and slow traditional design-build-test approach that ignores the tight coupling between hardware specifications and control algorithm performance. The vision of this work is to provide a methodology---inspired by advancements in robotic locomotion---that allows lower-limb prostheses and exoskeletons to meet real-world requirements through the co-design of the electromechanical and feedback systems. The transformative nature of this work, therefore, stems from its ability to realize wearable robots that synergize with humans to achieve increased mobility, providing a template for the growing robotic assistive device industry and potentially improving the quality of life of millions. <br/><br/>To realize the vision of this work, the overarching research goal is to create a new unified control and design framework that will allow for the efficient and stable locomotion of robots, prostheses, and exoskeletons. A key aspect of this control methodology is the ability to continuously mediate between different objectives enforcing stability and safety in an efficient manner through force-based interactions among (wearable) robotic devices, their environment and the user. The resulting framework will be utilized via control-in-the-loop mechanical design of prostheses and exoskeletons with stringent design requirements, tested experimentally on a novel humanoid robot, and clinically evaluated through human subject trials. This work is, therefore, guided by the following specific goals: (1) develop a unified online optimization-based control framework for (wearable) robotic locomotion that efficiently mediates stability, safety and force constraints, (2) create a feedback loop between formal control synthesis and the mechanical design of wearable robots that satisfy stringent performance requirements, (3) accelerate clinical testing by translating controllers formally and experimentally from bipedal humanoid robots to prostheses and exoskeletons. As a result of these research goals, this work has the potential to create the next generation of robotic systems that enable stable, safe and efficient human mobility."
"1227234","NRI-Large: Collaborative Research: Purposeful Prediction: Co-robot Interaction via Understanding Intent and Goals","IIS","National Robotics Initiative","10/01/2012","07/08/2014","Dieter Fox","WA","University of Washington","Continuing grant","Ephraim P. Glinert","09/30/2017","$533,332.00","","fox@cs.washington.edu","4333 Brooklyn Ave NE","Seattle","WA","981950001","2065434043","CSE","8013","7925, 8086","$0.00","In order for robots to collaborate with humans, they need to be able to accurately forecast human intent and action. People act with purpose: that is, they make sequences of decisions to achieve long-term objectives. For instance, in driving from home to a store, people carefully plan a sequence of roads that will get them there efficiently. In predicting a person's next decision, algorithms must be developed that reflect these purposeful actions. <br/><br/>Currently, robots are unable to anticipate human needs and goals, and this represents a fundamental barrier to their large-scale deployment in the home and workplace. The aim of this project is to develop a new science of purposeful prediction that can be applied to human-robot interaction across a wide variety of domains. The work draws on recent techniques based on Inverse Optimal Control and Inverse Equilibria Theory that enable statistically sound reasoning about observed deliberate behavior. These new methods provide the foundations of a theoretical framework that integrates traditional decision making techniques like optimal control, search and planning with probabilistic methods that reason about uncertainty and hidden information, particularly about goals, utility and intent. <br/><br/>Intellectual merit: The project will provide a general framework that allows robots to anticipate and adapt to the activities of their human co-workers based on perceptual cues. The investigators will develop the theory, a computational toolbox, and, in collaboration with industrial partners, prototype deployments of these new methods for the prediction of peoples' behavior in a diverse set of robotics domains from computer vision to motor control. The project is transformative in that it combines a novel theoretical/algorithmic framework with extensive support in terms of volume of data and validation infrastructure in the context of many applications. <br/><br/>Broader impacts: A revolution in personal robotics in both the home and workplace depends on the ability to forecast human activities and intents; small- and medium- scale manufacturing will make a leap forward through agile robotic systems intelligent enough to understand and assist their co-workers in flexible assembly tasks; and robust models of pedestrian and vehicular traffic flow will enable more effective driver warning systems and safer autonomous mobile robots. Purposeful prediction technology is an important step towards enabling such understanding of actions and intents in these arenas. The research work will involve the training and mentoring of undergraduate, masters and doctoral students as well as post-doctoral fellows in this emerging multi-disciplinary research area at the intersection of computer and cognitive sciences and robotics."
"1637892","NRI: Towards Restoring Natural Sensation of Hand Amputees via Wearable Surface Grid Electrodes","IIS","National Robotics Initiative","09/01/2016","08/31/2016","Xiaogang Hu","NC","University of North Carolina at Chapel Hill","Standard Grant","Wendy Nilsen","08/31/2019","$1,000,000.00","Yong Zhu, He Huang","xiaogang@unc.edu","104 AIRPORT DR STE 2200","CHAPEL HILL","NC","275991350","9199663411","CSE","8013","8086","$0.00","Hand amputation can severely limit the quality of life, for example by making it impossible to sense and manipulate objects, or to express gestures. Many robotic prosthetic hands have been developed to date, some of which have dexterity approaching that of a human hand, but a key factor limiting acceptance of these devices is the lack of natural and reliable sensory feedback to the user; the substitution of un-natural stimuli such as skin vibration, visual or audio cues doesn't really cut it. The PI's goal in this research is to explore the use of non-invasive grid electrodes for electrically stimulating the peripheral sensory nerves so they transmit natural (high resolution) haptic sensations to the central nervous system. Success of this project will revolutionize the way in which human beings communicate with robotic prostheses and transform research in close-loop prosthesis control, shifting amputee haptic sensation feedback from invasive implant techniques to non-invasive surface probing techniques. The non-invasive nature of the new technology presents the potential for rapid clinical translations with high functional efficiency and user acceptance. Thus, the research will lead to dramatic improvements in hand amputees' quality of life. In addition, the work will be integrated into graduate and undergraduate student education at the PI's institution, and outreach programs for K-12 students (especially underrepresented STEM students) will expose them to this innovative science.<br/><br/>This highly creative project adopts an approach that is completely different from the existing techniques for providing sensory restoration/augmentation, and which is supported by the team's preliminary studies. First, the investigators will design a novel, non-invasive nanowire sensor array that will provide natural sensation of the missing hand. The thin-film electrode grid will be self-adhesive and highly stretchable. The multifunctional electrodes will be able not only to provide targeted nerve stimulation but also to record pressures applied on the prosthetic hand, so they can both obtain a rich set of haptic information and also deliver this information to the user accurately and precisely, while inducing minimal interference such as skin discomfort, added weight due to the device, and control signal interference. Second, the team will create a new way of affording sensory restoration by developing a dynamic stimulation scheme that encodes spatially distinct haptic sensations in the digits and palm. This will be achieved by selectively recruiting the various afferent fibers innervating different regions of the hand. The investigators believe that with high spatial resolution based on hand region mapping, the haptic feedback could for the first time enable users to truly perceive the environment by ""using"" their lost hand, and thereby push the sense of embodiment to a new level. Lastly, new knowledge will be obtained by quantifying the effect of the sensorimotor integration process on closed-loop control of a dexterous prosthetic hand in amputees."
"1637824","NRI: Collaborative Research: Towards Robots with Human Dexterity","IIS","National Robotics Initiative","01/01/2017","08/31/2016","Neville Hogan","MA","Massachusetts Institute of Technology","Standard Grant","Jeffrey Trinkle","12/31/2019","$500,000.00","","neville@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","8086, 8089","$0.00","Despite vastly slower ""hardware"" and ""wetware,"" human dexterity vastly out-performs modern robots. This project studies apparently-simple tasks - managing the kinematic constraint on hand motion required to open a door; and dealing with the dynamic complexity of liquid sloshing in a cup of coffee - that profoundly challenge robots but humans perform with ease. The key idea is that humans manage skillful physical interaction with these objects by exploiting clever combinations of primitive dynamic actions that do not require continuous intervention. A novel theory to describe the effectiveness of this approach is developed and tested by experiments with human subjects. The theory is applied to transfer comparable skill to robots, despite manifestly different hardware. If successful, these robots will be more capable, more comprehensible, and more collaborative partners with humans.<br/><br/>The central experimental challenge is to determine the essential strategy underlying humans' remarkable competence in physical interaction tasks. Three hypotheses reflecting major themes in contemporary motor neuroscience are tested: Humans 1) develop models of object dynamics sufficient to pre-compute and execute required hand motions (similar to modern robot programming); 2) choose forces and motions to minimize muscular effort (similar to optimizing efficiency); or 3) exploit dynamic primitives to robustly achieve satisficing (good-enough) performance. The theoretical challenge is to formulate a coherent account combining the information-processing of brains (or computers) with the ""energy-processing"" of physical objects and their interactions. Classical equivalent circuit theory is re-purposed to define a neo-classical equivalent network theory, combining dynamic motion primitives with mechanical impedances (interactive dynamics). Mechanical impedances enjoy a key property, compositionality, that overcomes the curse of dimensionality."
"1637854","NRI: Collaborative Research: Towards Robots with Human Dexterity","IIS","National Robotics Initiative","01/01/2017","08/31/2016","Dagmar Sternad","MA","Northeastern University","Standard Grant","Jeffrey Trinkle","12/31/2019","$500,000.00","","d.sternad@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173732508","CSE","8013","8086, 8089","$0.00","Despite vastly slower ""hardware"" and ""wetware,"" human dexterity vastly out-performs modern robots. This project studies apparently-simple tasks - managing the kinematic constraint on hand motion required to open a door; and dealing with the dynamic complexity of liquid sloshing in a cup of coffee - that profoundly challenge robots but humans perform with ease. The key idea is that humans manage skillful physical interaction with these objects by exploiting clever combinations of primitive dynamic actions that do not require continuous intervention. A novel theory to describe the effectiveness of this approach is developed and tested by experiments with human subjects. The theory is applied to transfer comparable skill to robots, despite manifestly different hardware. If successful, these robots will be more capable, more comprehensible, and more collaborative partners with humans.<br/><br/>The central experimental challenge is to determine the essential strategy underlying humans? remarkable competence in physical interaction tasks. Three hypotheses reflecting major themes in contemporary motor neuroscience are tested: Humans 1) develop models of object dynamics sufficient to pre-compute and execute required hand motions (similar to modern robot programming); 2) choose forces and motions to minimize muscular effort (similar to optimizing efficiency); or 3) exploit dynamic primitives to robustly achieve satisficing (good-enough) performance. The theoretical challenge is to formulate a coherent account combining the information-processing of brains (or computers) with the ""energy-processing"" of physical objects and their interactions. Classical equivalent circuit theory is re-purposed to define a neo-classical equivalent network theory, combining dynamic motion primitives with mechanical impedances (interactive dynamics). Mechanical impedances enjoy a key property, compositionality, that overcomes the curse of dimensionality."
"1522954","NRI: Collaborative Research: RobotSLANG: Simultaneous Localization, Mapping, and Language Acquisition","IIS","National Robotics Initiative","09/01/2015","08/06/2015","Jeffrey Siskind","IN","Purdue University","Standard Grant","Ephraim P. Glinert","08/31/2018","$650,000.00","","qobi@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","8013","8086","$0.00","Humans and robots alike have a critical need to navigate through new environments to carry out everyday tasks. A parent and child may be touring a college campus; a robot may be searching for survivors after a building has collapsed. In this collaboration by faculty at two institutions, the PIs envision human and robotic partners sharing common perceptual-linguistic experiences and cooperating in mundane tasks like janitorial work and home care as well as in critical tasks like emergency response or search-and-rescue. But while mapping and navigation are now commonplace for mobile robots, when considering human-robot collaboration for even simple tasks one is confronted by a critical barrier: robots and people do not share a common language. Human language is rich in linguistic elements for describing our spatial environment, the objects and places within it, and navigable paths through it (e.g., ""go down the hallway and enter the third door on the right.""). Robots, on the other hand, inhabit a metric world of occupied and unoccupied discretized grid cells, wherein most objects are devoid of meaning (semantics). The PIs' goal in this project is to overcome this limitation by conjoining the well understood problem of simultaneous localization and mapping (SLAM) with that of language acquisition, in order to enable robots to learn to communicate with people in English about navigation tasks. The PIs will spur interest in this novel research area within the scientific community by means of an Amazing Race challenge problem modeled after the reality television show of the same name, which will place robots and human-robot teams in unknown environments and charge them with completing a specific task as quickly as possible. Other outreach activities will include visits to K-12 schools with demonstrations. <br/><br/>This work will focus on simultaneous localization, mapping, and language acquisition, a field of inquiry that remains untouched. The crucial principles are that semantics are formulated as a cost function, which in turn specifies a joint distribution over many variables including those capturing sensory input, language, the environment map, and robot motor control. The cost function and joint distribution support standard inference of many forms, such as command following. More importantly, they support multidirectional inference over multiple variable sets jointly, such as simultaneous mapping and language interpretation. Within this innovative multivariate optimization-based framework, the PIs plan a thorough experimental regimen including both synthetic and real-world datasets of challenging environments, grounding the semantics of natural language in spatial maps of the realistic visual world and robot motor control, while navigating along particular paths or to arrive at particular destinations in (possibly novel) environments that are mapped not only in a geometric sense but also with linguistic underpinning to these particular paths and destinations. The language approach is compositional and uses spatially-grounded representations of nouns (objects/places) and prepositions (relations between them). These spatially-grounded representations will be modeled in the context of mapping. Furthermore, the PIs will consider realistic environments and adapt visual models thereof according to the joint model. The PIs are aware of no other work that jointly models mapping, vision, and language acquisition."
"1638034","NRI: Operating in the Abyss: Bringing Together Humans and Bio-Inpsired Autonomous Vehicles for Maritime Applications","IIS","National Robotics Initiative","09/01/2016","08/29/2016","Kamran Mohseni","FL","University of Florida","Standard Grant","Ralph Wachter","08/31/2019","$599,981.00","","mohseni@ufl.edu","1 UNIVERSITY OF FLORIDA","GAINESVILLE","FL","326112002","3523923516","CSE","8013","8086","$0.00","The ocean covers about two-thirds of the planet's surface, drives weather, regulates temperature, produces about half of the oxygen in the atmosphere, absorbs most carbons from the atmosphere, and ultimately supports all living organisms on Earth. Furthermore, the ocean has been essential to humans for commerce, transport, food, and sustenance. Nevertheless, 95% of the ocean remains unexplored, and the long term impact of natural or man-made changes on the health of the planet and its occupants are far from understood. This investigation is aimed at addressing some of the challenges in the design and operation of a sustained networked robotic system for monitoring and exploring the vast ocean in cooperation with or in replacement for humans. This project proposes research that will fill the knowledge gap in underwater hybrid robotics, effective navigation and coordination of a team of robots with significant constraints and limited resources, and path planning for a team of robots in harsh mediums and with restricted resources.<br/><br/>The proposed hybrid robotic system takes inspiration from marine animals, with a healthy balance between migratory capabilities and accurate maneuvering in proximity of obstacles. The robot will be outfitted with a distributed pressure and surface velocity sensors to provide total hydrodynamic forces for vehicle control and vortex street identification for obstacle detection. Novel underwater robotic actuators are also proposed and will be employed in the design and operation of a hybrid class of underwater robot with efficient high speed cruising and precise low speed maneuvering capabilities required in many marine applications. These new sensing and actuation capabilities facilitate safe co-operation of robots with humans in the ocean and in proximity of obstacles, humans, and other robots. Availability of such new sensory information and actuation capabilities will also result in a paradigm shift in our approach to vehicle control, path planning, and cooperation. To this end new algorithms will be developed and tested in simulations and experiments in a well-equipped underwater laboratory. The system capability to maximize its contribution as human assistants or replacements in existing and emerging marine applications will be explored."
"1526487","NRI: Collaborative Research: Enabling Risk-Aware Decision Making in Human-Guided Unmanned Surface Vehicle Teams","IIS","National Robotics Initiative","09/01/2015","09/03/2015","Satyandra Gupta","MD","University of Maryland College Park","Standard Grant","Radhakisan S. Baheti","04/30/2016","$526,167.00","","guptask@usc.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","CSE","8013","092E, 8086","$0.00","Over the last ten years, substantial progress has been made in the development of small low-cost unmanned surface vehicles (USVs). There are a number of civilian applications where deploying a human-robot team consisting of several small USVs and one or more human supervisors can significantly reduce costs, improve safety, and increase operational efficiencies. Representative applications include remote/persistent ocean sensing, marine search and rescue, maritime operations in congested port environments, and industrial offshore supply and support. USVs face unique challenges that are not experienced by robots operating indoors, such as: the need to adhere to marine navigation rules (COLREGs); local current, wave and wind conditions that can severely reduce the dynamic range of sensors and actuators; frequent communication interruptions; and risk and urgency due to rapidly changing situations during outdoor on-water operations. This research aims to develop decision making foundations for enabling teams of humans and USVs to perform complex collaborative tasks. Advances in this area could be extremely important from both a regulatory and practical standpoint for the future deployment of USV systems. Results from this research will enable leveraging the tremendous potential of USVs by reducing the cost of deployment and operational risks in civilian applications. The integration of the research with graduate and undergraduate courses will enhance the robotics and ocean engineering curricula and enrich learning experiences of the participating students. Outreach activities will educate and inform K-12 students about career opportunities in marine robotics. <br/><br/>The overall goal of the proposed effort is to make advances in risk-informed decision making so that teams of USVs and human supervisors can work cooperatively on a wide variety of missions. The proposed work will develop a comprehensive distributed decision making approach by leveraging the latest advances in task coordination and assignment, planning, reactive behaviors, and control to enable the deployment of human-guided USV teams in civilian applications. Progress in these constituent components will be pursued to ensure that they are consistent with each other and to explicitly account for risk during decision making. This research will develop methodologies to model team missions to ensure that all phases of decision making will have the required information for making informed decisions. Decision making methodologies will be developed for sparse advisory control of USV teams to mitigate risks and for coordinating and assigning tasks to different USVs in the team. Algorithms will also be developed for risk-aware deliberative trajectory planning and generating and executing reactive behaviors for mitigating risks. The methods developed will be validated through on-water field experiments."
"1426752","NRI: Collaborative Research: Minimally Invasive Robotic Non-Destructive Evaluation and Rehabilitation for Bridge Decks (Bridge-MINDER)","IIS","National Robotics Initiative","08/01/2014","08/10/2014","Dezhen Song","TX","Texas A&M Engineering Experiment Station","Standard Grant","Reid Simmons","07/31/2017","$300,000.00","","dzsong@cs.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","8013","8086","$0.00","Bridges are critical components of modern transportation infrastructure. Bridges deteriorate over time as a result of material aging, excessive use and overloading, environmental conditions, inadequate maintenance, and deficiencies in inspection and evaluation. Building on the recent advances in robotics and automation technologies, the objective of this project is to develop a novel Minimally Invasive robotic Non-Destructive Evaluation and Rehabilitation for bridge decks. <br/><br/>The project develops new robotic system, algorithms and control schemes for sustainable civil infrastructure, including (1) development of a novel robotics-assisted in-traffic system for highly-efficient, safety-guaranteed bridge deck inspection and diagnosis; (2) development of a new robotic rehabilitation system for delivering fast, cost-effective, minimally invasive repair and maintenance for bridge decks; (3) a human-in-the-loop robotic coordination for minimally traffic interference; and (4) experimental platforms and field performance evaluation for the proposed system. The project also provides insights to advance real time scene understanding, multiple robot coordination, and human-robot collaboration in complex robotic systems. If successful, this project can drastically reduce bridge maintenance cost and mitigate negative impact caused by closing the bridge traffic flow under traditional bridge inspection and repair. The project outcomes, including source code, datasets, and publications, are to be shared among research community and the general public. The project also includes a number of integrated research and education programs to attract students from underrepresented groups into engineering and involve students into robotics research."
"1427396","NRI: Collaborative Research: Human-Centered Modeling and Control of Cooperative Manipulation with Bimanual Robots","IIS","National Robotics Initiative","08/01/2014","08/10/2014","Oussama Khatib","CA","Stanford University","Standard Grant","Reid Simmons","07/31/2017","$600,000.00","","ok@robotics.stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","8013","8086","$0.00","This proposal addresses modeling and control aspects of human-robot interaction by considering constraints imposed by an individual's physiology. The project is motivated by increasing demand for automation in unstructured environments that require high-level cognitive processing and complex decision-making which cannot yet be fully automated. By taking human-centric approach, data-driven musculoskeletal models are incorporated into the robot interaction model to account for differences of individuals. <br/><br/>Each cooperative activity is divided into action primitives requiring different control strategies while estimating human intent from various sensors. The framework is based on theory of hybrid systems that provides provable safety and stability criteria. The outcome of this research will facilitate methodology for safer and more reliable human-robot interaction and advance state-of-the-art in human movement analysis and control theory. The broader impacts of this research will be realized through new insights into understanding of human intent and haptic cooperation applicable to general human-machine interaction. With increasing interest in service robotics safe and reliable interaction will be the key to successful introduction of robots in human-occupied environments. The potential economic impact of robots engaged in services and manufacturing alongside humans are significant due to increased productivity and reduced costs. Another emerging area is rehabilitation and assistive robotics. The developed data-driven musculoskeletal models will also be applicable to quantification of physical impairments and estimation of muscular stress in healthcare and ergonomics. This interdisciplinary research provides excellent opportunities for undergraduate and graduate students to be engaged in analytical challenges, laboratory demonstrations of theoretical results, and experimental evaluations."
"1426907","NRI: Formal Methods for Motion Planning and Control with Human-in-the-Loop","IIS","National Robotics Initiative","08/01/2014","08/04/2014","Calin Belta","MA","Trustees of Boston University","Standard Grant","Reid Simmons","07/31/2017","$488,644.00","","cbelta@bu.edu","881 COMMONWEALTH AVE","BOSTON","MA","022151300","6173534365","CSE","8013","8086","$0.00","How much autonomy should a robotic system have in a safety critical application? The proposed project addresses this fundamental question. Consider a disaster relief scenario in which an autonomous aerial vehicle (a robot) is required to monitor some areas of interest, while fighting fires and searching for survivors. The survivors should be provided medical assistance and the fires should be extinguished, with priority given to stabilizing and extracting survivors. During the mission, the aerial vehicle should stay away from areas where explosions are likely, so it can continue to do its job. An emergency medical technician (EMT) and firefighter specify the robot?s mission in a way that can be easily understood by the robot. During the execution, depending on what is discovered, the robot makes decisions on its own as long as the high-level specification is not violated. If this is not the case (e.g., there is an obstacle blocking the access to a survivor), the system initiates a dialog with the firefighter and EMT, who provide instructions to help the robot safely cope with the unexpected situation. The research plan of this project is integrated with an education and outreach plan that includes a rich spectrum of robotic-related activities for university and high-school students. <br/><br/>This research project combines ideas and techniques from robot motion planning, formal verification, and control theory to provide a rigorous answer to the question of how much autonomy a robot should be given. A specification language inspired by temporal logics is used to communicate the mission to the robot, whose motions are directed by a hierarchical controller. At the top level, automata game techniques and two discretization schemes, one based on cellular decomposition and the other one on randomized sampling, are used. At the low level, input-output linearization combined with path and vector field following are used to implement the high level plans in quadrotors and differential drive ground vehicles. The human-robot negotiation process is based on the internal representation of temporal logic formulas and their quantitative semantics. While directed at robotics, the project impacts a number of safety critical areas, such as cyber physical systems (construction of correct-by-design systems), air traffic control (design of safe minimum-energy paths for airplanes taking off and landing in a crowded airport), etc."
"1638007","NRI: Goal-Oriented, subject-Adaptive, robot-assisted Locomotor Learning (GOALL)","CBET","National Robotics Initiative","09/01/2016","08/26/2016","Fabrizio Sergi","DE","University of Delaware","Standard Grant","Alexander Leonessa","08/31/2019","$569,903.00","Jill Higginson","fabs@udel.edu","210 Hullihen Hall","Newark","DE","197162553","3028312136","ENG","8013","8086, 9150","$0.00","1638007<br/>Sergi, Fabrizio<br/><br/>Demand for technology to support gait training after neurological injury is increasing due to population aging. Due to recent advances in sensing, actuation, and computation, robots are ideal tools to deliver gait training, but their potential in gait neurorehabilitation has not yet been fully realized. In this context, crucial difficulties are identified in the employed control schemes, which are required to accommodate inter-individual gait variations, while promoting stable and energetically efficient gait patterns. The proposed project combines experiments with a lower limb exoskeleton with biomechanical modeling to determine subject-specific assistance strategies that enable a new approach to robot-aided gait neurorehabilitation, named GOALL (Goal-Oriented, subject Adaptive, robot-assisted Locomotor Learning). The conducted research activities have relevant applications both in rehabilitation and in human augmentation, while improving our basic understanding of gait biomechanics. The planned education and outreach components will be targeted to engage a community of graduate, undergraduate and K-12 students in topics at the intersection of robotics and biomechanics. The dissemination of the research methods and results in an open source format will benefit the robotics and biomechanics communities.<br/><br/>The proposed project formalizes new control methods to modulate discrete kinematic variables of gait, achieving controllability of such variables without fully constraining the gait cycle kinematics, thus promoting inter-individual variability in gait kinematics. To this aim, we pursue a systematic approach to the design of gait assistance primitives, i.e. multi-joint coordination patterns capable of modulating a chosen gait parameter, at different gait speeds. The proposed approach is based on inverse dynamics and pulsed torque approximation, and is followed by human-in-the-loop experiments to test the efficacy of assistance primitives to modulate a selected gait parameter during motor adaptation. The experimental investigation is paralleled by neuromechanical modeling of the response to robotic intervention, with the ultimate goal of generalizing the results to other gait parameters of interest for various patient populations."
"1637764","NRI: Task-Based Assistance for Software-Enabled Biomedical Devices","CBET","National Robotics Initiative","09/01/2016","08/26/2016","Todd Murphey","IL","Northwestern University","Standard Grant","Alexander Leonessa","08/31/2019","$429,782.00","","t-murphey@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","ENG","8013","8086","$0.00","1637764 - Murphey<br/><br/>Robotic assistive devices help people execute and learn physical tasks. Sometimes these tasks are relatively simple, sometimes they are in a particular context, and sometimes they are highly dynamic and very task specific. This work will create algorithms that enable the delivered assistance to take into account algorithmic descriptions of the underlying task. As an example, walking is a highly structured task that simultaneously requires efficiency and stability during motion and must take into account terrain characteristics. The work takes advantage of task knowledge to either modify a person's motion or exert forces that help the person complete the task. This capability is relevant to rehabilitation and physical therapy, where one may wish to only minimally help a person in order to improve therapy outcomes. This work will therefore impact the development of software that supports people engaged in robot-assisted physical therapy, including people recovering from various forms of injury. The key to this work is that knowledge of a task is combined with knowledge of a person's capabilities to synthesize software decisions that ensure safety while also maximizing a person's agency during motion. Broader impact of this work includes technology transfer to rehabilitation, outreach through the Museum of Science and Industry in Chicago, classroom innovation, and industry collaboration.<br/><br/>The proposed work will create software-enabled, task-specific support for assistive biomedical devices. Dynamic tasks require that a combination of the robot and the assisted person be both effective and safe, and the proposed research will create algorithms and software that ensure efficacy and safety while leaving the user free to both move and exert effort. The latter is important in contexts like physical therapy, where effort is important to therapeutic impact. The proposed work will leverage recent results in real-time nonlinear optimal control techniques for human-in-the-loop systems. Specifically, sequential action control (SAC) will be used to both filter and assist human subject dynamic behavior, using a method called the Maxwell's Demon Algorithm. The work will additionally develop formal methodologies for establishing stability and performance guarantees for the proposed algorithms. Lastly, the proposed work will develop compact representations of the controlled assistance algorithms appropriate for computationally minimal embedded systems. All the work will be developed in the Robot Operating System (ROS), making the developed tools widely available to both researchers and companies. The algorithms will be tested on haptic devices and an exoskeleton. The broader impacts for this work will include outreach, technology transfer to rehabilitation, the development of courses in dynamics and analysis, and industrial collaboration. The PI is currently working with the Museum of Science and Industry, and as part of the proposed work the PI and supported students will participate in a National Robotics Week exhibit in the main rotunda of the museum with an estimated viewership of over ten thousand on-site visitors. The PI is involved in significant classroom innovations, and the proposed work will include development of courses in analysis and dynamics. Lastly, the project will include a collaboration with Ekso Bionics, leveraging and impacting their unparalleled expertise in exoskeleton development."
"1225934","NRI-Large: Collaborative Research: Soft Compliant Robotic Augmentation for Human-Robot Teams","IIS","National Robotics Initiative, ROBUST INTELLIGENCE, IIS SPECIAL PROJECTS, COLLABORATIVE RESEARCH","10/01/2012","05/28/2015","Nikolaus Correll","CO","University of Colorado at Boulder","Continuing grant","Reid Simmons","09/30/2017","$264,757.00","","ncorrell@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8013, 7495, 7484, 7298","5936, 7925, 8086, 9251","$0.00","This proposal addresses the hardware, control and planning technologies required to achieve soft robotic systems, in order to offer inherent safety and adaptation to the human-machine systems of the future. The project is motivated by a broad range of aspects of human-robot interaction, including soft augmentation tools for safe compliant manipulation, soft exoskeletons for rehabilitation of neuromuscular disorders, or active clamping systems that can conform to arbitrary surfaces. The proposed research will address the algorithmic and device-level challenges that arise in the design of soft compliant robots capable of pose-invariant and shape-invariant grasping. A combination of algorithmic solutions to modeling, control, planning, and adaptation will lead to new soft compliant manipulators that do not need accurate geometric models for grasping. By designing soft compliant fingers and hands, new approaches to grasp planning and manipulation will be enabled. A novel composable actuation system and supporting planning and control algorithms with features inspired by natural muscle will be developed.<br/><br/>Broader Impacts: Soft robots are inherently low-cost. Affordable soft manipulators will enable in-home assistants for the elderly or incapacitated, but these robots must be able to manipulate the natural world as easily as people do. The next generation robot manipulators will also support new levels of factory automation, in which robots will work synergistically with humans with the ultimate goal of reducing the cost of manufacturing in the USA. The proposed soft devices will provide a new approach to assistive and rehabilitative usage of compliant robotic platforms. Their functional compliant properties will enable them to work side-by-side with human beings or as part of their bodies, to augment and improve human productivity and performance. This new wearable soft robotic technology will not only help workers perform tasks, but also improve the quality of life for many people. In addition, the PIs have a long tradition of integrating research and education by providing research training at all levels, from high-school teachers and students to undergraduate and graduate students, and postdocs. A range of activities to reach out to undergraduate students, high-school communities, women and minorities is planned."
"1226075","NRI-Large: Collaborative Research: Soft Compliant Robotic Augmentation for Human-Robot Teams","IIS","National Robotics Initiative","10/01/2012","07/02/2014","Robert Wood","MA","Harvard University","Continuing grant","Reid Simmons","09/30/2017","$519,578.00","","rjwood@eecs.harvard.edu","1033 MASSACHUSETTS AVE","Cambridge","MA","021385366","6174955501","CSE","8013","7925, 8086","$0.00","This proposal addresses the hardware, control and planning technologies required to achieve soft robotic systems, in order to offer inherent safety and adaptation to the human-machine systems of the future. The project is motivated by a broad range of aspects of human-robot interaction, including soft augmentation tools for safe compliant manipulation, soft exoskeletons for rehabilitation of neuromuscular disorders, or active clamping systems that can conform to arbitrary surfaces. The proposed research will address the algorithmic and device-level challenges that arise in the design of soft compliant robots capable of pose-invariant and shape-invariant grasping. A combination of algorithmic solutions to modeling, control, planning, and adaptation will lead to new soft compliant manipulators that do not need accurate geometric models for grasping. By designing soft compliant fingers and hands, new approaches to grasp planning and manipulation will be enabled. A novel composable actuation system and supporting planning and control algorithms with features inspired by natural muscle will be developed.<br/><br/>Broader Impacts: Soft robots are inherently low-cost. Affordable soft manipulators will enable in-home assistants for the elderly or incapacitated, but these robots must be able to manipulate the natural world as easily as people do. The next generation robot manipulators will also support new levels of factory automation, in which robots will work synergistically with humans with the ultimate goal of reducing the cost of manufacturing in the USA. The proposed soft devices will provide a new approach to assistive and rehabilitative usage of compliant robotic platforms. Their functional compliant properties will enable them to work side-by-side with human beings or as part of their bodies, to augment and improve human productivity and performance. This new wearable soft robotic technology will not only help workers perform tasks, but also improve the quality of life for many people. In addition, the PIs have a long tradition of integrating research and education by providing research training at all levels, from high-school teachers and students to undergraduate and graduate students, and postdocs. A range of activities to reach out to undergraduate students, high-school communities, women and minorities is planned."
"1226883","NRI-Large: Collaborative Research: Soft Compliant Robotic Augmentation for Human-Robot Teams","IIS","National Robotics Initiative","10/01/2012","07/02/2014","Daniela Rus","MA","Massachusetts Institute of Technology","Continuing grant","Reid Simmons","09/30/2017","$1,099,650.00","","rus@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","7925, 8086","$0.00","This proposal addresses the hardware, control and planning technologies required to achieve soft robotic systems, in order to offer inherent safety and adaptation to the human-machine systems of the future. The project is motivated by a broad range of aspects of human-robot interaction, including soft augmentation tools for safe compliant manipulation, soft exoskeletons for rehabilitation of neuromuscular disorders, or active clamping systems that can conform to arbitrary surfaces. The proposed research will address the algorithmic and device-level challenges that arise in the design of soft compliant robots capable of pose-invariant and shape-invariant grasping. A combination of algorithmic solutions to modeling, control, planning, and adaptation will lead to new soft compliant manipulators that do not need accurate geometric models for grasping. By designing soft compliant fingers and hands, new approaches to grasp planning and manipulation will be enabled. A novel composable actuation system and supporting planning and control algorithms with features inspired by natural muscle will be developed.<br/><br/>Broader Impacts: Soft robots are inherently low-cost. Affordable soft manipulators will enable in-home assistants for the elderly or incapacitated, but these robots must be able to manipulate the natural world as easily as people do. The next generation robot manipulators will also support new levels of factory automation, in which robots will work synergistically with humans with the ultimate goal of reducing the cost of manufacturing in the USA. The proposed soft devices will provide a new approach to assistive and rehabilitative usage of compliant robotic platforms. Their functional compliant properties will enable them to work side-by-side with human beings or as part of their bodies, to augment and improve human productivity and performance. This new wearable soft robotic technology will not only help workers perform tasks, but also improve the quality of life for many people. In addition, the PIs have a long tradition of integrating research and education by providing research training at all levels, from high-school teachers and students to undergraduate and graduate students, and postdocs. A range of activities to reach out to undergraduate students, high-school communities, women and minorities is planned."
"1208245","NRI-Small: Expert-Apprentice Collaboration","IIS","ROBUST INTELLIGENCE, INFO INTEGRATION & INFORMATICS, National Robotics Initiative","10/01/2012","09/19/2012","Carlo Tomasi","NC","Duke University","Standard Grant","Sylvia J. Spengler","09/30/2017","$746,924.00","Ronald Parr","tomasi@cs.duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7495, 7364, 8013","7923, 8086","$0.00","Recent advances in robot platforms have outpaced our ability to effectively program robots to accomplish useful tasks, often in complex environments that they share with humans. In order for flexible, general purpose robots to become widespread e.g., in teaching skills to children, assisting the elderly, there must be a way of interacting with them beyond programming. Teaching by demonstration offers a potentially powerful and practical approach to realizing the promise of large scale personal robotics in a wide range of applications. In teaching by demonstration, the expert (human), demonstrates the task on different hardware than what the apprentice or student (robot) uses. <br/><br/>The project aims to develop visual feature-based methods that allow robots to teach humans and learn from them by unifying apprenticeship learning, learning by demonstration (or by imitation) and teaching humans, taking into account the differences between experts and apprentices. The resulting system will be evaluated on a PR2 robot (mostly on grasping and manipulation tasks). The scientific advances resulting from the project in learning from demonstration and imitation learning, both general techniques with broad applicability, will greatly simplify the programming of robots which would make it easier for non-expert users to perform this important task which currently requires considerable expertise in robotics as well as computer science. <br/><br/>Broader impacts of the research include development of new robotics curricula, enhanced opportunities for research-based interdisciplinary training at the intersection of computer vision, machine learning, and robotics, outreach activities (including participation in a public school robotics instruction program). All of the results of the research, including publications, open-source software and datasets, will be made freely available to the larger scientific and academic community."
"1208632","NRI-Small: Perceptually Inspired Dynamics for Robot Arm Motion","IIS","National Robotics Initiative","09/01/2012","02/18/2014","Michael Gleicher","WI","University of Wisconsin-Madison","Standard Grant","Reid Simmons","08/31/2017","$799,889.00","Nicola Ferrier, Bilge Mutlu","gleicher@cs.wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","8013","7923, 8086","$0.00","In order for robots to collaborate efficiently and effectively with humans, the human perception of their movement must be considered in motion creation. Because a human collaborator will interpret the movements of a robot (even subconsciously), robot motion synthesis algorithms that do not consider the human observer may create motions that are perceived incorrectly, interpreted negatively (e.g. as being angry or threatening), or at least miss out on the opportunity to use this subtle communication channel effectively. The key idea of this project is to develop an understanding of human perception of movement that can be applied to the development of robot trajectory planning and control algorithms. The team will use human subjects experiments to understand and evaluate the interpretation of movements and apply these findings in robotics and motion synthesis. The research plan interleaves empirical studies of how people interpret motions, algorithm development to create methods that generate robot motions in a controllable manner, and contextualized deployments that allow the PIs to evaluate the success of the methods. The success of the project will provide a deeper understanding of how people interpret movements, new algorithms for synthesizing robot movements, and demonstrations of the potential applications of collaborative robots.<br/><br/>Broader Impact: Perceptually inspired robot motion synthesis algorithms will enable robots to collaborate more effectively with people. It will enable more communicative robots that can serve as teachers and guides; more approachable and acceptable robots that can work in domestic situations such as elder care; more cooperative robots that can work as assistants to workers; and easier to instruct robots that can be trained by non-experts. This project will enhance the education and outreach efforts of hte PIs by connecting empirical human studies to the technical challenges of robot trajectory planning."
"1637876","NRI: Enhancing Mapping Capabilities of Underwater Caves using Robotic Assistive Technology","IIS","National Robotics Initiative, EXP PROG TO STIM COMP RES","11/01/2016","08/25/2016","Ioannis Rekleitis","SC","University of South Carolina at Columbia","Standard Grant","Jie Yang","10/31/2019","$526,405.00","","yiannisr@cse.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","8013, 9150","8086, 9150","$0.00","This project develops robotic assistive technologies to improve mapping capabilities of underwater caves. The project enables the practical construction of accurate volumetric models for water-filled caves. The technology of this robotic system can also be deployed on underwater vehicles enabling the autonomous exploration of caves and other underwater structures. The developed techniques can also be used in some applications of aerial and ground vehicles. Collected data from field deployments of the developed sensor are made available to the wider robotic, geological, and speleological research community through public-domain releases in order to further innovation. Furthermore, the data and software, released under an open-source license, enable researchers to test algorithms on computer vision, state estimation, and sensor fusion, in challenging environments. The project integrates research and education through training graduate and undergraduate students and enhancing several graduate and undergraduate courses at the University of South Carolina. The project also engages undergraduate students from Benedict College, a Historically Black College or University (HBCU). The collected data are used in outreach activities to recruit high-school students of the greater Columbia area in STEM education, engaging students and educators, particularly in underserved communities.<br/><br/>This research develops 3D reconstruction algorithms utilizing the environmental characteristic of a cave system. The research team studies robotic technologies for sensor fusion of multiple data streams in a single unit and validates experimentally the developed system via extensive testing in underwater cave explorations in collaboration with expert cave divers. The project introduces robotic technology to the underwater cave explorer community by capitalizing on existing practices in three steps: (a) deploying stereo cameras to be used in conjunction with structured light carried by the divers, (b) developing a bearing-only Cooperative Localization system for accurately recording the skeleton of explored caves; (c) developing a sensor suite that seamlessly integrates inertial measurement unit, sonar, depth, and visual data with state estimation algorithms for the volumetric mapping of the cave. The project enhances underwater cave mapping abilities by increasing: 1) the scale of the area mapped, 2) the safety of the divers by reducing their cognitive load during exploration and 3) the quality of the produced maps."
"1524420","NRI: Collaborative Research: Human-Supervised Manipulation of Deformable Objects","IIS","National Robotics Initiative","08/01/2015","08/03/2015","Dmitry Berenson","MA","Worcester Polytechnic Institute","Standard Grant","Alexander Leonessa","09/30/2016","$322,319.00","","berenson@eecs.umich.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","8013","8086","$0.00","The goal of the proposed work is to develop algorithms that enable human-supervised robotic manipulation of deformable objects under significant uncertainty. Manipulation of deformable objects is essential in surgery, which involves complex manipulations of delicate and highly deformable tissue under substantial uncertainty, and in manufacturing, where many assembly tasks involve flexible parts and materials (e.g. cables, textiles, and composites). Recent advances in robot hardware (e.g. the da Vinci and Baxter robots) have made robotic manipulation of deformable objects physically possible but robots still lack the algorithms necessary to perform practical tasks in this domain when there is significant model uncertainty. This project will have broad societal impact through its applications in surgical and manufacturing robotics. The ability to robustly manipulate deformable structures is an important precursor technology towards realizing intelligent robotic surgical assistants. Robotics and its medical applications have the potential to inspire children to pursue careers in STEM fields and meet the needs of America's growing health-care and manufacturing robotics industry. Integration of the research activities with education will emphasize actively involving undergraduates in research activities and introducing new lecture material and projects into undergraduate and graduate courses. Also, special emphasis will be given to recruit qualified students from under-represented groups. <br/><br/>The purpose of the proposed research is to develop algorithms that enable human-supervised robotic manipulation of deformable objects under substantial uncertainty. Specifically, the research will focus on developing adaptive complexity models for modeling deformable object dynamics and associated uncertainty, planning algorithms for integrated exploration and task execution, control algorithms for robust manipulation of deformable objects under uncertainty, and algorithms for effective human supervision of robotic manipulation of deformable objects. The intellectual merit of the project comes from its fundamental contributions to robotic modeling, planning, and control algorithms for manipulation of deformable objects. The research will have impact in the fields of robotic manipulation, motion planning under uncertainty, and compliant manipulation control."
"1656101","NRI: Collaborative Research: Human-Supervised Manipulation of Deformable Objects","IIS","National Robotics Initiative","07/01/2016","08/24/2016","Dmitry Berenson","MI","University of Michigan Ann Arbor","Standard Grant","Alexander Leonessa","07/31/2018","$322,319.00","","berenson@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8013","8086","$0.00","The goal of the proposed work is to develop algorithms that enable human-supervised robotic manipulation of deformable objects under significant uncertainty. Manipulation of deformable objects is essential in surgery, which involves complex manipulations of delicate and highly deformable tissue under substantial uncertainty, and in manufacturing, where many assembly tasks involve flexible parts and materials (e.g. cables, textiles, and composites). Recent advances in robot hardware (e.g. the da Vinci and Baxter robots) have made robotic manipulation of deformable objects physically possible but robots still lack the algorithms necessary to perform practical tasks in this domain when there is significant model uncertainty. This project will have broad societal impact through its applications in surgical and manufacturing robotics. The ability to robustly manipulate deformable structures is an important precursor technology towards realizing intelligent robotic surgical assistants. Robotics and its medical applications have the potential to inspire children to pursue careers in STEM fields and meet the needs of America's growing health-care and manufacturing robotics industry. Integration of the research activities with education will emphasize actively involving undergraduates in research activities and introducing new lecture material and projects into undergraduate and graduate courses. Also, special emphasis will be given to recruit qualified students from under-represented groups. <br/><br/>The purpose of the proposed research is to develop algorithms that enable human-supervised robotic manipulation of deformable objects under substantial uncertainty. Specifically, the research will focus on developing adaptive complexity models for modeling deformable object dynamics and associated uncertainty, planning algorithms for integrated exploration and task execution, control algorithms for robust manipulation of deformable objects under uncertainty, and algorithms for effective human supervision of robotic manipulation of deformable objects. The intellectual merit of the project comes from its fundamental contributions to robotic modeling, planning, and control algorithms for manipulation of deformable objects. The research will have impact in the fields of robotic manipulation, motion planning under uncertainty, and compliant manipulation control."
"1426787","NRI: Collaborative Research: Shall I Touch This?: Navigating the Look and Feel of Complex Surfaces","IIS","National Robotics Initiative","07/15/2014","04/29/2015","Katherine Kuchenbecker","PA","University of Pennsylvania","Standard Grant","Jie Yang","06/30/2017","$408,000.00","","kuchenbe@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","8013","8086, 9251","$0.00","This project improves autonomous robotic perception so that future co-robots can glance around any scene and accurately estimate how it would feel to grasp or step on all of the visible surfaces. Just as people do, robots should use such these physical predictions to guide their interactions with the world, for example avoiding dangerous ice patches on the ground when walking and driving, and adeptly anticipating the grasp force needed to pick up everything from ice cubes to stuffed animals. These research activities are accompanied by significant outreach efforts, including a new program on ""Look and Touch Robotics"" to get middle-school students, particularly those from underrepresented groups, excited about computer science, engineering, and robotics. This program uses simple experiments to highlight the dual importance of visual and haptic information during interactions with physical objects, along with demonstrations of a robot showing visuo-haptic intelligence. This project also integrates research and education by involving undergraduates in the research and via hands-on projects in the vision and robotics classes taught by the Principal Investigators.<br/><br/>This research involves extensive collection of data from real objects and surfaces using both visual and haptic sensors. The recorded interactions are analyzed to uncover visual clues that can allow a robot to infer the physical characteristics of the surface, such as slipperiness, hardness, and roughness. This problem is addressed using deep learning, a recently developed approach that has been successful in enabling robots to visually recognize a wide variety of objects in diverse circumstances. The research team also builds the database of visuo-haptic recordings and the learned cross-modal sensory, and makes it available to other robotics researchers at the end of the project."
"1427250","NRI: Multi-Digit Coordination by Compliant Connections in an Anthropomorphic Hand","IIS","National Robotics Initiative","08/01/2014","08/23/2016","Joshua Schultz","OK","University of Tulsa","Standard Grant","Ephraim P. Glinert","07/31/2017","$443,583.00","Yuri Lansinger, Gavin O'Mahony","joshua-schultz@utulsa.edu","800 S. Tucker Drive","Tulsa","OK","741049700","9186312192","CSE","8013","8086, 9150","$0.00","Robust, soft, lightweight, dexterous hands will be a crucial component of the next generation of robots that physically interact with human co-workers. To date, under-actuated robot hands (where one motor moves multiple joints) that grasp objects reliably have been created largely by designer ingenuity using ""tricks of the trade"" without fully capitalizing on the wealth of mathematical grasping and manipulation theory. Moreover, anatomical studies of the human hand and robotic hands that do useful work have largely evolved separately, leading to similar conclusions expressed in a different way. This collaborative effort between engineering and medical faculty will help bridge these gaps. Engineering graduate students and medical residents will interact regularly; this will serve to translate knowledge and experience across the boundaries of these two disciplines, thereby fostering the ability to solve complex, multi-faceted bioengineering problems. A key contribution of this project will be a theoretical description of an elastic drive train that maps individual actuator motions to key movements in tasks of daily living. The robotic hand produced as another outcome of this work will serve as a robust platform for future experimentation (for example, in studies on gesturing, communication for the hearing impaired, and tactile exploration), and it will also be useful for evaluating and visualizing the effect of potential surgical interventions ex vivo. An important educational and research tool in the PI's institution, this hand will be used to educate and interest university and K-12 students in robotics. The PI team will coordinate with student groups and Native American community organizations, to recruit qualified underrepresented minorities to participate in the project.<br/><br/>Moving from single- to multi-actuator multi-fingered dexterous robot hand design is not straightforward and has frustrated researchers for several years; it will require a principled formal treatment. In single-actuator hands, adding a spring to the drive train improves the hand; the exact spring characteristics are not critical. In the general case of multi-actuator compliant hands simply adding a spring is not enough; a multidimensional spring of specific characteristic is needed. Concepts from multiport circuit theory, mechanics of materials, linear algebra and elements of general multi-fingered grasping theory will be used to understand the behavior of such a multi-dimensional spring. The PI's approach is to break this problem into sub-problems, greatly simplifying the analysis. There is also the matter of which actions an under-actuated hand should be able to perform; the PI argues that complete finger individuation is not necessary, rather coupled finger movements should correspond to those used by humans to perform tasks. Anatomical analysis of both normal humans and those that have undergone surgical intervention will be used to prioritize basic human motion primitives. Combined with the novel compliant grasping theory, under-actuated hand synthesis will be formulated as a parameter fitting problem. Detailed evaluation of the effects of surgical intervention on the human hand, the study of human anatomy, multi-port circuit models, and consideration of compliant mechanisms and the fundamental subspaces of linear algebra will all be blended. Taken together, this will result in a method to synthesize an elastic drive train that maps actuator contributions to useful, coordinated motions of the fingers. Implications of the compliant actuation theory will be examined in the mechanism synthesis and grasp stability contexts, and these will be evaluated experimentally. The investigators will pursue this line of reasoning all the way to implementation in a functioning anthropomorphic hand capable of performing the most common tasks of everyday life."
"1527140","NRI: Balance Recovery Control for Amputees Using Powered Leg Prostheses","IIS","National Robotics Initiative","08/01/2015","08/12/2015","Hartmut Geyer","PA","Carnegie-Mellon University","Standard Grant","Alexander Leonessa","07/31/2018","$900,000.00","Steve Collins","hgeyer@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","8086","$0.00","Falls among amputees are frequent, costly and negatively impact quality of life. In 2005, there were one million people in the United States with lower limb amputation. About 10 percent of these people experience at least one fall that results in serious injury, resulting in estimated costs to the United States health care system of about $1.1 billion annually. This number is expected to quadruple by the year 2050 due to increasing rates of obesity and diabetes. People with amputation are conscious of their increased fall risk, leading to reduced mobility and social activity. About half of the amputee population reports a fear of falling. Similarly large numbers list as major limitations to their quality of life the inability to walk on uneven terrain or without a stabilizing gait aid. These facts highlight the challenge imposed by imbalance, and suggest that improving balance recovery in amputee locomotion would significantly improve the quality of life of lower-limb amputees as well as reduce related health care costs. The emerging field of robotic prosthetics provides the opportunity to attack this problem with novel prosthesis designs and control strategies. The project seeks to take advantage of this opportunity. It combines computational models of the human neuromuscular control system with novel designs of powered knee-and-ankle prostheses and biomechanical gait analysis to establish new control paradigms for powered prostheses that substantially improve the ability of amputees to recover from large disturbances. Implemented in commercial devices, these control techniques could reduce fall rates and fear of falling, thereby improving the mobility and quality of life for millions of people. Other benefits of the project include the support of education. Graduate and undergraduate students will be trained and mentored, and research tools and outcomes will be integrated into coursework, including the software and hardware tools developed in this project for studying prosthesis control and disturbance recovery in human locomotion. In addition, the project supports the dissemination of research results by publishing and freely providing simulation code, control implementation code, and hardware designs online.<br/><br/>The overarching goal of this project is to test the hypothesis that a reflex-like prosthesis control strategy inspired by human motor control substantially improves the balance recovery for above-knee amputees during walking. Balance recovery has evolved into a major research area as fall-connected injuries are one of the main causes of impairment, disability and death in aging societies. Lower limb amputees are especially at risk of falling as current prosthetic limbs provide only limited functionality for recovering from unexpected disturbances. The project combines methods from computational neuromechanics, robotic prosthetics, and biomechanical gait analysis to identify prosthesis control strategies that help above-knee amputees recover balance after large disturbances such as trips, slips and pushes. An existing reflex control model of human locomotion is adapted to amputee gait, involving theoretical research on feedback control algorithms for powered prosthetic limbs and predictions of amputee recovery behavior in simulated experiments. Prototypes of powered knee-ankle prostheses are developed, including a tethered prosthesis emulator for rapid human-in-the-loop control design and evaluation on a treadmill, and a mobile prosthesis allowing evaluation outside the laboratory. Control algorithms identified in the reflex control model are embedded in these prototypes and systematically evaluated in balance recovery experiments with above-knee amputees. An outcome confirming the hypothesis could establish new control paradigms for powered prostheses and enable practical controllers for improved balance recovery in amputee gait. In addition, the project will advance theoretical models of human balance recovery as well as control algorithms and hardware designs for robotic knee-ankle prostheses."
"1426744","NRI: Collaborative Research: Jointly Learning Language and Affordances","IIS","National Robotics Initiative","08/01/2014","07/09/2015","Ashutosh Saxena","NY","Cornell University","Standard Grant","Reid Simmons","07/31/2017","$341,548.00","Bart Selman","asaxena@cs.cornell.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","8013","8086","$0.00","The investigators of this project envision a world where robots surround us, in our homes, in our hospitals, and in our factories, helping people by delivering medicine, preparing food, and assembling objects. Achieving this vision requires robots to communicate with people about their needs, and then plan their activities to help meet those needs. Previous research has addressed these two problems separately, leading to technical solutions that do not work reliably in real-world situations, and to difficulties in human-robot communication. To solve these problems, we are developing the Physically-Grounded Language with Affordances (PGLA) framework and concentrate our research into two thrusts: 1) enable a robot to observe a patient, then answer a nurse's questions about the patient's activity, and 2) enable a robot to respond to natural language requests in a collaborative cooking task and in a manufacturing setting. We will release our open-source data sets and code, which will have impact in other technical areas beyond robotics, such as computer vision and machine learning. The results of our proposed research will find direct applications in industries such as manufacturing and assistive robotics.<br/><br/>This project takes a probabilistic approach to jointly learn to recognize affordances in the environment and predict associated natural language requests and descriptions. Since the affordance map is grounded to perceptual data, our robots will learn to robustly manipulate objects in the physical world, respond to natural language commands, and describe their experiences using words. Our learning approach enables the robot to infer cross-model knowledge from large data sets of people carrying out activities paired with natural language descriptions of the activities, leveraging the strength of each modality to inform the others. Our novel learning algorithms will integrate and learn from multi-domain databases such as the semantic web, visual scenes, and a novel activity database paired with natural language descriptions."
"1426655","NRI: Collaborative Research: Exploiting Granular Mechanics to Enable Robotic Locomotion","IIS","National Robotics Initiative","08/15/2014","08/08/2014","Howard Choset","PA","Carnegie-Mellon University","Standard Grant","Jeffrey Trinkle","07/31/2018","$475,406.00","","choset@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","8086","$0.00","We need robots to extend our reach into dirty and dangerous environments. To do so, mobile robots must be able to locomote in messy unstructured terrains. Conventional mobile robots have not begun to display the multi-functionality of organisms that inhabit natural terrains. This is because mobile robots have been created in, and their models mainly validated on, clean hard laboratory floors, whereas biological organisms have evolved to contend with heterogeneous, dirty and unpredictable environments. One important example of such real world complex terrain, often overlooked by our community despite its ubiquity, involves loose granular materials commonly found in deserts, disaster sites, containers, and caves. Therefore creation of the next level of mobility to traverse dirty environments requires simultaneous advances in both robotics and physics, particularly regarding the interactions associated with desired behaviors. <br/><br/>The proposed work is built on a foundation of geometric mechanics, granular physics of intrusion and biological inspiration from desert-dwelling snakes. We use geometric mechanics, a field that applies principles from differential geometry to problems in classical mechanics, to design gaits for biologically inspired robots. We bring the benefits of the geometric tools to bear on granular environments: in even these mathematically ""messy"" systems, we can begin to efficiently analyze gaits. The key concept in this effort is that systems with complicated, nonlinear low-level physics often exhibit much ""cleaner"" high-level motion, often approximated by a kinematic relationship. Development of such high-level motion controllers will be aided by our ability to discover basic biological principles of locomotion in granular media. We will therefore develop computationally efficient analysis tools for granular materials and will develop techniques to study the locomotion of systems on the surface of granular media."
"1637815","NRI: 3-D Maneuverable Feedback-Controlled Micro Swimming Drone for Biomedical Applications","ECCS","National Robotics Initiative","09/01/2016","08/22/2016","Sung Cho","PA","University of Pittsburgh","Standard Grant","Radhakisan S. Baheti","08/31/2019","$724,691.00","Kang Kim, Nitin Sharma","skcho@pitt.edu","University Club","Pittsburgh","PA","152132303","4126247400","ENG","8013","8086","$0.00","Inspired by the old classic movie ""Fantastic Voyage"" and the relatively recent movie ""Inner Space"", many scientist and engineers have investigated and developed medical microswimmers that possibly navigate inside human body for the purpose of drug delivery, bio-sensing, imaging, micro surgery, etc. in the hard-to-reach spots. So far, several methods for microswimmers have been developed including magnetic actuation, harness of bacteria, use of biological chemical/biological fuels, etc. However, these methods have many drawbacks including high bulkiness, high cost and incompatibility with human body. In addition, all the above methods were never or rarely integrated with feedback control or tracing systems to maneuver the microswimmer in a three-dimensional space. This National Robotics Initiative (NRI) award supports fundamental research on manufacturing three-dimensionally maneuverable, biodegradable, untethered, micro swimming drones, studying/developing feedback control algorithms to control their three-dimensional trajectory, and evaluating them in biological environments. The proposed micro drone is propelled by acoustically excited micro bubbles such that its driving system can be integrated with the current clinical ultrasound system with minimal amendment. The proposed drone has tremendous impact with societal benefits on various potential medical applications: local treatments of tumors, removal of fatty deposits on blood vessel walls, break or removal of blood clots, kidney stones, liver stones, gouts, burn cleaning and wound debriding, attack and removal of parasites, removal and break of tar in lungs, drug delivery and controlled release, etc. This research project will also have significant impact on education by (i) re-engineering coursework for both undergraduates and graduates in inter/multi-disciplinary areas; (ii) having graduates and undergraduates involved in research especially from the underrepresented groups; and (iii) demonstrating results from this project in K-12 schools and in public websites and hosting a robotics workshop for underrepresented high school students. Finally, the completion of experimental setups in this research will improve infrastructure for training in science and engineering at University of Pittsburgh. <br/><br/>The 3-D micro swimming drone will be microfabricated from biodegradable materials. The drone has gaseous bubbles being oscillated by externally applied ultrasound waves. The waves with focused or unfocused excitation allows individual drones to maneuver in a 3-D space. A dynamic inversion-based feedback controller and a state observer will control the frequency and amplitude of the exciting US waves to force the drone to follow/track a user-defined 3-D path. The developed drone integrated with actuation and control units will be tested under hydrodynamic conditions similar to living organs to explore possible practical applications. In parallel, the underlying physics of 3-D manufacturing, bubble/fluid dynamics, ultrasound beamforming method, and a Lyapunov stability based feedback controller-estimator configuration will be investigated. In addition, stability and convergence guarantees in control will be provided. The fabrication and assembly technique of 3-D structures can be readily applied to many fields whose applications otherwise remain on 2-D structures. Novel beamforming technologies developed for US actuating/imaging of micro object can be adapted for high quality ultrasound imaging for broader, general applications. Advances in understanding and new findings of nonlinear (bubble) cavity oscillation and associated fluid dynamics will help develop the best imaging strategy for microbubbles inside microvasculature structures. In addition, the Lyapunov stability-based nonlinear control design method with a state estimator can be applied to control other robots (and other nonlinear systems with zero dynamics) where only partial information is available. The research results will be disseminated through academic/industrial meetings and publications and integrated with multi-/inter-disciplinary education and public outreach programs."
"1651089","EAGER: Human-Aware Navigation in Populated Indoor Environments","IIS","National Robotics Initiative","09/01/2016","08/18/2016","Peter Stone","TX","University of Texas at Austin","Standard Grant","Reid Simmons","08/31/2017","$259,396.00","Luis Sentis, Ufuk Topcu","pstone@cs.utexas.edu","101 E. 27th Street, Suite 5.300","Austin","TX","787121532","5124716424","CSE","8013","7916, 8086","$0.00","Current autonomous mobile robots are able to navigate accurately through sparsely populated areas without bumping into things. However, they have more trouble in situations that commonly arise in public buildings, such as when passing people in narrow hallways, when moving through open, populated spaces, or when crossing a crowd of people exiting an auditorium. Thus, for autonomous robots to reach their full potential, in terms of positive impact on society, they will need to improve their navigational abilities to be more ""human-aware."" That is, they will explicitly need to take account the characteristics of the people with whom they need to interact in public spaces. With this motivation in mind, the goal of this research is to understand how best to enable mobile robots to navigate smoothly, robustly, and safely through human-populated indoor environments in pursuit of high-level goals, with varying levels of guidance from a human operator in a fully human-aware manner.<br/><br/>This project focuses on two complementary, high-risk, and potentially foundational research thrusts as being crucial to laying the groundwork for eventual development of a robust, human-aware navigation system. First, it aims to develop formal specifications for safe robot-operator-pedestrian interactions, using probabilistic temporal logics. Second, it aims to develop methods for generating learned models of operator preferences that can influence the robot's choice of paths with regards to, for example, trajectory smoothness, order of subgoal achievement, task completion time, travel speed, and proximity of trajectory to pedestrians and fixed objects, learning user preferences and determining how to combine them with task-achieving reward functions."
"1637759","NRI: Collaborative Research: Software Framework for Research in Semi-Autonomous Teleoperation","IIS","National Robotics Initiative","10/01/2016","08/18/2016","Gregory Fischer","MA","Worcester Polytechnic Institute","Standard Grant","Jeffrey Trinkle","09/30/2019","$550,157.00","","gfischer@wpi.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","8013","8086","$0.00","Telemanipulation systems consist of a human interacting with a mechanical device on the master side to operate a robot at the remote side. They provide natural opportunities for research in intelligent human/robot collaboration, but existing commercial systems, used in areas such as telesurgery, are not intelligent and therefore only replicate the actions of the human operator. These systems are also proprietary, expensive, and not available for modification by researchers. The goal of this NRI project is to provide an open-source software infrastructure that is designed to work with a broad range of hardware and simulated devices to enable a larger community to pursue research and education in intelligent telemanipulation at a lower cost.<br/><br/>The increasing pace of robotics research can be attributed, at least in part, to the increasing availability of software infrastructure, such as Robot Operating System (ROS), and open hardware platforms. This NRI project focuses on providing a software infrastructure for research in intelligent telemanipulation, leveraging infrastructure developed for the Raven II robot and the da Vinci Research Kit (dVRK) and continuing to extend it to other systems, including simulated robots. The three main tasks are to: (1) engage the community to guide development, (2) develop and implement a common API for the diverse hardware platforms, and (3) provide a set of high-level, platform-independent software modules. The goal is to support research towards semi-autonomous telerobotic systems that can more effectively combine the knowledge, reasoning, and decision-making capabilities of a human with the sensing and manipulation capabilities of a robot."
"1637789","NRI: Collaborative Research: Software Framework for Research in Semi-Autonomous Teleoperation","IIS","National Robotics Initiative","10/01/2016","08/18/2016","Peter Kazanzides","MD","Johns Hopkins University","Standard Grant","Jeffrey Trinkle","09/30/2019","$969,800.00","Russell Taylor","pkaz@jhu.edu","3400 N CHARLES ST","Baltimore","MD","212182608","4105168668","CSE","8013","8086","$0.00","Telemanipulation systems consist of a human interacting with a mechanical device on the master side to operate a robot at the remote side. They provide natural opportunities for research in intelligent human/robot collaboration, but existing commercial systems, used in areas such as telesurgery, are not intelligent and therefore only replicate the actions of the human operator. These systems are also proprietary, expensive, and not available for modification by researchers. The goal of this NRI project is to provide an open-source software infrastructure that is designed to work with a broad range of hardware and simulated devices to enable a larger community to pursue research and education in intelligent telemanipulation at a lower cost.<br/><br/>The increasing pace of robotics research can be attributed, at least in part, to the increasing availability of software infrastructure, such as Robot Operating System (ROS), and open hardware platforms. This NRI project focuses on providing a software infrastructure for research in intelligent telemanipulation, leveraging infrastructure developed for the Raven II robot and the da Vinci Research Kit (dVRK) and continuing to extend it to other systems, including simulated robots. The three main tasks are to: (1) engage the community to guide development, (2) develop and implement a common API for the diverse hardware platforms, and (3) provide a set of high-level, platform-independent software modules. The goal is to support research towards semi-autonomous telerobotic systems that can more effectively combine the knowledge, reasoning, and decision-making capabilities of a human with the sensing and manipulation capabilities of a robot."
"1446785","CPS: Synergy: Collaborative Research: Designing semi-autonomous networks of miniature robots for inspection of bridges and other large infrastructures","ECCS","ENERGY,POWER,ADAPTIVE SYS, CYBER-PHYSICAL SYSTEMS (CPS), National Robotics Initiative","11/01/2014","08/18/2016","Nuno Miguel Martins","MD","University of Maryland College Park","Standard Grant","Radhakisan S. Baheti","10/31/2017","$855,500.00","Sarah Bergbreiter, Richard La","nmartins@isr.umd.edu","3112 LEE BLDG 7809 Regents Drive","COLLEGE PARK","MD","207425141","3014056269","ENG","7607, 7918, 8013","092E, 7918, 8235, 9251","$0.00","Designing semi-autonomous networks of miniature robots for inspection of bridges and other large civil infrastructure<br/><br/>According to the U.S. Department of Transportation, the United States has 605102 bridges of which 64% are 30 years or older and 11% are structurally deficient. Visual inspection is a standard procedure to identify structural flaws and possibly predict the imminent collapse of a bridge and determine effective precautionary measures and repairs. Experts who carry out this difficult task must travel to the location of the bridge and spend many hours assessing the integrity of the structure. <br/><br/>The proposal is to establish (i) new design and performance analysis principles and (ii) technologies for creating a self-organizing network of small robots to aid visual inspection of bridges and other large civilian infrastructure. The main idea is to use such a network to aid the experts in remotely and routinely inspecting complex structures, such as the typical girder assemblage that supports the decks of a suspension bridge. The robots will use wireless information exchange to autonomously coordinate and cooperate in the inspection of pre-specified portions of a bridge. At the end of the task, or whenever possible, they will report images as well as other key measurements back to the experts for further evaluation. <br/><br/>Common systems to aid visual inspection rely either on stationary cameras with restricted field of view, or tethered ground vehicles. Unmanned aerial vehicles cannot access constricted spaces and must be tethered due to power requirements and the need for uninterrupted communication to support the continual safety critical supervision by one or more operators. In contrast, the system proposed here would be able to access tight spaces, operate under any weather, and execute tasks autonomously over long periods of time. <br/><br/>The fact that the proposed framework allows remote expert supervision will reduce cost and time between inspections. The added flexibility as well as the increased regularity and longevity of the deployments will improve the detection and diagnosis of problems, which will increase safety and support effective preventive maintenance. <br/><br/>This project will be carried out by a multidisciplinary team specialized in diverse areas of cyber-physical systems and robotics, such as locomotion, network science, modeling, control systems, hardware sensor design and optimization. It involves collaboration between faculty from the University of Maryland (UMD) and Resensys, which specializes in remote bridge monitoring. The proposed system will be tested in collaboration with the Maryland State Highway Administration, which will also provide feedback and expertise throughout the project.<br/><br/>This project includes concrete plans to involve undergraduate students throughout its duration. The investigators, who have an established record of STEM outreach and education, will also leverage on exiting programs and resources at the Maryland Robotics Center to support this initiative and carry out outreach activities. In order to make student participation more productive and educational, the structure of the proposed system conforms to a hardware architecture adopted at UMD and many other schools for the teaching of undergraduate courses relevant to cyber-physical systems and robotics. <br/><br/>This grant will support research on fundamental principles and design of robotic and cyber-physical systems. It will focus on algorithm design for control and coordination, network science, performance evaluation, microfabrication and system integration to address the following challenges: (i) Devise new locomotion and adhesion principles to support mobility within steel and concrete girder structures. (ii) Investigate the design of location estimators, omniscience and coordination algorithms that are provably optimal, subject to power and computational constraints. (iii) Methods to design and analyze the performance of energy-efficient communication protocols to support robot coordination and localization in the presence of the severe propagation barriers caused by metal and concrete structures of a bridge."
"1637875","NRI: Collaborative Research: Autonomous Quadrotors for 3D Modeling and Inspection of Outdoor Infrastructure","IIS","National Robotics Initiative","09/01/2016","08/18/2016","Stergios Roumeliotis","MN","University of Minnesota-Twin Cities","Standard Grant","Jie Yang","08/31/2019","$830,280.00","Peter Seiler","stergios@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","8013","8086","$0.00","This project develops technologies to collect visual and inertial data necessary for constructing, offline, high-accuracy 3D maps of the structure for civil and industrial infrastructure such as bridges, power plants, and refineries. It also develops technologies for online processing including localization, path planning and obstacle avoidance. The project builds a system that employs quadrotors to assist their human co-workers in visual inspections of the outdoor infrastructure to enhance efficiency and effectiveness of such operations. The research advances the current state of the art in key areas of sensing, estimation, and control necessary for enabling small-size quadrotors to assist humans in visual inspections. In addition to improving the reliability of the nation's infrastructure, the project benefits researchers, developers, educators, and end-users in robotics by developing open-source, modular algorithms for quadrotors. The project offers educational and community outreach activities aligned with local efforts and state-wide initiatives, and seeks to increase diversity and attract underrepresented groups to Science, Technology, Engineering, and Mathematics (STEM) via a partnership with local high schools. <br/><br/>This research addresses the fundamental challenges stemming from sensing and processing limitations that prevent the use of low-cost, small-size quadrotors in visual-inspection tasks. It focuses on a four-step process, where initially a quadrotor is tele-operated at a safe distance from the structure of interest to collect visual and inertial data necessary for constructing, offline, high-accuracy 3D maps of the structure. These maps are then used, by the inspection engineer, to designate areas of interest. Lastly, the quadrotor employs its onboard sensors to precisely localize with respect to the structure and navigate along the inspection route, while collecting additional data for increasing the accuracy and improving the reliability of future inspections. A key innovation is making information available in multiple forms and levels of abstraction so as to meet the often-conflicting needs of offline (e.g., visualization of inspection areas and planning information-rich paths) and online (e.g., map-based localization and obstacle avoidance) uses. Also critical is an information-driven approach for making maximum use of the limited sensing and processing resources available to the quadrotor. Lastly, a key advantage of the proposed approach is that it provides the foundation for continual improvement in accuracy and efficiency after each inspection flight."
"1637761","NRI: Collaborative Research: Autonomous Quadrotors for 3D Modeling and Inspection of Outdoor Infrastructure","IIS","National Robotics Initiative","09/01/2016","08/18/2016","Philippos Mordohai","NJ","Stevens Institute of Technology","Standard Grant","Jie Yang","08/31/2019","$290,734.00","","Philippos.Mordohai@stevens.edu","CASTLE POINT ON HUDSON","HOBOKEN","NJ","070305991","2012168762","CSE","8013","8086","$0.00","This project develops technologies to collect visual and inertial data necessary for constructing, offline, high-accuracy 3D maps of the structure for civil and industrial infrastructure such as bridges, power plants, and refineries. It also develops technologies for online processing including localization, path planning and obstacle avoidance. The project builds a system that employs quadrotors to assist their human co-workers in visual inspections of the outdoor infrastructure to enhance efficiency and effectiveness of such operations. The research advances the current state of the art in key areas of sensing, estimation, and control necessary for enabling small-size quadrotors to assist humans in visual inspections. In addition to improving the reliability of the nation's infrastructure, the project benefits researchers, developers, educators, and end-users in robotics by developing open-source, modular algorithms for quadrotors. The project offers educational and community outreach activities aligned with local efforts and state-wide initiatives, and seeks to increase diversity and attract underrepresented groups to Science, Technology, Engineering, and Mathematics (STEM) via a partnership with local high schools. <br/><br/>This research addresses the fundamental challenges stemming from sensing and processing limitations that prevent the use of low-cost, small-size quadrotors in visual-inspection tasks. It focuses on a four-step process, where initially a quadrotor is tele-operated at a safe distance from the structure of interest to collect visual and inertial data necessary for constructing, offline, high-accuracy 3D maps of the structure. These maps are then used, by the inspection engineer, to designate areas of interest. Lastly, the quadrotor employs its onboard sensors to precisely localize with respect to the structure and navigate along the inspection route, while collecting additional data for increasing the accuracy and improving the reliability of future inspections. A key innovation is making information available in multiple forms and levels of abstraction so as to meet the often-conflicting needs of offline (e.g., visualization of inspection areas and planning information-rich paths) and online (e.g., map-based localization and obstacle avoidance) uses. Also critical is an information-driven approach for making maximum use of the limited sensing and processing resources available to the quadrotor. Lastly, a key advantage of the proposed approach is that it provides the foundation for continual improvement in accuracy and efficiency after each inspection flight."
"1637941","NRI: Real-Time Semantic Computer Vision for Co-Robotics","IIS","National Robotics Initiative","09/01/2016","08/18/2016","Nuno Vasconcelos","CA","University of California-San Diego","Standard Grant","Jie Yang","08/31/2019","$719,116.00","","nuno@ece.ucsd.edu","Office of Contract & Grant Admin","La Jolla","CA","920930934","8585344896","CSE","8013","8086","$0.00","This project develops real-time object recognition algorithms that generate extensive semantic object descriptions as a side effect of recognition. This side-information includes perceived object costs, attributes (object properties), and affordances (actions afforded by objects). With these, the act of recognizing a ""door knob"" would automatically produce the information that this is a ""flexible"" object, ""made of metal,"" which ""can be grasped"" and ""can be twisted,"" but ""cannot be eaten."" For robotics, this information is sometimes more important than the recognition of the object itself. The project enables robots to perform zero shot learning, e.g. learn to recognize door knobs by simply being told that these are objects that ""are flexible, made of metal, can be grasped and twisted but not eaten."" The research has applicability in areas such as manufacturing, intelligent systems, assisted living, and homeland security. Educationally, the project provides an exciting opportunity for undergraduate research.<br/><br/>This research develops new methods for top-down (task-driven) regularization of deep learning algorithms, though a combination of structural and loss-based regularizers. Structural regularizers constrain object and scene recognition models to guarantee speed and automatic generation of rich mid-level semantic (MLS) descriptions as a side effect of recognition. Loss-based regularizers penalize errors in the multiple semantic outputs of these models, enabling simultaneously high performance in object recognition, MLS predictions, and zero-shot learning. The resulting learning algorithms will endow robots with human-like abilities to infer rich MLS descriptions of objects and scenes as a ""side effect"" of object recognition and scene classification, in real-time. These contributions will be developed in the context of a new co-robotics problem, person-following unmanned aerial vehicles, where computer vision plays a mission critical role for tasks such as control and semantic motion planning but whose requirements in terms of speed and MLS inference are far superior to what is feasible today."
"1637949","NRI: Collaborative Research: Experiential Learning for Robots: From Physics to Actions to Tasks","IIS","National Robotics Initiative","10/01/2016","08/17/2016","Gregory Hager","MD","Johns Hopkins University","Standard Grant","Hector Munoz-Avila","09/30/2019","$648,000.00","Marin Kobilarov","hager@cs.jhu.edu","3400 N CHARLES ST","Baltimore","MD","212182608","4105168668","CSE","8013","8086","$0.00","Recent advances in machine learning coupled with unprecedented archives of labeled data are advancing machine perception at a remarkable rate. However, applying these advances to robotics has not advanced as quickly because learning for robotics requires both active interaction with the physical world, and the ability to generalize over a variety of task contexts. This project addresses this knowledge gap through the development of new learning methods to produce experience-based models of physics. In this approach, an object or category specific model of physics is learned directly from perceptual data rather than deploying general-purpose physical simulation methods. These physical models will support both direct control of action - for example pouring a liquid into a container, and the learning of the physical effects of sequences of actions - for example planning to handle fluids in a laboratory. More generally, these methods will provide a means for robots to learn how to handle fluids, soft materials, and other complex physical phenomena.<br/><br/> The proposed experiential learning framework will build on recent advances in deep neural networks. The key problem is to learn the mappings between raw perceptual and control data via a low-dimensional implicit physics space representing a perception-based physical model of how an object acts in the environment. Three directions will be investigated: 1) the development of experiential physics models for object interaction and fluid flow that have strong predictive capabilities, 2) creating mappings directly from experiential models to control of actions such as pouring or moving an object, 3) the assembly of local experience-based controllers into complex tasks from interactive demonstration. Additionally, the project will develop unique data sets that include physical models, simulations, data components, and learned components that other groups can access and build on to enable comparative research similar to what has emerged in machine perception."
"1637937","NRI: Collaborative Research: A Framework for Hierarchical, Probabilistic Planning and Learning","IIS","National Robotics Initiative","09/01/2016","08/17/2016","Marie desJardins","MD","University of Maryland Baltimore County","Standard Grant","Reid Simmons","08/31/2019","$365,437.00","","mariedj@cs.umbc.edu","1000 Hilltop Circle","Baltimore","MD","212500002","4104553140","CSE","8013","8086, 9150","$0.00","This project is an effort to create a unified framework for solving very large problems with uncertain states and actions, such as manipulator robots acting in real-world environments. The results may have especially great promise for assistive technologies, including autonomous robots that can be used by elderly and disabled populations to aid them in their daily activities. The proposed integrated framework will represent, apply, and learn hierarchical domain knowledge, and will include the ability to transfer knowledge from simpler problems to more complex ones. The research will enable autonomous agents to develop a structured representation of complex domains based on experience. The agents will use learned representations to interpret natural language commands for both low-level and high-level requests. <br/><br/>The technical focus is enabling tractable planning in large, uncertain domains by generating and leveraging probabilistic domain knowledge at multiple levels of abstraction. Agents will autonomously create layered representations in which the layers build on one another to produce complex behaviors. Agents will learn to perform useful behaviors, such as navigating using low-level sensor feedback or assembling complex objects such as a bridge or a table. The key technical contributions will be methods for (1) planning in large state/action spaces using the abstract object-oriented Markov decision process (AMDP) model, a new formalism for representing probabilistic domain knowledge at multiple levels of abstraction; (2) learning hierarchical task knowledge in the form of AMDPs; and (3) interpreting natural language commands at multiple levels of abstraction by mapping to the learned hierarchical structure. The formalism will be demonstrated and validated in several domains, including a simulated ""cleanup"" toy domain, challenging and complex video games, and a robot manipulation task."
"1560761","EAGER: Characterizing Physical Interaction in Instrument Manipulations","IIS","National Robotics Initiative","03/01/2016","02/25/2016","Yu Sun","FL","University of South Florida","Standard Grant","Jeffrey Trinkle","02/28/2018","$299,887.00","","yusun@cse.usf.edu","3702 Spectrum Blvd.","Tampa","FL","336129446","8139742897","CSE","8013","7916, 8086","$0.00","As personal robots become common in our homes, they will perform a broad range of helpful tasks for their owners. In doing so, all sorts of physical interactions will occur between the robot and the home environment, for example, picking up a mug requires the robot to carefully control contact forces as it secures a grasp. As tasks become more advanced, for example, operating a can opener, the robot will have to ""understand"" how to grasp the can and the can opener, so that it can operate the opener. The goal of this project is to gather data and develop algorithms that will allow a robot to understand how to grasp tools in a way that facilitates its ability to operate that tool safely and effectively. <br/><br/>This research aims to improve the understanding of the physical interactions between the instruments and the environment in daily manipulation tasks with the goal of developing effective robotic grasp and manipulation planners to facilitate the tasks. The research designs and develops a physical-interaction observation system to observe both the interactive motion and wrench between the instruments and environment in several representative instrumental manipulation tasks by a number of participants. Each manipulation task is characterized with its instrumental motion models and wrench distribution models. Using the models, optimal grasps are generated using the task wrench coverage measure and evaluated using a real robotic arm and hand platform. The data collected in this work enables researchers to fully explore daily living tasks and provide excellent training and testing data sets for other related research. The optimal manipulation grasping approach equips robot manipulators with the ability to hold instruments with a firm grasp to withstand the disturbance in daily living interactions and perform tasks efficiently."
"1637614","NRI: Collaborative Research: A Framework for Hierarchical, Probabilistic Planning and Learning","IIS","National Robotics Initiative","09/01/2016","08/17/2016","Stefanie Tellex","RI","Brown University","Standard Grant","Reid Simmons","08/31/2019","$542,682.00","Michael Littman","stefie10@cs.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","8013","8086, 9150","$0.00","This project is an effort to create a unified framework for solving very large problems with uncertain states and actions, such as manipulator robots acting in real-world environments. The results may have especially great promise for assistive technologies, including autonomous robots that can be used by elderly and disabled populations to aid them in their daily activities. The proposed integrated framework will represent, apply, and learn hierarchical domain knowledge, and will include the ability to transfer knowledge from simpler problems to more complex ones. The research will enable autonomous agents to develop a structured representation of complex domains based on experience. The agents will use learned representations to interpret natural language commands for both low-level and high-level requests. <br/><br/>The technical focus is enabling tractable planning in large, uncertain domains by generating and leveraging probabilistic domain knowledge at multiple levels of abstraction. Agents will autonomously create layered representations in which the layers build on one another to produce complex behaviors. Agents will learn to perform useful behaviors, such as navigating using low-level sensor feedback or assembling complex objects such as a bridge or a table. The key technical contributions will be methods for (1) planning in large state/action spaces using the abstract object-oriented Markov decision process (AMDP) model, a new formalism for representing probabilistic domain knowledge at multiple levels of abstraction; (2) learning hierarchical task knowledge in the form of AMDPs; and (3) interpreting natural language commands at multiple levels of abstraction by mapping to the learned hierarchical structure. The formalism will be demonstrated and validated in several domains, including a simulated ""cleanup"" toy domain, challenging and complex video games, and a robot manipulation task."
"1327597","NRI: Large: Collaborative Research: Complementary Situational Awareness for Human-Robot Partnerships","IIS","National Robotics Initiative","10/01/2013","05/11/2016","Howard Choset","PA","Carnegie-Mellon University","Continuing grant","Jeffrey Trinkle","09/30/2018","$1,286,883.00","","choset@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","7925, 8086, 9251","$0.00","This work will advance human-robot partnerships by establishing a new concept called complementary situational awareness (CSA), which is the simultaneous perception and use of the environment and operational constraints for task execution. The proposed CSA is transformative because it ushers in a new era of human-robot partnerships where robots act as our partners, not only in manipulation, but in perception and control. This research will establish the foundations for CSA to enable multifaceted human-robot partnerships. Three main research objectives guide this effort: 1) Real-time Sensing during Task Execution: design low-level control algorithms providing wire-actuated or flexible continuum robots with sensory awareness by supporting force sensing, exploration, and modulated force interaction in flexible unstructured environments; 2) Situational Awareness Modeling: prescribe information fusion and simultaneous localization and mapping (SLAM) algorithms suitable for surgical planning and in-vivo surgical plan adaptation; 3) Telemanipulation based on CSA: Design, construct, and integrate robotic testbeds with telemanipulation algorithms that use SLAM and exploration data for online adaptation of assistive telemanipulation virtual fixtures. This research also includes investigation of previously unaddressed questions on how sensory exploration and palpation data can be used to enable online-adaptation of assistive virtual fixtures based on force and stiffness data while also taking into account preoperative data and intraoperative correction of registration parameters.<br/><br/>The proposed work will restore the situational awareness readily available in open surgery to minimally invasive surgery. This will benefit patients by enabling core technologies for effective and safe natural orifice surgery or single port access surgery. The societal impact of the proposed work on these two surgical paradigms is reduced pain for patients, shorter hospital stay, improved cosmesis and patients' self image, and lower costs. We also believe that CSA will impact manufacturing where its future will require people and robots working together in a shared space on collaborative tasks. Also, the same concepts of CSA apply to telemanipulation in constrained and unstructured environments and the proposed research has direct relevance to robot-human partnerships for space exploration. To ensure this broader impact will be achieved, an advisory board has been assembled with experts from medicine, manufacturing and aerospace. Finally, the PIs will facilitate collaboration in the medical robotics research community by making our software and hardware designs available on-line and using commercial-grade hardware available at multiple institutions."
"1327566","NRI: Large: Collaborative Research: Complementary Situational Awareness for Human-Robot Partnerships","IIS","National Robotics Initiative","10/01/2013","07/05/2016","Nabil Simaan","TN","Vanderbilt University","Continuing grant","Jeffrey Trinkle","09/30/2018","$1,279,600.00","","nabil.simaan@vanderbilt.edu","Sponsored Programs Administratio","Nashville","TN","372350002","6153222631","CSE","8013","7925, 8086, 9150, 9251","$0.00","This work will advance human-robot partnerships by establishing a new concept called complementary situational awareness (CSA), which is the simultaneous perception and use of the environment and operational constraints for task execution. The proposed CSA is transformative because it ushers in a new era of human-robot partnerships where robots act as our partners, not only in manipulation, but in perception and control. This research will establish the foundations for CSA to enable multifaceted human-robot partnerships. Three main research objectives guide this effort: 1) Real-time Sensing during Task Execution: design low-level control algorithms providing wire-actuated or flexible continuum robots with sensory awareness by supporting force sensing, exploration, and modulated force interaction in flexible unstructured environments; 2) Situational Awareness Modeling: prescribe information fusion and simultaneous localization and mapping (SLAM) algorithms suitable for surgical planning and in-vivo surgical plan adaptation; 3) Telemanipulation based on CSA: Design, construct, and integrate robotic testbeds with telemanipulation algorithms that use SLAM and exploration data for online adaptation of assistive telemanipulation virtual fixtures. This research also includes investigation of previously unaddressed questions on how sensory exploration and palpation data can be used to enable online-adaptation of assistive virtual fixtures based on force and stiffness data while also taking into account preoperative data and intraoperative correction of registration parameters.<br/><br/>The proposed work will restore the situational awareness readily available in open surgery to minimally invasive surgery. This will benefit patients by enabling core technologies for effective and safe natural orifice surgery or single port access surgery. The societal impact of the proposed work on these two surgical paradigms is reduced pain for patients, shorter hospital stay, improved cosmesis and patients' self image, and lower costs. We also believe that CSA will impact manufacturing where its future will require people and robots working together in a shared space on collaborative tasks. Also, the same concepts of CSA apply to telemanipulation in constrained and unstructured environments and the proposed research has direct relevance to robot-human partnerships for space exploration. To ensure this broader impact will be achieved, an advisory board has been assembled with experts from medicine, manufacturing and aerospace. Finally, the PIs will facilitate collaboration in the medical robotics research community by making our software and hardware designs available on-line and using commercial-grade hardware available at multiple institutions."
"1427004","NRI: Peer-to-Peer Human-Robot Coalitions","IIS","National Robotics Initiative","08/01/2014","12/19/2014","Lynne Parker","TN","University of Tennessee Knoxville","Standard Grant","Reid Simmons","07/31/2017","$521,309.00","","parker@eecs.utk.edu","1 CIRCLE PARK","KNOXVILLE","TN","379960003","8659743466","CSE","8013","8086, 9150","$0.00","This research aims to create large-scale teams of human and robot peers that operate side-by-side in the same physical space, with each human and robot performing physical actions based upon their own skills and capabilities. The intent is to generate an interaction style that is not based on direct commands and controls from humans to robots, but rather on the idea that robots can implicitly infer the intent of human teammates through passive observation, and then take appropriate actions in the current context. In this interaction, humans perform tasks in a very natural manner, as he/she would when working with a human teammate, thus bypassing the difficulty of cognitive overload that occurs when humans are required to explicitly supervise the actions of several robot team members. This research can revolutionize how humans and robots work together in applications such as search and rescue, firefighting, security, defense, light construction, manufacturing, home assistance, and healthcare.<br/><br/>This research focuses on two key challenges: (1) how robots can determine humans' current goals, intents, and activities via sensor observation only, and (2) how robots can respond appropriately to help humans with the ongoing task, consistent with the inferred human intent. Input to the robot system is a set of learned models, along with color and depth sensing. Models are learned using novel features for human perception and representation, including Depth of Interest features, 4-dimensional local spatio-temporal features, adaptive human-centered features, and simplex-based orientation descriptors. Learning techniques make use of novel maximum temporal certainty models for sequential activity recognition, and conditional random fields for environmental monitoring. Robot activity selection is achieved via a novel risk-aware cognitive model. The outcome of this research will be new software methodologies enabling robot cognition, learning, sensing, perception, and action selection for peer-to-peer human-robot teaming."
"1208623","NRI-Small: Multi-modal sensor skin and garments for healthcare and home robots","IIS","National Robotics Initiative","10/01/2012","11/07/2013","Dan Popa","TX","University of Texas at Arlington","Standard Grant","Jeffrey Trinkle","05/31/2017","$1,349,766.00","Woo Ho Lee, Muthu Wijesundara, Nicoleta Bugnariu, Frank Lewis, Zeynep Celik-Butler, Donald Butler","dan.popa@louisville.edu","1 UNIVERSITY OF TEXAS AT","Arlington","TX","760190145","8172722105","CSE","8013","1653, 7923, 8086","$0.00","The objective of this research is to answer fundamental design questions for multi-functional robotic skin sensors, optimize their placement onto assistive robotic devices, have the robot and human ""learn"" how to use the skin sensors efficiently, and quantitatively assess the impact of this assistive technology to humans. The approach is to design and fabricate integrated micro-scale sensors in conjunction with iterative simulation and experimental studies of the performance of physical human-robot interaction enabled by this technology. <br/>Intellectual Merit<br/>This project will contribute efficient algorithms for optimal placement and data networking of distributed skin sensors on robots; new learning and control algorithms to sense human intent and improve interactivity; practical robotic skin and garment hardware with distributed sensors to include tactile, thermal imaging, and acceleration sensing in flexible materials that can be easily attached on and peeled off robots; and new metrics to evaluate the impact of this skin to humans including level of assistance, safety, ease of use, aesthetics, and therapeutic benefits.<br/>Broader Impacts<br/>Co-robots of the future will share their living spaces with humans, and, like people, will wear sensor skins and clothing that must be interconnected, fitted, cleaned, repaired, and replaced. In addition to aesthetic purposes that increase societal acceptance, these sensorized garments will also enhance robot perception of the environment, and enable extraordinary levels of safety, cooperation, and therapy for humans. The research proposed here will unlock near-term and also unforeseen applications of robotic skin with broad applicability, and especially to home assistance, medical rehabilitation, and prosthetics."
"1641276","NSF National Robotics Initiative (NRI) 2016 PI Meeting","IIS","National Robotics Initiative","08/15/2016","08/09/2016","Mark Yim","PA","University of Pennsylvania","Standard Grant","Reid Simmons","04/30/2017","$123,819.00","R. Vijay Kumar, Kostas Daniilidis, Daniel Lee","yim@grasp.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","8013","8086, 7556","$0.00","The objective of this award is to organize the annual Principal Investigators (PI) meeting for the National Robotics Initiative (NRI), which was launched in 2011. The PI meeting brings together the community of researchers, companies, and program officers who are actively engaged in the NRI to provide cross-project coordination in terms of common intellectual challenges, methods for education and training, best practices in terms of transition of results, and a centralized and lasting repository illustrating the research ideas explored and milestones achieved by the NRI projects. The meeting will be two days during late fall 2016 in the vicinity of Washington DC. The format will include short presentations by all the attending PIs, a poster session, keynote speeches, and panel discussions. Invitations to the meeting will include all PIs with active NRI grants, program managers with robotics-related programs, and members of the press."
"1317379","NRI: Small: Modeling, Quantification, and Optimization of Prosthesis-User Interface","CBET","National Robotics Initiative","09/01/2013","01/22/2014","Levi Hargrove","IL","Rehabilitation Institute of Chicago","Standard Grant","Alexander Leonessa","08/31/2018","$999,900.00","Konrad Kording, Levi Hargrove","l-hargrove@northwestern.edu","345 East Superior Street","Chicago","IL","606112654","3122384534","ENG","8013","7923, 8086","$0.00","PI: Sensinger, J. W.; Hargrove, L.; and Kording, K. P.<br/>Proposal Number: 1317379<br/><br/>Problem Description: Better robotic prostheses can dramatically improve the quality of life for the more than 40,000 Americans with an upper limb amputation, many of whom reject existing devices because they have trouble controlling them in the same intuitive, subconscious way that they controlled their intact arms. Prosthesis control is difficult because amputees experience great uncertainty both with respect to whether their device will respond appropriately to their control signals and whether sensory feedback cues accurately reflect the actual movement. Researchers have focused on improving isolated aspects of control, for example by improving filters or mimicking able-bodied sensory cues through haptic devices, but these approaches have minimally reduced the uncertainty of prosthesis control. Human interaction with a prosthesis is a multifaceted, time-varying problem that is difficult to solve. What is missing from robotic prosthesis research are principled methods for optimizing control strategies and sensory cues which take into account behavioral choices people are known to make in the face of high uncertainty.<br/><br/>Intellectual Merit: The proposed research is innovative because it poses the co-robot problem in a broader context that incorporates the highly sophisticated behavioral decisions that humans make in optimizing their control strategy and sensory cues. This principled approach is able to integrate multiple effects in ways that were not possible using previous approaches. For example, the proposed approach naturally incorporates the fact that people prefer to use less exerted effort to accomplish a task, but tolerate more effort during portions of movement that require greater precision (e.g. final portion of a trajectory). On the other hand, the approach does not favor high-certainty haptic cues if those cues provide redundant information to existing sensory cues such as vision, or if the haptic information does not reduce the uncertainty of controllable system dynamics. Due to the large sources of control-signal noise present in amputees, the proposed work will lead to improved techniques within the fields of computational motor control and optimal control. This research builds on the team?s extensive experience in the design and control of upper-limb prostheses and in developing the field of computational motor control. Achievement of the proposed aims will contribute to the field of robotic control and to such diverse fields as human-robot interaction, perception, manipulation, and exoskeletons.<br/><br/>Broader Impacts: True biomimetic prostheses, exoskeletons, and humanoid robot control will not be possible until there is a firm understanding of how humans integrate with these co-robots in the face of interacting sources of uncertainty. This computational motor project will provide transformative insight into how humans control movement in the presence of large uncertainty and thus fill a critical gap in the knowledge base of this field. The framework developed in this research will be of great interest to the motor-control research community and may be useful in the restoration of other movement disorders such as spinal cord injury and stroke. The lead institution of this proposal, the Rehabilitation Institute of Chicago (RIC), is consistently ranked the top rehabilitation hospital in the country. The close proximity of research and clinical excellence within RIC ensures that the benefits resulting from this work will be quickly disseminated to prosthesis users. The research team will also seek to reach a broader audience?the laboratories at the RIC are regularly visited by students from local high schools and universities, and the RIC also contributes to outreach activities within inner-city Chicago. These outreach programs promote an awareness of rehabilitation research and an enthusiasm for pursuing a career in engineering. Additionally, the team will develop a K-12 educational module based on the template of the successful Summer School in Computational Sensory-Motor Neuroscience developed at Northwestern University and Queen?s University, which will provide a combination of theory and student-driven experimentation using games that will address many of the Illinois Learning Standards in science, math, and English language arts."
"1523767","NRI: Learning to Plan for New Robot Manipulation Tasks","IIS","National Robotics Initiative","09/01/2015","09/17/2015","Tomas Lozano-Perez","MA","Massachusetts Institute of Technology","Continuing grant","Hector Munoz-Avila","08/31/2018","$900,000.00","Leslie Kaelbling","tlp@csail.mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","8086","$0.00","Robots have great potential societal benefits, especially working with humans in tasks such as manufacturing, disaster relief and elder care. Robots are however very difficult to program to perform new tasks: non-programmers can teach relatively stereotyped action sequences and expert programmers can generate more elaborate action strategies through long programming and debugging processes. Part of the difficulty stems from trying to teach the robot at the level of actions, since the actions to achieve a desired effect depend strongly on details of the environment. Instead, this project focuses on teaching the robot models of the environment. The robot can then use these models to plan its actions automatically. This approach leads to more adaptable behavior. Models are also easier to extend and re-use than action sequences, thereby reducing the burden for teaching subsequent tasks. The project involves a thorough integration of research and education. Graduate and undergraduate students are involved in all aspects of the research. Furthermore, the research in this project will become part of an undergraduate subject on robot algorithms at MIT.<br/><br/>This project will develop techniques to teach a robot to perform long-horizon tasks in complex, uncertain domains, in a way that equips the robot with knowledge it can re-use and re-combine with previous knowledge to solve not just the task it was taught, but a broad array of additional tasks. Furthermore, the robot will be aware of its own knowledge and lack of knowledge, and will be able to plan to take actions, including performing experiments and asking humans for further information, to improve its own knowledge about how to behave in its environment. The project will develop a set of machine learning tools that will allow humans to, relatively quickly and straightforwardly, teach the basic ideas of a new domain to the robot, and then enable to robot to continue to improve its knowledge as it gains experience in the domain. This project will build on a new hierarchical framework for integrating robot motion planning, symbolic planning, purposive perception and decision-theoretic reasoning. The framework, as it stands, supports planning and execution to achieve pick-and-place tasks in complex domains that may require moving objects out of the way, using real, noisy, robot perception and actuation. However, it requires a specification of the domain it is to operate in. In our existing implementation, the domain description was written by hand, by experts, through a long period of trial-and-error. The concrete objective of the project is to develop methods enabling a robot to learn to perform high-level tasks in new domains by acquiring new domain models through human-provided examples and advice. These methods will be evaluated in three domains using a Willow Garage PR2 mobile manipulation robot. The overriding objective will be to develop methods that apply broadly and can be used to instruct robots to perform a wide variety of tasks."
"1525900","NRI: Collaborative Research: ASPIRE: Automation Supporting Prolonged Independent Residence for the Elderly","IIS","National Robotics Initiative","09/01/2015","08/18/2015","Xiaofeng Wang","SC","University of South Carolina at Columbia","Standard Grant","Alexander Leonessa","08/31/2018","$203,347.00","","wangxi@cec.sc.edu","Sponsored Awards Management","COLUMBIA","SC","292080001","8037777093","CSE","8013","010E, 8086, 9150","$0.00","Because of the graying of the population, there is a growing need for new assistive technologies to aid the elderly in their daily living. Based on figures provided by the U.S. Census Bureau, and due largely to the aging of the ""baby boomer"" generation, the population of U.S. adults who are 65 and older is projected to be twice as large in 2030 as it was in 2000, increasing from 35 million to 71.5 million and representing nearly 20 percent of the total U.S. population. This trend is placing enormous burdens on health care costs and causing disruptive changes to how individuals and families manage key late-in-life decisions, including residence. A recent (2015) report by the U.S. Department of Housing and Urban Development concluded that most seniors would prefer to age in place, and a 2010 survey found that 88% of respondents over 65 preferred to remain in their homes as long as possible. It is important to note that these residential preferences must typically be viewed in light of alternatives that are likely to be much more expensive and/or more socially taxing, such as older adults living instead in hospitals, assistive living facilities or with family members. While estimates of the costs associated with these trends vary greatly, it is almost certain that extending the portions of older adults' life spans in which they can live safely and independently through technological means could have enormous positive societal impact. In order to assist in successful aging in place, assistive robots have been developed in the past few decades. However, very few of them have become commercially available, and their use in domesticated environments remains highly limited. The major cause of the problem is that assistive robots typically take the form of full-size humanoid devices or something equally as cumbersome, expensive, and limited in movement and function. We propose a novel assistive robotic system that provides: (i) flexibility, allowing the designed system to be personalized based on users' needs without demanding any home modification upon installation; (ii) safety, ensuring that the system development process accounts for perceived safety by the user, and that the underlying theoretical framework guarantees collision avoidance; (iii) usability, consisting of a minimal and intuitive user interface to provide acceptable controls; (iv) reduced costs, with respect to currently available solutions on the market. The idea behind this research project is the development of a general framework that enables a team of unmanned ground vehicles and small multirotor unmanned aerial vehicles to safely cooperate with the elderly in a home environment. Equipped with appropriate human-machine interfaces, the co-robots will be able to accomplish a number of tasks as demanded by the users. <br/><br/>The project addresses fundamental problems in the domain of multi-agent cooperative systems, comprised of humans and co-robots interacting in shared, highly constrained spaces. In order to assist humans, the co-robots have to be trusted by humans, implying that their behaviors be predictable and consistent with principles of human spatial perception, and their appearance must foster a high level of comfort and not create high cognitive demands on the user. Inspired by these challenges, this proposal focuses on the design and control of co-robots, which can adapt to unstructured and rapidly changing environments in a manner consistent with human perception and cognition, thus enhancing safety and robustness. The key focus areas include the design and acceptance of mobile ground and aerial robots that coexist in environments inhabited by humans and the development of a multi-objective control framework to allow intuitive user control over an ensemble of co-robots, which includes the design of both low-level controllers (LLC) and a supervisory, high-level controller (HLC). To demonstrate the benefits of the framework and to engage student groups from various, diverse populations, the following scenario will be considered as a test case: multirotor unmanned aerial vehicles and ground robots acting as domestic assistive devices for healthy older adults in a research laboratory. These co-robots will safely navigate the shared space and accomplish domestic tasks requested by humans while displaying behaviors and appearances that are perceived as safe and trusted. Humans will use an intuitively designed interface for both controlling and monitoring co-robots on a tablet or a smartphone device. A motion capture system and virtual reality Cube at the Beckman Institute will provide the context for data collection, iterative testing and validation."
"1528047","NRI: Planning, Collaborative Guidance and Navigation in Uncertain Dynamic Environments","IIS","National Robotics Initiative","08/01/2015","08/04/2015","Lydia Tapia","NM","University of New Mexico","Standard Grant","Hector Munoz-Avila","07/31/2018","$999,998.00","Meeko Oishi, Patrick Kelley","tapia@cs.unm.edu","1700 Lomas Blvd. NE, Suite 2200","ALBUQUERQUE","NM","871310001","5052774186","CSE","8013","8086, 9150","$0.00","Navigation in dynamic, uncertain environments is a difficult yet ubiquitous problem in diverse applications such as search and rescue, coordinated movement, distributed monitoring and surveillance. Collaborative solutions are highly promising because of their potential to exploit strengths of both the human and the automation. However, major challenges in collaborative navigation include not only the ability of the underlying automation to effectively handle novel scenarios and changing environments that may not have been considered at the design stage, but also human-automation interaction requirements. The design of methods and tools that can address these challenges could enable fundamentally new functionality in collaborative human-robot systems. The novelty of the proposed research is in the integration of control theory, motion planning, and human guidance to provide highly effective solutions for navigation in highly dynamic and uncertain environments. The proposed project will also create opportunities to involve under-represented minorities in K-12 outreach and in undergraduate and graduate research, to facilitate interdisciplinary collaboration, and to develop a new interdisciplinary graduate course. <br/><br/>This proposal aims to develop a generic framework for collaborative navigation in complex environments that can accommodate hundreds of moving obstacles (with possibly stochastic dynamics), non-trivial static obstacles, and humans in the loop. We propose to a) evaluate the tradeoff between short-term and long-term information for both users and autonomous systems in highly dynamic environments, b) extend our existing algorithmic techniques to environments of higher complexity, e.g., multi-robots and non-planar environments, c) design and test several user-interfaces, which satisfy pre-determined conditions for user-observability and user-predictability, for their effectiveness in improving safe navigation, and d) experimentally validate our existing setup for collaborative navigation in dynamic, uncertain environments via an Android app. We will develop tightly coupled planning and control tools, integrate human guidance and decision making with automated tools, and complete a rigorous analysis of safety in highly dynamic environments with uncertainty. The developed methods will be validated in multiple environments, with human subjects, and on a micro robot testbed."
"1638072","NRI: Collaborative Research: Learning Adaptive Representations for Robust Mobile Robot Navigation from Multi-Modal Interactions","IIS","National Robotics Initiative","10/01/2016","08/10/2016","Matthew Walter","IL","Toyota Technological Institute at Chicago","Standard Grant","Reid Simmons","09/30/2019","$332,728.00","","mwalter@ttic.edu","6045 S. Kenwood Avenue","Chicago","IL","606372902","7738340409","CSE","8013","8086","$0.00","Most existing autonomous systems reason over flat, task-dependent models of the world that do not scale to large, complex environments. This lack of scalability and generalizability is a significant barrier to the widespread adoption of robots for common tasks. This research will advance the state-of-the-art in robot perception, natural language understanding, and learning to develop new models and algorithms that significantly improve the scalability and efficiency of mapping and motion planning in large, complex environments. These contributions will impact the next generation of autonomous systems that interact with humans in many domains, including manufacturing, healthcare, and exploration. Outcomes will include the release of open source software and data, workshops, K-12 STEM outreach efforts, and undergraduate and graduate education in the unique, multidisciplinary fields of perception, natural language understanding, and motion planning.<br/><br/>As robots perform a wider variety of tasks within increasingly complex environments, their ability to learn and reason over expressive models of their environment becomes critical. The goal of this research is to develop models and algorithms for learning adaptive, hierarchical environment representations that afford efficient planning for mobility tasks. These representations will take the form of probabilistic models that capture the rich spatial-semantic properties of the robot's environment and are factorable to enable scalable inference. This research will develop algorithms that learn and adapt these representations by fusing knowledge conveyed through human-provided natural language utterances with information extracted from the robot's multimodal sensor streams. This research will develop algorithms that then reason over the complexity of these models in the context of the inferred task, thereby identifying simplifications that enable more efficient robot motion planning."
"1637889","NRI: A Model based Approach to Distributed Adaptive Sampling of Spatio-Temporally Varying Fields","ECCS","National Robotics Initiative","09/01/2016","08/10/2016","Suman Chakravorty","TX","Texas A&M Engineering Experiment Station","Standard Grant","Radhakisan S. Baheti","08/31/2019","$499,996.00","Dylan Shell","schakrav@aero.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","ENG","8013","8086","$0.00","This research project studies the problem of designing active sensing systems for monitoring dynamically evolving spatial fields using mobile robotic sensor networks. As a particular motivating problem, we consider fields governed by advection-diffusion equations, a model sufficiently general to cover a huge range of important phenomena: from the recent Aliso Canyon gas leak in California and the volcanic ash clouds of Eyjafjallajokull, to the temperature profile within a building. The development of a realistic open-source simulation toolbox for the active sensing problem will allow the assimilation of K-12/undergraduate/graduate students, and high school teachers in projects related to the research, and also allow a broader dissemination of the research to the general public at the annual TAMU Physics and Engineering fair while educating them about the benefits of the project, for instance, in response to a hazardous situation such as a chemical leak or an oil spill.<br/><br/>In the current literature, statistical black-boxes (such as Gaussian Processes), which were originally developed for (quasi)-static spatial fields, are being used to model fields with structured temporal dynamics. In this process, two issues which ought to be distinct, the correctness of the model, and considerations of computational efficiency, have become entangled and the consequences can be dangerous: state-of-the-art methods may provide cheap but drastically wrong estimates, along with error bounds that are grossly over-confident when the spatial fields are temporally varying. The investigators will seek to produce adaptive estimation techniques for dynamic spatial fields that are optimal and correct. In particular, randomized model reduction techniques shall be used to attain computational tractability whilst preserving correctness. Further, the project shall seek to develop receding horizon sensor tasking strategies that can drastically outperform greedy strategies in terms of the information content of the estimated field."
"1638060","NRI: Collaborative Research: Sketching Geometry and Physics Informed Inference for Mobile Robot Manipulation in Cluttered Scenes","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","09/01/2016","08/10/2016","Joseph LaViola","FL","University of Central Florida","Standard Grant","Reid Simmons","08/31/2019","$286,434.00","","jjl@cs.ucf.edu","4000 CNTRL FLORIDA BLVD","ORLANDO","FL","328168005","4078821120","CSE","7495, 8013","8086","$0.00","The goal of this project is to improve the ability of robots to manipulate and interact with objects, such as when assisting people to support their daily activities. The key idea is that people can provide robots with important information about their environment and the objects within their environment. In particular, people can use their cognitive skills to name objects, provide an understanding of the geometrical structure of objects, and describe an object's behavior in relation to other objects. Specifically, the project will develop a natural user interface that enables people to provide such information by drawing and sketching on top of the robot's view of the world. Physical simulation will then be used to fill in the missing gaps needed for a robot to complete autonomous manipulation tasks. Thus, the project aims to combine object sketching and physical simulation to better support mobile manipulation tasks as well as learn to perform new manipulation tasks when encountered. The project will support a ""Put That There"" task, where a user can simply give high-level manipulation commands, with the robot filling in the details necessary to complete the task in a cluttered environment.<br/><br/>This project aims to improve goal-directed dexterous robotic manipulation in cluttered and unstructured environments through sketching and physical simulation. Robots operating in human environments face considerable uncertainty in perception due to physical contact and occlusions between objects. This project will address such perceptual uncertainty by combining methods for probabilistic inference with natural sketch-based interfaces to extract, label, and automatically infer the geometry, pose, and behavior of objects in complicated scenes. From a human usability perspective, the project addresses how to best create a sketching language and interfaces for intuitive human-in-the-loop extraction of object geometries and behavior from robot sensing. The planned exploration into sketching methods will also explore what underlying representations, raw point clouds, RGB images and video, or RGBD images will be most conducive to supporting accurate geometry extraction and grasp location identification. Given sketched objects, the project will develop probabilistic physically plausible methods for scene estimation that will enable perception for manipulation in cluttered environments. These methods build upon advances in physical simulation to constrain scene estimates to only plausible configurations to both improve estimation accuracy and enable computational tractability. The project will also develop a ""Put That There"" testbed using a tablet-based web application to support exploration of these concepts as well as act as user studies to evaluate geometry extraction accuracy and the robustness of physics-based scene estimation algorithms."
"1638047","NRI: Collaborative Research: Sketching Geometry and Physics Informed Inference for Mobile Robot Manipulation in Cluttered Scenes","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","09/01/2016","08/10/2016","Odest Jenkins","MI","University of Michigan Ann Arbor","Standard Grant","Reid Simmons","08/31/2019","$400,857.00","","ocj@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7495, 8013","7495, 8086","$0.00","The goal of this project is to improve the ability of robots to manipulate and interact with objects, such as when assisting people to support their daily activities. The key idea is that people can provide robots with important information about their environment and the objects within their environment. In particular, people can use their cognitive skills to name objects, provide an understanding of the geometrical structure of objects, and describe an object's behavior in relation to other objects. Specifically, the project will develop a natural user interface that enables people to provide such information by drawing and sketching on top of the robot's view of the world. Physical simulation will then be used to fill in the missing gaps needed for a robot to complete autonomous manipulation tasks. Thus, the project aims to combine object sketching and physical simulation to better support mobile manipulation tasks as well as learn to perform new manipulation tasks when encountered. The project will support a ""Put That There"" task, where a user can simply give high-level manipulation commands, with the robot filling in the details necessary to complete the task in a cluttered environment.<br/><br/>This project aims to improve goal-directed dexterous robotic manipulation in cluttered and unstructured environments through sketching and physical simulation. Robots operating in human environments face considerable uncertainty in perception due to physical contact and occlusions between objects. This project will address such perceptual uncertainty by combining methods for probabilistic inference with natural sketch-based interfaces to extract, label, and automatically infer the geometry, pose, and behavior of objects in complicated scenes. From a human usability perspective, the project addresses how to best create a sketching language and interfaces for intuitive human-in-the-loop extraction of object geometries and behavior from robot sensing. The planned exploration into sketching methods will also explore what underlying representations, raw point clouds, RGB images and video, or RGBD images will be most conducive to supporting accurate geometry extraction and grasp location identification. Given sketched objects, the project will develop probabilistic physically plausible methods for scene estimation that will enable perception for manipulation in cluttered environments. These methods build upon advances in physical simulation to constrain scene estimates to only plausible configurations to both improve estimation accuracy and enable computational tractability. The project will also develop a ""Put That There"" testbed using a tablet-based web application to support exploration of these concepts as well as act as user studies to evaluate geometry extraction accuracy and the robustness of physics-based scene estimation algorithms."
"1651792","EAGER: Dexterous Robotic Cutting","IIS","National Robotics Initiative","10/01/2016","08/10/2016","Yan-Bin Jia","IA","Iowa State University","Standard Grant","Ralph Wachter","09/30/2018","$299,858.00","","jia@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","8013","7916, 8086","$0.00","This research will form a theory of robotic cutting and manipulation of soft and irregularly shaped objects. Applications of the results are leading to more skillful dexterous robots geared for a range of industrial applications, improving safety risks, and increasing reliability and consistency. <br/><br/>The technical goal of this project is to understand in depth about manipulation of delicate, flexible, and slippery items, handling of tools with skills, coordination among robotic arms and hands, and motion planning and control based on multi-modality sensing and deformable modeling. The project consists of two phases. The first phase develops strategies for basic maneuvers of cutting devices, tackling issues that include vision-guided hand placements, large deformable modeling in real time, pickup and stabilization, and dexterous grasping and re-grasping. The second phase investigates coordination of two robotic arm-hand pairs to carry out cutting, and synthesizes trajectories and controls to implement different cutting skills. Technical issues to address in this phase include material viscoelasticity, contact detection, cut planning and execution on the fly, arm and hand trajectory planning, and fusion of visual and force data."
"1637915","NRI: Coordinated Detection and Tracking of Hazardous Agents with Aerial and Aquatic Robots to Inform Emergency Responders","IIS","National Robotics Initiative","10/01/2016","08/10/2016","Pratap Tokekar","VA","Virginia Polytechnic Institute and State University","Standard Grant","Jeffrey Trinkle","09/30/2019","$900,835.00","David Schmale","tokekar@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8013","8086","$0.00","New tools and technology are needed to rapidly assess hazardous agents in the environment and inform emergency responders. Unmanned surface vehicles (USVs) have been used to monitor pollutant plumes in aquatic environments. However, USVs can only provide a close-up view of the plumes. Unmanned aerial systems (UASs) can survey large areas, but only from a distance. This project addresses the challenges in tracking flows of pollutants using a team of UASs and USVs. UASs are used as scouts to direct USVs to efficiently sample the water for testing. The results will provide emergency responders with technology that can provide rapid, actionable information on the dispersal of hazardous agents. This technology will be of immediate value in the development of early-warning systems for airborne hazards such as chemicals and radioactive particles. In an effort to encourage careers in robotics and emergency response, this project delivers a unique interdisciplinary robotics unit for 75 high school students. <br/><br/>Efficient coordination algorithms are required to fully exploit heterogeneity in sensing of teams of autonomous surface and aerial vehicles. When moving through the water, a plume of hazardous agents may bifurcate into different flow regimes, complicating the sampling and control scenario. This project: (1) develops approximation algorithms for multi-resolution, informative trajectory planning to track spatio-temporal plumes with UAS teams; (2) develops an autonomous USV to sample and characterize a surrogate hazardous agent in the water and air; (3) conducts field sampling campaigns with emergency responders (Virginia Tech Rescue Squad, a unique group of stakeholders consisting of >50 student members) to find and localize sources of a surrogate hazardous agent; and (4) develops a unique interdisciplinary robotics unit for 75 high school students representing a range of ages, ethnicities, and socioeconomic classes."
"1637813","NRI: Collaborative Research: Learning Adaptive Representations for Robust Mobile Robot Navigation from Multi-Modal Interactions","IIS","National Robotics Initiative","10/01/2016","08/10/2016","Thomas Howard","NY","University of Rochester","Standard Grant","Reid Simmons","09/30/2019","$289,376.00","","thoward@cs.rochester.edu","518 HYLAN, RC BOX 270140","Rochester","NY","146270140","5852754031","CSE","8013","8086","$0.00","Most existing autonomous systems reason over flat, task-dependent models of the world that do not scale to large, complex environments. This lack of scalability and generalizability is a significant barrier to the widespread adoption of robots for common tasks. This research will advance the state-of-the-art in robot perception, natural language understanding, and learning to develop new models and algorithms that significantly improve the scalability and efficiency of mapping and motion planning in large, complex environments. These contributions will impact the next generation of autonomous systems that interact with humans in many domains, including manufacturing, healthcare, and exploration. Outcomes will include the release of open source software and data, workshops, K-12 STEM outreach efforts, and undergraduate and graduate education in the unique, multidisciplinary fields of perception, natural language understanding, and motion planning. <br/><br/>As robots perform a wider variety of tasks within increasingly complex environments, their ability to learn and reason over expressive models of their environment becomes critical. The goal of this research is to develop models and algorithms for learning adaptive, hierarchical environment representations that afford efficient planning for mobility tasks. These representations will take the form of probabilistic models that capture the rich spatial-semantic properties of the robot's environment and are factorable to enable scalable inference. This research will develop algorithms that learn and adapt these representations by fusing knowledge conveyed through human-provided natural language utterances with information extracted from the robot's multimodal sensor streams. This research will develop algorithms that then reason over the complexity of these models in the context of the inferred task, thereby identifying simplifications that enable more efficient robot motion planning."
"1638099","NRI: Enabling Unmanned Aerial Systems (UAS) Fire Ignitions in Complex Firefighting Contexts","IIS","National Robotics Initiative","08/01/2016","08/09/2016","Sebastian Elbaum","NE","University of Nebraska-Lincoln","Standard Grant","Jeffrey Trinkle","07/31/2019","$995,470.00","Dirac Twidwell, Brittany Duncan, Carrick Detweiler, Justin Bradley","elbaum@cse.unl.edu","2200 Vine St, 151 Whittier","Lincoln","NE","685031435","4024723171","CSE","8013","8086, 9150","$0.00","Prescribed fire is critical for reducing catastrophic wildfires and sustaining healthy ecosystems. Yet the technology to support fire ignition and monitoring remains stagnant, risky, and expensive. This project aims to develop the Unmanned Aerial System (UAS) technology that can transform prescribed fire ignition and monitoring by: 1) enabling the communication between UASs and humans by sharing the vehicle intention through maneuvers, 2) improving UAS operation by taking into account operator availability, 3) leveraging the operator's knowledge to improve control of multiple vehicles, 4) fixing failures by enabling the operator and the system to work together, and 5) assessing the technological capabilities and associated users' acceptance of this technology. This effort is significant because it addresses unique co-robotic challenges in the UAS domain and is transformative in its potential to change how a range of organizations maintain their ecosystems and manage wildfires.<br/><br/>The project aims at developing and assessing techniques, tools, and systems to dramatically improve the potential for UASs to safely ignite and monitor fire. To achieve that goal, it conducts multidisciplinary work on: 1) motion-based languages that communicate UAS intention and knowledge to operators and bystanders, 2) co-regulation methodologies that incorporate operator availability and attention into traditional control and planning loops, 3) integrative functions that map the environmental knowledge and domain expertise of an operator into a fleet of vehicles to support different levels of autonomy, 4) co-debugging techniques from program analysis that collaborate with the operator to help diagnose and overcome failures caused by misconfigurations, and 5) cross-cutting studies to gain a better understanding of the attitudes of stakeholders towards UASs, and the features that are likely to promote stakeholder trust and acceptance."
"1317775","NRI: Small: Collaborative Research: Learning from Demonstration for Cloud Robotics","IIS","National Robotics Initiative","10/01/2013","04/15/2014","Sonia Chernova","MA","Worcester Polytechnic Institute","Standard Grant","Reid Simmons","09/30/2017","$433,351.00","","chernova@cc.gatech.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","8013","7923, 8086, 9251","$0.00","The proposed work seeks to leverage cloud computing to enable robots to efficiently learn from remote human domain experts - ""Cloud Learning from Demonstration."" Building on RobotsFor.Me, a remote robotics research lab, this research will unite Learning from Demonstration (LfD) and Cloud Robotics to enable anyone with Internet access to teach a robot household tasks. The value of this work stems from three aspects. First is the remote system that can learn task models from a series of remote demonstrations from a single user, focusing on learning high-level tasks as opposed to low-level motor skills. The second is the extension of learning from demonstration to multiple teachers. This represents an important relaxation of a limiting assumption to focus on evaluating teacher strengths and effectively handling distinct task solutions. Finally, transparency mechanisms to allow a remote user to develop a correct mental model about the robot?s learning process.<br/><br/>The long term goal of this research is to one day make personal robots accessible to everyday people. The interactive learning framework based on RobotsFor.Me provides unique opportunities for education and outreach. Thomaz and Chernova will outreach to K-12 teachers and students by creating an education portal surrounding RobotsFor.Me containing hands-on workshop curricula. This material will be integrated with the WPI Frontiers program for middle school students, and the GT ePDN professional education network for teachers. A key impact on students at GT and WPI will be direct involvement in this research agenda, and integration with AI, robotics and HRI courses. Chernova is the Diversity Coordinator in the Robotics Engineering Program, and faculty advisor for Women In Robotics Engineering and Women in Technology student groups which will enable braod exposure. Thomaz mentors the RoboWomen graduate women?s group. Software components will also be made available as open source and the PIs have a collaboration plan in place with researchers at Willow Garage, and through student internships will transfer technology to their labs."
"1426452","NRI: Collaborative Research: Jointly Learning Language and Affordances","IIS","National Robotics Initiative","08/01/2014","08/10/2016","Stefanie Tellex","RI","Brown University","Standard Grant","Jeffrey Trinkle","07/31/2017","$347,622.00","","stefie10@cs.brown.edu","BOX 1929","Providence","RI","029129002","4018632777","CSE","8013","7218, 8086, 9150","$0.00","The investigators of this project envision a world where robots surround us, in our homes, in our hospitals, and in our factories, helping people by delivering medicine, preparing food, and assembling objects. Achieving this vision requires robots to communicate with people about their needs, and then plan their activities to help meet those needs. Previous research has addressed these two problems separately, leading to technical solutions that do not work reliably in real-world situations, and to difficulties in human-robot communication. To solve these problems, we are developing the Physically-Grounded Language with Affordances (PGLA) framework and concentrate our research into two thrusts: 1) enable a robot to observe a patient, then answer a nurse's questions about the patient's activity, and 2) enable a robot to respond to natural language requests in a collaborative cooking task and in a manufacturing setting. We will release our open-source data sets and code, which will have impact in other technical areas beyond robotics, such as computer vision and machine learning. The results of our proposed research will find direct applications in industries such as manufacturing and assistive robotics.<br/><br/>This project takes a probabilistic approach to jointly learn to recognize affordances in the environment and predict associated natural language requests and descriptions. Since the affordance map is grounded to perceptual data, our robots will learn to robustly manipulate objects in the physical world, respond to natural language commands, and describe their experiences using words. Our learning approach enables the robot to infer cross-model knowledge from large data sets of people carrying out activities paired with natural language descriptions of the activities, leveraging the strength of each modality to inform the others. Our novel learning algorithms will integrate and learn from multi-domain databases such as the semantic web, visual scenes, and a novel activity database paired with natural language descriptions."
"1527747","NRI: Collaborative Research: Multimodal Brain Computer Interface for Human-Robot Interaction","IIS","National Robotics Initiative","05/15/2016","05/27/2016","Peter Allen","NY","Columbia University","Standard Grant","Tatiana D. Korelsky","04/30/2019","$736,552.00","Paul Sajda","allen@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8013","8086, 8089","$0.00","Human Robot Interaction (HRI) is research that is a key component in making robots part of our everyday life. Current interface modalities such as video, keyboard, tactile, audio, and speech can all contribute to an HRI interface. However, an emerging area is the use of Brain-Computer Interfaces (BCI) for communication and information exchange between humans and robots. BCIs can provide another channel of communication with more direct access to physiological changes in the brain. BCIs vary widely in their capabilities, particularly with respect to spatial resolution, temporal resolution and noise. This project is aimed at exploring the use of multimodal BCIs for HRI. Multimodal BCIs, also referred to as hybrid BCIs (hBCI), have been shown to improve performance over single modality interfaces. This project is focused on using a novel suite of sensors (Electroencephalography (EEG), eye-tracking, pupillary size, computer vision, and functional Near Infrared Spectroscopy (fNIRS)) to improve current HRI systems. Each of these sensing modalities can reinforce and complement each other, and when used together, can address a major shortcoming of current BCIs which is the determination of the user state or situational awareness (SA). SA is a necessary component of any complex interaction between agents, as each agent has its own expectations and assumptions about the environment. Traditional BCI systems have difficulty recognizing state and context, and accordingly can become confusing and unreliable. This project will develop techniques to recognize state from multiple modalities, and will also allow the robot and human to learn about each other's state and expectations using the hBCI we are developing. The goal is to build a usable hBCI for real physical robot environments, with noise, real-time constraints, and added complexity.<br/><br/>The technical contributions of this project include:<br/>1. Characterization of a novel hBCI interface for visual recognition and labeling tasks with real physical data and environments.<br/>2. Integration of fNIRS sensing with EEG and other modalities in human robot interaction tasks. We will test our ability in the temporal domain to determine at what timescale we can correctly classify movement components that would predict a correct (rewarding) trial or non-rewarding/incorrect movement.<br/>3. Analysis and validation of the hBCI in complex robotic tele-operation tasks with human subject operators such as open door, grasp object on table, pick up item off floor etc.<br/>4. Use of hBCI to characterize human/robot state and create a learning method to recognize state over time.<br/>5. Use of augmented reality for HRI decision making.<br/>6. Further develop hBCI for tracking cognitive states related to reward, motivation, attention and value.<br/>A new class of HRI interfaces will be developed that can expand the ability of humans to work with robots; promote the use and acceptance of robot agent systems in everyday life; expand the use of hBCIs in areas other than robotics for human-machine interaction; further the development of hBCIs as our system will be tapping into reward modulated activity that will be used via reinforcement learning to autonomously update the learning machinery; and bridge the educational divide between Engineering/Computer Science and Neuroscience."
"1527828","NRI: Collaborative Goal and Policy Learning from Human Operators of Construction Co-Robots","IIS","National Robotics Initiative","08/01/2015","08/07/2015","Girish Chowdhary","OK","Oklahoma State University","Standard Grant","Reid Simmons","07/31/2018","$900,000.00","Christopher Crick, Charles Abramson, Prabhakar Pagilla","girish.chowdhary@okstate.edu","101 WHITEHURST HALL","Stillwater","OK","740781011","4057449995","CSE","8013","8086, 9150","$0.00","The overall goal of this research is to investigate and significantly advance the science of collaborative interaction between human operators and co-robots. This includes the development of algorithms that can be used to train co-robots from skilled human operators to efficiently perform complex tasks in the face of real-world uncertainty, and to guide novice operators in performing such tasks. The primary targeted application is the construction and farming equipment industry that includes complex co-robots such as excavators, wheel loaders, tractors, forage harvesters where there is a significant need to understand and improve human-robot collaborative learning.<br/><br/>There are significant scientific challenges in developing efficient algorithms for co-robots that can actively learn from skilled human operators by observing and posing appropriate queries to close the feedback loop between the co-robot and the human operator. This project addresses these challenges by systematically formulating and investigating focused problems to create efficient algorithms that can enhance collaborative human-robot learning. To achieve the goal, algorithms are designed to collaboratively learn latent subgoal structures from ill-defined complex tasks, real-time path planning and control algorithms are developed for co-robots to achieve the learned subgoals, and techniques are developed to provide operator skill specific task decomposition and motion execution guidance. In addition, the developed algorithms are corroborated by simulators, hardware experimentation on laboratory and field co-robots, and theoretical analysis."
"1327657","NRI: Large: Collaborative Research: Complementary Situational Awareness for Human-Robot Partnerships","IIS","COLLABORATIVE RESEARCH, IIS SPECIAL PROJECTS, National Robotics Initiative","10/01/2013","04/17/2015","Russell Taylor","MD","Johns Hopkins University","Continuing grant","Jeffrey Trinkle","09/30/2018","$1,228,609.00","","rht@cs.jhu.edu","3400 N CHARLES ST","Baltimore","MD","212182608","4105168668","CSE","7298, 7484, 8013","5946, 7925, 8086, 9251","$0.00","This work will advance human-robot partnerships by establishing a new concept called complementary situational awareness (CSA), which is the simultaneous perception and use of the environment and operational constraints for task execution. The proposed CSA is transformative because it ushers in a new era of human-robot partnerships where robots act as our partners, not only in manipulation, but in perception and control. This research will establish the foundations for CSA to enable multifaceted human-robot partnerships. Three main research objectives guide this effort: 1) Real-time Sensing during Task Execution: design low-level control algorithms providing wire-actuated or flexible continuum robots with sensory awareness by supporting force sensing, exploration, and modulated force interaction in flexible unstructured environments; 2) Situational Awareness Modeling: prescribe information fusion and simultaneous localization and mapping (SLAM) algorithms suitable for surgical planning and in-vivo surgical plan adaptation; 3) Telemanipulation based on CSA: Design, construct, and integrate robotic testbeds with telemanipulation algorithms that use SLAM and exploration data for online adaptation of assistive telemanipulation virtual fixtures. This research also includes investigation of previously unaddressed questions on how sensory exploration and palpation data can be used to enable online-adaptation of assistive virtual fixtures based on force and stiffness data while also taking into account preoperative data and intraoperative correction of registration parameters.<br/><br/>The proposed work will restore the situational awareness readily available in open surgery to minimally invasive surgery. This will benefit patients by enabling core technologies for effective and safe natural orifice surgery or single port access surgery. The societal impact of the proposed work on these two surgical paradigms is reduced pain for patients, shorter hospital stay, improved cosmesis and patients' self image, and lower costs. We also believe that CSA will impact manufacturing where its future will require people and robots working together in a shared space on collaborative tasks. Also, the same concepts of CSA apply to telemanipulation in constrained and unstructured environments and the proposed research has direct relevance to robot-human partnerships for space exploration. To ensure this broader impact will be achieved, an advisory board has been assembled with experts from medicine, manufacturing and aerospace. Finally, the PIs will facilitate collaboration in the medical robotics research community by making our software and hardware designs available on-line and using commercial-grade hardware available at multiple institutions."
"1427425","NRI: Collaborative Research: Shall I Touch This?: Navigating the Look and Feel of Complex Surfaces","IIS","National Robotics Initiative","07/15/2014","08/18/2014","Trevor Darrell","CA","University of California-Berkeley","Standard Grant","Jie Yang","06/30/2017","$600,000.00","","trevor@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","8013","8086","$0.00","This project improves autonomous robotic perception so that future co-robots can glance around any scene and accurately estimate how it would feel to grasp or step on all of the visible surfaces. Just as people do, robots should use such these physical predictions to guide their interactions with the world, for example avoiding dangerous ice patches on the ground when walking and driving, and adeptly anticipating the grasp force needed to pick up everything from ice cubes to stuffed animals. These research activities are accompanied by significant outreach efforts, including a new program on ""Look and Touch Robotics"" to get middle-school students, particularly those from underrepresented groups, excited about computer science, engineering, and robotics. This program uses simple experiments to highlight the dual importance of visual and haptic information during interactions with physical objects, along with demonstrations of a robot showing visuo-haptic intelligence. This project also integrates research and education by involving undergraduates in the research and via hands-on projects in the vision and robotics classes taught by the Principal Investigators.<br/><br/>This research involves extensive collection of data from real objects and surfaces using both visual and haptic sensors. The recorded interactions are analyzed to uncover visual clues that can allow a robot to infer the physical characteristics of the surface, such as slipperiness, hardness, and roughness. This problem is addressed using deep learning, a recently developed approach that has been successful in enabling robots to visually recognize a wide variety of objects in diverse circumstances. The research team also builds the database of visuo-haptic recordings and the learned cross-modal sensory, and makes it available to other robotics researchers at the end of the project."
"1514649","RAPID: Robot-assisted Doffing of Personal Protective Equipment","IIS","INFORMATION TECHNOLOGY RESEARC, National Robotics Initiative","12/01/2014","12/02/2014","Dmitry Berenson","MA","Worcester Polytechnic Institute","Standard Grant","Jeffrey Trinkle","01/31/2016","$75,243.00","Taskin Padir","berenson@eecs.umich.edu","100 INSTITUTE RD","WORCESTER","MA","016092247","5088315000","CSE","1640, 8013","001Z, 1640, 7914, 8013","$0.00","The goal of this Rapid Response Research is to create a human-robot system that will assist in the ``doffing'' (i.e., removal) of Personal Protective Equipment (PPE) worn by health-care workers treating Ebola. Because the PPE has multiple layers of clothing and involves numerous steps, and because doffing must be repeated often under stressful conditions, the process poses a significant risk of infection for health-care workers. <br/><br/>This proposal seeks to use a robot to minimize the amount of contact between the worker's hands (covered with gloves) and the PPE, as this contact may enlarge the areas of the PPE that are contaminated and thus increase the risk of infecting the worker. To accomplish this goal, the PI and co-PI seek to develop manipulation strategies and primitives that will allow the robot to assist in the doffing process by using its hands as hooks, braces, and clamps so that the health-care worker uses their hands as little as possible during the doffing process. The work will be performed in close consultation with medical professionals to ensure that the developed procedures are both safe and provide an advantage over the manual doffing process."
"1602141","EAGER: A Soft Contact Model for Simulation and Optimization","IIS","National Robotics Initiative","06/15/2016","06/07/2016","Emanuil Todorov","WA","Roboti LLC","Standard Grant","Jeffrey Trinkle","05/31/2017","$183,870.00","","etodorov@gmail.com","17315 NE 126th Place","Redmond","WA","980522295","8586997740","CSE","8013","7916, 8013","$0.00","Robots and humans interact with the world through physical contact. But since modern robots do not yet ""understand"" contact, they have very little capability to figure out how to do useful work. For example, robots today cannot figure out how to help a person with a sprained ankle climb a flight of stairs. This project will develop simulation models of contact that robots can use to ""think"" about useful work requiring contact. Through several mental experiments (simulations) a robot will be able to determine the best strategy to carry out whatever contact task it faces. These contact models will be computationally efficient, so the robot won't have to think too long, and realistic, so that the robot's plan has a high probability of success when executed. Since robots are relatively good at working with hard solid objects, this project focuses on soft bodies, such as people, food, and ropes. The resulting new algorithms will be incorporated in the publicly available physics simulator, MuJoCo, which can be embedded easily into the ""brains"" of future robots to expand the kinds of work robots will be able to do. <br/><br/>The new soft contact model is derived from the Gauss principle of least constraint, applied to an augmented system that includes the rigid-body dynamics and the contact deformation dynamics. This yields a convex conic program in terms of the accelerations of the augmented system. The convex dual of this conic program is an optimization problem over contact forces. The project will develop this new theoretical framework, and explore its many properties including stability as well as computation of dynamics derivatives and their application to physically-consistent state estimation. Fast algorithms for computing both forward and inverse contact dynamics will also be developed. This will include a generalization to the projected Gauss-Seidel method that can handle conic constraints, as well as preconditioned conjugate gradient method exploiting the convexity and smoothness provided by the new framework."
"1527558","NRI: Collaborative Research: Multimodal Brain Computer Interface for Human-Robot Interaction","IIS","National Robotics Initiative","05/15/2016","05/27/2016","Joseph Francis","TX","University of Houston","Standard Grant","Tatiana D. Korelsky","04/30/2019","$308,077.00","","jtfranci@Central.UH.EDU","4800 Calhoun Boulevard","Houston","TX","772042015","7137435773","CSE","8013","8086, 8089","$0.00","Human Robot Interaction (HRI) is research that is a key component in making robots part of our everyday life. Current interface modalities such as video, keyboard, tactile, audio, and speech can all contribute to an HRI interface. However, an emerging area is the use of Brain-Computer Interfaces (BCI) for communication and information exchange between humans and robots. BCIs can provide another channel of communication with more direct access to physiological changes in the brain. BCIs vary widely in their capabilities, particularly with respect to spatial resolution, temporal resolution and noise. This project is aimed at exploring the use of multimodal BCIs for HRI. Multimodal BCIs, also referred to as hybrid BCIs (hBCI), have been shown to improve performance over single modality interfaces. This project is focused on using a novel suite of sensors (Electroencephalography (EEG), eye-tracking, pupillary size, computer vision, and functional Near Infrared Spectroscopy (fNIRS)) to improve current HRI systems. Each of these sensing modalities can reinforce and complement each other, and when used together, can address a major shortcoming of current BCIs which is the determination of the user state or situational awareness (SA). SA is a necessary component of any complex interaction between agents, as each agent has its own expectations and assumptions about the environment. Traditional BCI systems have difficulty recognizing state and context, and accordingly can become confusing and unreliable. This project will develop techniques to recognize state from multiple modalities, and will also allow the robot and human to learn about each other's state and expectations using the hBCI we are developing. The goal is to build a usable hBCI for real physical robot environments, with noise, real-time constraints, and added complexity.<br/><br/>The technical contributions of this project include:<br/>1. Characterization of a novel hBCI interface for visual recognition and labeling tasks with real physical data and environments.<br/>2. Integration of fNIRS sensing with EEG and other modalities in human robot interaction tasks. We will test our ability in the temporal domain to determine at what timescale we can correctly classify movement components that would predict a correct (rewarding) trial or non-rewarding/incorrect movement.<br/>3. Analysis and validation of the hBCI in complex robotic tele-operation tasks with human subject operators such as open door, grasp object on table, pick up item off floor etc.<br/>4. Use of hBCI to characterize human/robot state and create a learning method to recognize state over time.<br/>5. Use of augmented reality for HRI decision making.<br/>6. Further develop hBCI for tracking cognitive states related to reward, motivation, attention and value.<br/>A new class of HRI interfaces will be developed that can expand the ability of humans to work with robots; promote the use and acceptance of robot agent systems in everyday life; expand the use of hBCIs in areas other than robotics for human-machine interaction; further the development of hBCIs as our system will be tapping into reward modulated activity that will be used via reinforcement learning to autonomously update the learning machinery; and bridge the educational divide between Engineering and Neuroscience."
"1526016","NRI: Collaborative Research: Enabling Risk-Aware Decision Making in Human-Guided Unmanned Surface Vehicle Teams","ECCS","National Robotics Initiative","09/01/2015","09/03/2015","Karl von Ellenrieder","FL","Florida Atlantic University","Standard Grant","Radhakisan S. Baheti","08/31/2018","$449,520.00","","ellenrie@fau.edu","777 GLADES RD","BOCA RATON","FL","334316424","5612970777","ENG","8013","092E, 8086","$0.00","Over the last ten years, substantial progress has been made in the development of small low-cost unmanned surface vehicles (USVs). There are a number of civilian applications where deploying a human-robot team consisting of several small USVs and one or more human supervisors can significantly reduce costs, improve safety, and increase operational efficiencies. Representative applications include remote/persistent ocean sensing, marine search and rescue, maritime operations in congested port environments, and industrial offshore supply and support. USVs face unique challenges that are not experienced by robots operating indoors, such as: the need to adhere to marine navigation rules (COLREGs); local current, wave and wind conditions that can severely reduce the dynamic range of sensors and actuators; frequent communication interruptions; and risk and urgency due to rapidly changing situations during outdoor on-water operations. This research aims to develop decision making foundations for enabling teams of humans and USVs to perform complex collaborative tasks. Advances in this area could be extremely important from both a regulatory and practical standpoint for the future deployment of USV systems. Results from this research will enable leveraging the tremendous potential of USVs by reducing the cost of deployment and operational risks in civilian applications. The integration of the research with graduate and undergraduate courses will enhance the robotics and ocean engineering curricula and enrich learning experiences of the participating students. Outreach activities will educate and inform K-12 students about career opportunities in marine robotics. <br/><br/>The overall goal of the proposed effort is to make advances in risk-informed decision making so that teams of USVs and human supervisors can work cooperatively on a wide variety of missions. The proposed work will develop a comprehensive distributed decision making approach by leveraging the latest advances in task coordination and assignment, planning, reactive behaviors, and control to enable the deployment of human-guided USV teams in civilian applications. Progress in these constituent components will be pursued to ensure that they are consistent with each other and to explicitly account for risk during decision making. This research will develop methodologies to model team missions to ensure that all phases of decision making will have the required information for making informed decisions. Decision making methodologies will be developed for sparse advisory control of USV teams to mitigate risks and for coordinating and assigning tasks to different USVs in the team. Algorithms will also be developed for risk-aware deliberative trajectory planning and generating and executing reactive behaviors for mitigating risks. The methods developed will be validated through on-water field experiments."
"1426840","NRI: Collaborative Research: Robotics 2.0 for Disaster Response and Relief Operations","IIS","National Robotics Initiative","08/15/2014","08/18/2014","R. Vijay Kumar","PA","University of Pennsylvania","Standard Grant","Jie Yang","07/31/2017","$700,000.00","Kostas Daniilidis","Kumar@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","8013","8086","$0.00","The project develops and tests novel compressive sensing and sensor locating techniques that are adaptable to a myriad of different mobile robot designs while operable on today's wireless communication infrastructures. Unique in-situ laboratory and field experiments provide tangible results to scientists and other stakeholders that can be leveraged to advance these systems into future real-world hazard management scenarios. The research team develops new technological approaches that results in mobilizing more intelligent, automated ""eyes and ears on the ground."" Outreach efforts include: (i) integration of the activities with practitioners; (ii) Seminars/webcasts to audiences like environmental engineers and first responders; (iii) Annual technology day camps to attract middle-schoolers from under-represented groups to engineering; (iv) Demonstrations to local K-12 institutions; (v) Inclusion of the project themes to the regular curricula; and (vi) International collaborations.<br/><br/>This project introduces Robotics 2.0; a framework that targets autonomous robots that are co-workers and co-protectors, adapting to and working with humans. The research team develops a Cyber-Control Network (CCN) to allow multiple fixed and mobile robotic environmental sensing and measurements to adapt quickly to the changing environment by dynamically linking sub-networks of actuation, sensing, and control together. The design of such CCN ControlWare, and compressive sensing architectures, could be adapted to other large-scale problems beyond disaster response, mitigation, and management, such as power grid monitoring and reconfiguration, or regional urban traffic operations to respond to traffic congestion and incidents. The robotic sensing platforms do not require a-priori knowledge of the hazardous and dynamically changing environments they are monitoring. The Robotics 2.0 framework allows to swiftly respond, to prepare, and to manage various types of disasters."
"1528036","NRI: Collaborative Research: ASPIRE: Automation Supporting Prolonged Independent Residence for the Elderly","IIS","National Robotics Initiative","09/01/2015","08/18/2015","Naira Hovakimyan","IL","University of Illinois at Urbana-Champaign","Standard Grant","Alexander Leonessa","08/31/2018","$1,295,917.00","Alex Kirlik, Ranxiao Wang, Dusan Stipanovic, Amy LaViers","nhovakim@illinois.edu","SUITE A","CHAMPAIGN","IL","618207473","2173332187","CSE","8013","010E, 8086","$0.00","Because of the graying of the population, there is a growing need for new assistive technologies to aid the elderly in their daily living. Based on figures provided by the U.S. Census Bureau, and due largely to the aging of the ""baby boomer"" generation, the population of U.S. adults who are 65 and older is projected to be twice as large in 2030 as it was in 2000, increasing from 35 million to 71.5 million and representing nearly 20 percent of the total U.S. population. This trend is placing enormous burdens on health care costs and causing disruptive changes to how individuals and families manage key late-in-life decisions, including residence. A recent (2015) report by the U.S. Department of Housing and Urban Development concluded that most seniors would prefer to age in place, and a 2010 survey found that 88% of respondents over 65 preferred to remain in their homes as long as possible. It is important to note that these residential preferences must typically be viewed in light of alternatives that are likely to be much more expensive and/or more socially taxing, such as older adults living instead in hospitals, assistive living facilities or with family members. While estimates of the costs associated with these trends vary greatly, it is almost certain that extending the portions of older adults' life spans in which they can live safely and independently through technological means could have enormous positive societal impact. In order to assist in successful aging in place, assistive robots have been developed in the past few decades. However, very few of them have become commercially available, and their use in domesticated environments remains highly limited. The major cause of the problem is that assistive robots typically take the form of full-size humanoid devices or something equally as cumbersome, expensive, and limited in movement and function. We propose a novel assistive robotic system that provides: (i) flexibility, allowing the designed system to be personalized based on users' needs without demanding any home modification upon installation; (ii) safety, ensuring that the system development process accounts for perceived safety by the user, and that the underlying theoretical framework guarantees collision avoidance; (iii) usability, consisting of a minimal and intuitive user interface to provide acceptable controls; (iv) reduced costs, with respect to currently available solutions on the market. The idea behind this research project is the development of a general framework that enables a team of unmanned ground vehicles and small multirotor unmanned aerial vehicles to safely cooperate with the elderly in a home environment. Equipped with appropriate human-machine interfaces, the co-robots will be able to accomplish a number of tasks as demanded by the users. <br/><br/>The project addresses fundamental problems in the domain of multi-agent cooperative systems, comprised of humans and co-robots interacting in shared, highly constrained spaces. In order to assist humans, the co-robots have to be trusted by humans, implying that their behaviors be predictable and consistent with principles of human spatial perception, and their appearance must foster a high level of comfort and not create high cognitive demands on the user. Inspired by these challenges, this proposal focuses on the design and control of co-robots, which can adapt to unstructured and rapidly changing environments in a manner consistent with human perception and cognition, thus enhancing safety and robustness. The key focus areas include the design and acceptance of mobile ground and aerial robots that coexist in environments inhabited by humans and the development of a multi-objective control framework to allow intuitive user control over an ensemble of co-robots, which includes the design of both low-level controllers (LLC) and a supervisory, high-level controller (HLC). To demonstrate the benefits of the framework and to engage student groups from various, diverse populations, the following scenario will be considered as a test case: multirotor unmanned aerial vehicles and ground robots acting as domestic assistive devices for healthy older adults in a research laboratory. These co-robots will safely navigate the shared space and accomplish domestic tasks requested by humans while displaying behaviors and appearances that are perceived as safe and trusted. Humans will use an intuitively designed interface for both controlling and monitoring co-robots on a tablet or a smartphone device. A motion capture system and virtual reality Cube at the Beckman Institute will provide the context for data collection, iterative testing and validation."
"1316934","NRI: Small: Multirobot-Human Coordination for Visual Scene Understanding","IIS","COLLABORATIVE RESEARCH, IIS SPECIAL PROJECTS, National Robotics Initiative","09/01/2013","04/30/2015","Amit Roy Chowdhury","CA","University of California-Riverside","Standard Grant","Jeffrey Trinkle","08/31/2017","$821,934.00","Jay Farrell, Anastasios Mourikis","amitrc@ece.ucr.edu","Office of Research","RIVERSIDE","CA","925211000","9518275535","CSE","7298, 7484, 8013","5920, 7923, 8086","$0.00","The objective of this research is to enable the development of teams of robots, equipped with vision and other sensors, capable of working alongside humans in critical missions, such as search and rescue. Key requirements are situational awareness and coordinated action. The approach is to develop mathematical frameworks and algorithms to enable such a team of robots to coordinate their paths, share and analyze their sensor data, maintain communications, and interact effectively and safely with humans. The project brings together experts in computer vision, robotics, estimation theory and controls.<br/><br/>Intellectual Merit. Realizing the above goals will require advances in several inter-related domains. Specifically, the sensing, estimation, and trajectory control tasks must seamlessly integrate visual analysis with navigation and control strategies, as well as inputs from humans. Novel distributed estimation strategies must be developed to accommodate difficult and dynamic environments. Efficient human-robot coordination necessitates methodologies for joint exploration and mapping, identifying important visual information, and robots? operation at different levels of autonomy.<br/><br/>Broader Impact. The success of this project will be a major step towards the deployment of teams of robots to assist humans in dangerous and complex tasks like disaster response. Search-and-rescue experts will advise the team in developing a prototype system, and evaluating it in situations that mimic operational conditions. The developed software tools will be disseminated to other researchers so they can build on the results. Undergraduates from UCR's highly diverse student population will gain valuable experience working alongside graduate student researchers."
"1551391","EAGER: Toward Descriptive Mapping for Underwater Exploration","IIS","National Robotics Initiative","09/01/2015","08/18/2015","Brendan Englot","NJ","Stevens Institute of Technology","Standard Grant","Jeffrey Trinkle","08/31/2017","$94,995.00","","brendan.englot@stevens.edu","CASTLE POINT ON HUDSON","HOBOKEN","NJ","070305991","2012168762","CSE","8013","7916, 8086","$0.00","This project will provide a new solution for 3D mapping and exploration of underwater environments characterized by sparse and noisy data. It will enhance the capability of underwater robots to autonomously map, navigate, and inspect a previously unknown shallow-water environment. The development of this project will benefit from its long association with United States Coast Guard.<br/><br/>This project will advance the state of the art in three-dimensional occupancy mapping using Gaussian process regression, a supervised learning method with great promise for its application to robot mapping. This project will apply these methods to online 3D mapping with a field robot, mapping underwater structures with a scanning sonar."
"1317926","NRI: Small: Collaborative Research: Learning from Demonstration for Cloud Robotics","IIS","National Robotics Initiative","10/01/2013","02/09/2016","Sonia Chernova","GA","Georgia Tech Research Corporation","Standard Grant","Reid Simmons","09/30/2017","$426,060.00","","chernova@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","7923, 8086","$0.00","The proposed work seeks to leverage cloud computing to enable robots to efficiently learn from remote human domain experts - ""Cloud Learning from Demonstration."" Building on RobotsFor.Me, a remote robotics research lab, this research will unite Learning from Demonstration (LfD) and Cloud Robotics to enable anyone with Internet access to teach a robot household tasks. The value of this work stems from three aspects. First is the remote system that can learn task models from a series of remote demonstrations from a single user, focusing on learning high-level tasks as opposed to low-level motor skills. The second is the extension of learning from demonstration to multiple teachers. This represents an important relaxation of a limiting assumption to focus on evaluating teacher strengths and effectively handling distinct task solutions. Finally, transparency mechanisms to allow a remote user to develop a correct mental model about the robot?s learning process.<br/><br/>The long term goal of this research is to one day make personal robots accessible to everyday people. The interactive learning framework based on RobotsFor.Me provides unique opportunities for education and outreach. Thomaz and Chernova will outreach to K-12 teachers and students by creating an education portal surrounding RobotsFor.Me containing hands-on workshop curricula. This material will be integrated with the WPI Frontiers program for middle school students, and the GT ePDN professional education network for teachers. A key impact on students at GT and WPI will be direct involvement in this research agenda, and integration with AI, robotics and HRI courses. Chernova is the Diversity Coordinator in the Robotics Engineering Program, and faculty advisor for Women In Robotics Engineering and Women in Technology student groups which will enable braod exposure. Thomaz mentors the RoboWomen graduate women?s group. Software components will also be made available as open source and the PIs have a collaboration plan in place with researchers at Willow Garage, and through student internships will transfer technology to their labs."
"1427422","NRI: Biologically-inspired, hybrid quadruped robot control","IIS","National Robotics Initiative","08/15/2014","09/14/2015","Sanford Meek","UT","University of Utah","Standard Grant","Jeffrey Trinkle","07/31/2017","$499,760.00","V. John Mathews","meek@mech.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8013","8086, 9150","$0.00","This research project is about designing better legs for legged robots. When a robot needs to move across rough terrain, as would be the case when searching for victims in a disaster area, legged robots have advantages over wheeled or tracked robots. They can traverse such terrain more easily, for example, stepping over fallen beams and bricks. However, legged robots are inherently more complex. They have many more joints and are more difficult to control. The goal of the project is to use ideas from nature to overcome some of these difficulties. The shape and compliance of four-legged animals adds to their stability passively when they trot or gallop. These ideas will be incorporated into a four-legged robot, making its locomotion abilities more robust. A major issue to duplicate from nature is the animal's ability to adaptively change the compliance of their joints and legs. This is a primary objective of this project. In addition to the research objectives, this project also includes educational components involving graduate and undergraduate students as well as outreach to secondary schools.<br/><br/>The approach taken in this project will simplify the control of quadruped robots by the use of directionally compliant legs to passively stabilize the robot under dynamic gaits such as trotting. Active control of the stiffness and joint equilibrium positions will allow for adaptive parameter changes to changing terrain, gaits, and robot variation (carry a load for example). The system will employ a hybrid controller using passive, under-actuated legs during gaits, but actively changing the passive characteristics. Stability of the robot is measured by the pitch and roll of the robot body. The adaptive controller will minimize errors in the desired pitch and roll by changing the joint stiffness, equilibrium angles and positions of each joint."
"1317445","NRI: Small: Collaborative Research: Adaptive Motion Planning and Decision-Making for Human-Robot Collaboration in Manufacturing","IIS","National Robotics Initiative","09/15/2013","09/06/2013","Julie Shah","MA","Massachusetts Institute of Technology","Standard Grant","Reid Simmons","08/31/2017","$319,728.00","","arnoldj@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","7923, 8086","$0.00","This project addresses manufacturing tasks that cannot be fully automated because of either the limitations of current algorithms or prohibitive cost and set-up time. Such tasks generally require workers to collaborate in close proximity and adapt to each other's decisions and motions. This project explores accomplishing these tasks through human-robot collaboration. Recent hardware developments in robotics have made human-robot collaboration physically possible, but robots still require new algorithms to ensure safety, efficiency, and fluency when working with people. Creating such algorithms is difficult because there can be high uncertainty in what a person is going to do and how they are going to do it. This project explores the integration of reasoning about how a person moves and how he or she makes decisions into a robot motion planning and decision-making framework. The research centers on the development of new algorithmic frameworks for modeling, simulating, and planning for human-robot collaboration, which requires advances in robot training, task modeling, human motion understanding, high-dimensional motion planning with uncertainty, and metrics to assess human-robot joint action. The results of this project have the potential to signi&#64257;cantly improve American competitiveness in manufacturing; especially for small-batch manufacturing and burst production, where the cost and set-up time of fully-autonomous solutions is prohibitive. The work will be disseminated in research papers and integrated into curricula. The project is guided by an advisory board from the manufacturing industry, which provides another avenue for dissemination."
"1637647","NRI: Rethinking Multi-Legged Robots: Passive Terrain Adaptability through Underactuated Mechanisms and Exactly-Constrained Kinematics","IIS","National Robotics Initiative","10/01/2016","08/04/2016","Aaron Dollar","CT","Yale University","Standard Grant","Jeffrey Trinkle","09/30/2019","$718,214.00","","aaron.dollar@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","8013","8086","$0.00","Legged vehicles have long intrigued us with the promise of moving with animal-like agility, gracefully and swiftly going where wheeled vehicles cannot. Despite numerous successes, however, current legged robots are mostly unable to traverse rough terrain, and are mechanically complex, requiring elaborate hardware and control, and having short operation-times. Work on this project will investigate the development of multi-legged robots that utilize adaptive motors and transmissions, to reduce the complexity and cost, while also allowing the legs to adapt to rough terrain. These legged robot designs will be made freely available through OpenRobotHardware.org, and will be able to be easily and inexpensively fabricated with rapid prototyping techniques.<br/><br/>The project takes a new approach to legged robots by proposing new architectures that reduce over-constraint in the closed kinematic chains with the ground by applying under-actuated mechanisms - equipping robots with legs that passively adapt to large variations in terrain roughness with minimal disturbance forces to the body. As a result of this kind of passive adaptability, the robot is able to find a stable footing on rough terrain without any sensing or planning: the legs simply fall into place. Furthermore, adaptability in the mechanism also enables the forces and torques applied by the legs to the body to be passively regulated so as to minimize disturbances that would tend to destabilize the vehicle or cause it to lose its heading. This ability will drastically simplify the control of legged robot systems, greatly increase power efficiency and their robustness to terrain variations, and decrease production and maintenance costs."
"1426443","NRI: Collaborative Research: Exploiting Granular Mechanics to Enable Robotic Locomotion","IIS","National Robotics Initiative","08/15/2014","09/24/2015","Daniel Goldman","GA","Georgia Tech Research Corporation","Standard Grant","Jeffrey Trinkle","07/31/2017","$360,000.00","David Hu","daniel.goldman@physics.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","8086","$0.00","We need robots to extend our reach into dirty and dangerous environments. To do so, mobile robots must be able to locomote in messy unstructured terrains. Conventional mobile robots have not begun to display the multi-functionality of organisms that inhabit natural terrains. This is because mobile robots have been created in, and their models mainly validated on, clean hard laboratory floors, whereas biological organisms have evolved to contend with heterogeneous, dirty and unpredictable environments. One important example of such real world complex terrain, often overlooked by our community despite its ubiquity, involves loose granular materials commonly found in deserts, disaster sites, containers, and caves. Therefore creation of the next level of mobility to traverse dirty environments requires simultaneous advances in both robotics and physics, particularly regarding the interactions associated with desired behaviors. <br/><br/>The proposed work is built on a foundation of geometric mechanics, granular physics of intrusion and biological inspiration from desert-dwelling snakes. We use geometric mechanics, a field that applies principles from differential geometry to problems in classical mechanics, to design gaits for biologically inspired robots. We bring the benefits of the geometric tools to bear on granular environments: in even these mathematically ""messy"" systems, we can begin to efficiently analyze gaits. The key concept in this effort is that systems with complicated, nonlinear low-level physics often exhibit much ""cleaner"" high-level motion, often approximated by a kinematic relationship. Development of such high-level motion controllers will be aided by our ability to discover basic biological principles of locomotion in granular media. We will therefore develop computationally efficient analysis tools for granular materials and will develop techniques to study the locomotion of systems on the surface of granular media."
"1463960","NRI-Small: Context-Driven Haptic Inquiry of Objects Based on Task Requirements for Artificial Grasp and Manipulation","CBET","National Robotics Initiative","07/01/2014","10/14/2014","Veronica Santos","CA","University of California-Los Angeles","Standard Grant","Alexander Leonessa","07/31/2017","$454,632.00","","vjsantos@ucla.edu","11000 Kinross Avenue, Suite 211","LOS ANGELES","CA","900952000","3107940102","ENG","8013","7923, 8086","$0.00","PI: Santos, Veronica<br/>Proposal Number: 1208519<br/><br/>Intellectual Merit: Human-like dexterous manipulation is featured prominently as a grand challenge in the 2009 Roadmap for U.S. Robotics' report. Human dexterity relies heavily on tactile sensation and is influenced by proprioceptive and visual feedback. The proposed work aims to advance artificial manipulators by integrating a new class of multimodal tactile sensors with anthropomorphic artificial hands and developing generalizable routines for context-driven haptic inquiry of objects based on task requirements for artificial grasp and manipulation. A primary goal is the development of capabilities for a robot hand to efficiently learn about objects in its unstructured environment through touch, specifically for cases where computer vision would fail to provide critical information about the physical hand-object interactions. While computer vision provides preliminary information about an object and its environment, vision alone cannot provide all essential information necessary for successful physical hand-object interactions. This is especially true when digits are occluded by the grasped object, and when the hand-object interaction is completely out of view. Inspiration for the haptic inquiry framework will be drawn from a suite of human haptic exploration procedures. In contrast to haptic exploration, haptic inquiry will require that the order and time spent on each exploratory procedure depend on task goals. The order and type of questions to be asked haptically will be context-dependent and designed to yield high-level, task-directed information at a low cost of inquiry. The weight given to each mode of tactile sensing (force, vibration, temperature) will also be tuned according to the context of the task.<br/>This proposal aims to strengthen the robustness of co-robot systems by developing a framework for context-driven, task-directed haptic inquiry that integrates multi-digit tactile and proprioception data in a task-appropriate manner. The framework will be developed and deployed on an anthropomorphic robot hand outfitted with a new class of commercially-available multimodal tactile sensors. The work is transformative because it will enable co-robot systems to remain functional even in the absence of visual feedback, which is typically the primary form of feedback for robotic systems. The long-term research objective of this proposal is to reduce the cognitive burden on the user of an artificial manipulator. <br/><br/>Broader Impacts: The proposed translational research could enhance the functional capabilities of co-robot systems in which humans use artificial manipulators to work in unstructured, unsafe, or limited access environments (prosthetic, rehabilitative, assistive, space, underwater, military, rescue, surgery). The proposed work could benefit the human user of a co-robot system by empowering the robot with the ability to control low-level perception-action loops autonomously without burdening the human. The ROS operating system may be used to simulate and control an anthropomorphic robot hand outfitted with commercially-available tactile sensors using commercially-available actuators. Custom source code (C, MATLAB, ROS) and an open source haptic library for a commercially-available tactile sensor (suitable for data mining) will be made publicly available for the benefit and advancement of the robotics community."
"1427419","NRI: Electrosense imaging for underwater telepresence and manipulation","IIS","National Robotics Initiative","09/01/2014","08/01/2016","Michael Peshkin","IL","Northwestern University","Continuing grant","Jeffrey Trinkle","08/31/2017","$1,816,000.00","Alexander Makhlin, James Solberg, Malcolm MacIver, Konrad Kording, Joshua Smith","peshkin@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","8013","8086, 9251","$0.00","Human telepresence underwater is essential for tasks such as security sweeps in harbors and oil field servicing. Co-robotic solutions are needed, because the risks are great for human divers, while autonomous robots do not deal well with contingencies. A major problem is that vision works poorly in murky environments, such as when mud is kicked up from the bottom. In this National Robotics Initiative (NRI) project the researchers are investigating and developing a replacement for vision -- electrosense -- used by Amazonian fish that navigate and hunt in murky water. These ""weakly electric fish"" generate an AC electric field that is perturbed by objects nearby. Electroreceptors covering the body of the fish detect the perturbations, which the fish decodes into information about its surroundings. The researchers are developing methods of preprocessing electric images for human understanding, and new computed methods for machine interpretation. <br/><br/>The research creates electrosense hardware and practical testbeds, for navigation and for manipulation underwater. It investigates methods and software to facilitate human interpretation of electric images, as well as machine interpretation. In hardware, the researchers are creating a kilopixel-scale electrosense array as an input sensor for human interpretation of electric images, and development of preprocessing algorithms to make human interpretation workable. The researchers are also using sparser and non-coplanar groups of electroreceptors on a manipulator, for control of pre-grasp and manipulation tasks. For human interpretation, electric image preprocessing includes contour painting and spatial high-pass filtering, as well as temporal filtering. For machine interpretation, methods include specific recognition strategies for simple geometric primitives, and sparse beamforming techniques for more complex environments."
"1637853","NRI: Design and Fabrication of Robot Hands for Dexterous Tasks","IIS","National Robotics Initiative","08/01/2016","08/01/2016","Nancy Pollard","PA","Carnegie-Mellon University","Standard Grant","Hector Munoz-Avila","07/31/2019","$769,906.00","Stelian Coros","nsp@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","8086","$0.00","Dexterous manipulation is a Grand Challenge in robotics today. Dexterity is required for robots working in the home, in space, for medical applications, disaster scenarios, and flexible manufacturing. Robots in these environments must be able to maneuver, lift, and handle objects, use them as tools, and competently transfer objects from one secure hold to another in a wide variety of uncertain situations. Such operations are very difficult for robots and present a barrier to their wider adoption. Recent advances in rapid prototyping and digital manufacturing have made it possible to design, manufacture, and test custom robot manipulators at a very rapid pace. One aim of this project is to explore and understand how these new technologies may be used to design a radically different type of robot manipulator that is customized for dexterous maneuvers. A second aim of the project is to increase our fundamental understanding of dexterity itself. The end goal is to enable robots to live up to their potential. In so doing, robots will begin to play an increasingly important role in our daily lives, and will inspire young students to prepare for, and pursue STEM careers.<br/><br/>The key research goal of this project is to formalize novel mathematical models and computational approaches to co-design mechanical structures and control policies for dexterous robotic manipulation tasks. Through this approach, control policies can be made significantly simpler, and mechanical features such as joint stops and compliance can passively improve the robustness of the manipulation tasks while reducing sensing and actuation requirements. Grasp nets, which capture specific families of manipulation capabilities observed in human performances, focus the efforts of this project and guide the design processes that are developed."
"1637737","NRI: Workers, Firms, and Industries in Robotic Regions","IIS","National Robotics Initiative","09/01/2016","08/01/2016","Nancey Green Leigh","GA","Georgia Tech Research Corporation","Standard Grant","Frederick M Kronz","08/31/2018","$784,887.00","Henrik Christensen","ngleigh@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","8086","$0.00","This research project focuses on the U.S. robotics industry and the economic impacts of robotics technology. The researchers will generate new data and then analyze it. The goal of the project is to assess the impacts of robotics on work and the economy following a significant leap in robotic capabilities that has enabled robot-human collaboration. It will build on previous work of the research team, which characterized the U.S. robotics industry and identified regions that host relatively intense research, development, and commerce relating to robotics. The researchers will use a mixed methods approach that situates robot use and diffusion in a regional context. They will survey manufacturers about their robot use and associated employment patterns, and they survey of systems integrators; their prior research has determined that systems integrators are as a group crucial to the U.S. robotics industry. They will also analyze novel labor market data to assess robot-related employment supply and demand, and they will conduct a cross-case comparison of two robotic regions that have significant robotics-related employment. This research will enable current and future policy makers, workers, and corporate leaders to make more informed decisions in anticipation of-and in response to-the diffusion of robots throughout the economy. It will shed light on evolving employment structures, the changing nature of work, firm strategies, and regional economic evolution as robots diffuse. Professional and trade associations, regional planning, workforce and economic development agencies, and popular media will be targeted for dissemination efforts. <br/><br/>The proposed research will advance understanding of the relationship between 21st century technology and work, meeting a need to assess robots as more than just advanced manufacturing technology, given the rise of collaborative robots in manufacturing and expected diffusion into the service sector. It employs a comprehensive approach to assessing technological change by combining theoretical perspectives and empirical analysis of regional economic development with engineering approaches to robot development and adoption in production. The research team is multidisciplinary, including professors of economic development planning and robotics engineering. The proposed research extends the team's preliminary research, which indicates that distinct robotic regions have emerged within the US. It overcomes the significant deficit of actual data on robot use and diffusion at the national and subnational levels through the use of industry survey and case study methodologies, and through an analysis of real time labor market data made possible through advances in big data collection. By identifying regional factors that lead to innovations in robotics technologies and influence local firm decisions to use robots, this research will equip local policy makers with knowledge to foster competitive and resilient places in the context of rapid technological change."
"0964665","RI: Medium: Planning and Control for Dynamic Robotic Manipulation","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","04/15/2010","08/23/2013","Kevin Lynch","IL","Northwestern University","Continuing grant","Jeffrey Trinkle","03/31/2016","$854,050.00","","kmlynch@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","7495, 8013","7924, 8086, 9251, 7495","$0.00","Manipulation allows a robot to alter its environment by performing mechanical work on it. Combined with locomotion, autonomous manipulation will be a key enabling capability in future robot systems for exploration, home care, military, industrial, and space applications. While most current robotic manipulation consists of simple grasp-and-carry operations, the spectrum of manipulation methods is much broader, including throwing, pushing, rolling, sliding, tapping, etc. Humans and animals make use of these methods on a daily basis, but robots do not. What is missing is a theoretical framework for combining and sequencing such manipulation methods for automatic planning and control of robotic manipulation.<br/><br/>This project is working toward autonomous manipulation by bridging the gap from individual robotic manipulation primitives to full manipulation sequence planning. The project is developing theoretical tools, a mechanics-based manipulation planning framework, feedback control algorithms, and an experimental manipulation testbed incorporating high-speed vision. The tools developed for autonomous planning and execution of manipulation sequences will allow future robot users to program robots at a much higher level of abstraction than currently possible. For example, instead of programming low-level control of motors, the user can simply say ""place object A on object B."" This will help grow the field of personal robotics, by allowing non-experts to interact with robots.<br/><br/>International partners in this work include leading laboratories in Italy and Japan. These collaborations provide unique access for graduate and undergraduate researchers to the world's leading experts in robotic manipulation."
"1637736","NRI: Robots that Learn to Communicate through Natural Human Dialog","IIS","National Robotics Initiative","09/01/2016","08/01/2016","Raymond Mooney","TX","University of Texas at Austin","Standard Grant","Tatiana D. Korelsky","08/31/2019","$936,906.00","Peter Stone","mooney@cs.utexas.edu","101 E. 27th Street, Suite 5.300","Austin","TX","787121532","5124716424","CSE","8013","8086","$0.00","Robots are increasingly capable and are on the threshold of becoming a ubiquitous technology. For robots to be truly useful, people must be able to effectively communicate their needs in everyday human language. Although there is a growing body of research on natural-language processing for human-robot interaction, it typically requires some form of explicit supervision provided by an engineering expert and involves unnatural, laborious training to obtain robustness and coverage. This project involves the development of human-robot dialog systems that learn to communicate with users through natural dialog, learning from repeated normal user interactions to become more robust and capable. The project supports the education of students in the areas of natural-language processing, human-robot interaction, and machine learning, where there is significant demand for educated personnel. It is integrated with the university's Freshman Research Initiative, which gets undergraduate students involved in research in their first year.<br/><br/>In order to develop human-robot dialog systems that learn to improve their communication skills through normal user interactions, the project integrates and adapts learning techniques from three currently disparate technical areas: semantic parsing, spoken dialog management, and perceptual language grounding. The project adapts and integrates techniques for semantic-parser learning using combinatory categorial grammar (CCG), dialog management using Partially Observable Markov Decision Processes (POMDPs), and multi-modal language grounding using both visual and haptic sensors, in order to develop a dialog system for communicating with robots that comprise the Building Wide Intelligence (BWI) system being developed at the University of Texas at Austin. The research integrates the PI's expertise in semantic parsing and language grounding with the co-PI's expertise in robotics and reinforcement learning, forming a unique interdisciplinary team for developing novel and effective systems for human-robot interaction. The project includes rigorous evaluations using controlled experiments on a range of tasks using both on-line simulations with crowdsourced users, and natural user interaction with a mobile robot platform consisting of a wheeled Segway base and a Kinova robot arm being developed for the BWI system."
"1208639","NRI-Small: Robot Assistants for Promoting Crawling and Walking in Children at Risk of Cerebral Palsy","IIS","National Robotics Initiative, EXP PROG TO STIM COMP RES","10/01/2012","05/10/2016","Andrew Fagg","OK","University of Oklahoma Norman Campus","Standard Grant","Ephraim P. Glinert","09/30/2017","$1,175,000.00","Hlapang Kolobe, Lei Ding, David Miller","fagg@cs.ou.edu","201 Stephenson Parkway","NORMAN","OK","730199705","4053254757","CSE","8013, 9150","7923, 8086, 9150, 9251","$0.00","Effective robotic assistance of infants with or at risk of developing Cerebral Palsy (CP) has the potential to reduce the significant functional limitations as well as the potential deficits in cognitive development. This project focuses on the development and testing of a sequence of robotic assistants that promote early crawling, creeping, and walking, along with a model of infant-robot interaction that encourages the continued practice of movement patterns that will ultimately lead to unassisted locomotion. Typically developing infants initially learn to crawl through the generation of spontaneous limb and trunk movements. Early in the process, these spontaneous movements transport the infant across the floor. The rewarding locomotory experience drives the infant to refine the movements to intentional and exploratory skills. Ultimately, the infant intentionally engages these skills to solve larger problems, such as obtaining an interesting toy or exploring the environment. Infants with conditions such as CP lack the muscle strength, postural control, and motor coordination necessary for these early exploratory limb and trunk movements to result in locomotion. Without this positive feedback, the development of the neural pathways for productive limb use is diminished, which results in delayed or lack of development of crawling and walking. These limitations in mobility negatively affect other domains of development such as perception and cognition, with effects being visible even into adulthood.<br/><br/>The robotic assistants to be developed in this project will aid the infant in developing locomotory skills by selectively supporting a portion of his/her weight and providing artificial, rewarding locomotory experiences. The PI's approach to infant-robot interaction is to first instrument the infant with a set of sensors, allowing for reconstruction of the trunk and limb positions in real time. A semi-supervised clustering process will then identify a menu of canonical spatio-temporal limb and trunk movement patterns given observations of behavior that is exhibited by children who are either typically developing or at risk of developing CP. The robot will respond to the recognition of a canonical movement by assisting in the corresponding postural support and transport of the child. The PI's hypothesis is that this positive feedback will encourage the continued practice of the canonical movements, as well as their use in solving larger problems. The infant-robot interaction model will selectively reward specific canonical movements as different levels of capabilities are exhibited. As the child becomes proficient at using a simple movement to trigger robotic assistance, the robot will reduce (and ultimately eliminate) its response to that particular canonical movement. Other canonical movements that encode related, but more complex and/or coordinated limb movements, will continue to be available. As the limb movements are mastered the vertical support will be reduced to encourage the infant to bear more of his/her own weight. The hypothesis is that this early intervention approach will help to guide the child along a progressive developmental trajectory that will end with locomotory skills and muscle strength that require little or no assistance. EEG-based neuroimaging will be used to monitor the progression of the infant's development. The hypothesis is that the degree of proficiency of certain skills will be identifiable using the EEG index related to motor output. This information will be used to guide the semi-supervised clustering process, as well as the decision process for selectively rewarding certain canonical movements.<br/><br/>Broader Impacts: Equipping children with CP at an early age with locomotory skills will not only bring them more in line with typically developing children, but will also reduce their reliance on long-term care while increasing their success in self-help, in education, and in the workplace. The techniques will be applicable to a range of other childhood disorders (including Down Syndrome), to retraining patients following stroke, and to the creation of tunable gestural interfaces for intelligent prostheses."
"1208153","NRI-Small: Collaborative Research: Assistive Robotics for Grasping and Manipulation using Novel Brain Computer Interfaces","IIS","Gen & Age Rel Disabilities Eng, National Robotics Initiative","10/01/2012","06/20/2016","Peter Allen","NY","Columbia University","Standard Grant","Alexander Leonessa","09/30/2017","$800,498.00","Joel Stein","allen@cs.columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","5342, 8013","7923, 8086, 9251","$0.00","This is a collaborative proposal (with UC Davis) which is aimed at making concrete some of the major goals of Assistive Robotics. A team of experts has been brought together from the fields of signal processing and control, robotic grasping, and rehabilitative medicine to create a field-deployable assistive robotic system that will allow severely disabled patients to control a robot arm/hand system to perform complex grasping and manipulation tasks using novel Brain Muscle Computer Interfaces (BMCI). Further, the intent of this effort is not just technology-driven, but is also driven by clear and necessary clinical needs, and will be evaluated on how well it meets these clinical requirements. Validation will be performed at the Department of Regenerative and Rehabilitation Medicine at Columbia University on a diverse set of disabled users who will provide important feedback on the technology being developed, and this feedback will be used to iterate on the system design and implementation.<br/><br/>Intellectual Merit: The intellectual merit of this proposal includes:<br/>o Novel research in Human Machine Interfaces that has the potential to be transformative in eliciting rich, multi-degree-of-freedom signal content from simple and non-invasive surface electromyographic (sEMG) sensors.<br/>o Development of smart adaptive software that employs machine learning algorithms that can continually monitor user performance, and then automatically calibrate and tune system parameters based on system performance.<br/>o Data driven methods for real-time grasp planning algorithms that can be used with both known and unknown objects.<br/>o Methods for finding pose-robust grasps that are tolerant of errors in sensing.<br/>o Evaluation of an underactuated hand as a grasping device for certain application tasks.<br/>o Integration of 3D vision with real-time grasp planning.<br/>o Scientific evaluation at the clinical level of the impact of these new technologies on the disabled population.<br/><br/>Broader Impacts: The broader impacts of this proposal include:<br/>o Development of a complete system to aid the severely disabled population with tetraplegia.<br/>o Extensions of this technlogy to others lacking motor control function including multiple sclerosis, stroke, amyotrophic lateral sclerosis (ALS or Lou Gehrig disease), cerebral palsy, and muscular dystrophy.<br/>o New technology that can extend the reach and impact of the field of Assistive Robotics.<br/>o Major extensions to the open-source GraspIt! software system that will allow many other researchers to leverage the results of this project.<br/>o Educational thrusts that will bring together engineering students, clinicians and the disabled population to extend the reach and scope of Assistive Robotics.<br/>o New directions in Human Machine Interfaces that can extend beyond the disabled population and into a variety of other applications."
"1427014","NRI: Collaborative Research: Robotics 2.0 for Disaster Response and Relief Operations","IIS","National Robotics Initiative","08/15/2014","05/10/2016","Nikolaos Papanikolopoulos","MN","University of Minnesota-Twin Cities","Standard Grant","Jie Yang","07/31/2017","$1,008,000.00","Miki Hondzo, Ibrahim Isler, Jiarong Hong","npapas@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","8013","8086, 9251","$0.00","The project develops and tests novel compressive sensing and sensor locating techniques that are adaptable to a myriad of different mobile robot designs while operable on today's wireless communication infrastructures. Unique in-situ laboratory and field experiments provide tangible results to scientists and other stakeholders that can be leveraged to advance these systems into future real-world hazard management scenarios. The research team develops new technological approaches that results in mobilizing more intelligent, automated ""eyes and ears on the ground."" Outreach efforts include: (i) integration of the activities with practitioners; (ii) Seminars/webcasts to audiences like environmental engineers and first responders; (iii) Annual technology day camps to attract middle-schoolers from under-represented groups to engineering; (iv) Demonstrations to local K-12 institutions; (v) Inclusion of the project themes to the regular curricula; and (vi) International collaborations.<br/><br/>This project introduces Robotics 2.0; a framework that targets autonomous robots that are co-workers and co-protectors, adapting to and working with humans. The research team develops a Cyber-Control Network (CCN) to allow multiple fixed and mobile robotic environmental sensing and measurements to adapt quickly to the changing environment by dynamically linking sub-networks of actuation, sensing, and control together. The design of such CCN ControlWare, and compressive sensing architectures, could be adapted to other large-scale problems beyond disaster response, mitigation, and management, such as power grid monitoring and reconfiguration, or regional urban traffic operations to respond to traffic congestion and incidents. The robotic sensing platforms do not require a-priori knowledge of the hazardous and dynamically changing environments they are monitoring. The Robotics 2.0 framework allows to swiftly respond, to prepare, and to manage various types of disasters."
"1338042","MRI: Development of an Instrument that Monitors Behaviors Associated with Obsessive-Compulsive Behaviors and Schizophrenia","CNS","SPECIAL PROJECTS - CISE, INDUSTRY/UNIV COOP RES CENTERS, National Robotics Initiative","10/01/2013","07/26/2016","Nikolaos Papanikolopoulos","MN","University of Minnesota-Twin Cities","Standard Grant","Rita V. Rodriguez","09/30/2017","$598,000.00","Kelvin Lim, Gail Bernstein, Tasoulla Hadjiyanni, Arindam Banerjee","npapas@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","1714, 5761, 8013","1189, 9178, 9251, 1714","$0.00","Proposal #: 13-38042<br/>PI(s): Papanikolopoulos, Nikolaos<br/> Banerjee, Arindam; Bernstein, Gail; Hadjiyanni, Tassoulla; Lim, Kelvin, K.<br/>Institution: University of Minnesota-Twin Cities<br/>Title: MRI/Dev.: Instrument that Monitors Behaviors Associated with OCD and Schizophrenia <br/>Project Proposed:<br/>This project, developing a new instrument to facilitate data collection associated with clinical assessment of complex mental health disorders such as obsessive-compulsive behaviors (known as OCD), aims to enable long-term research advances in computer vision, activity recognition and tridimensional reconstruction algorithms to automate the identification of behaviors typical of OCD subjects. The immersion of selected subjects in a virtual reality room (CAVE) is used to trigger specific behaviors to be captured and analyzed. The sophisticated sensor system under development will serve to collect these data and provide intelligent data processing capabilities that would enable future exploration and testing of new diagnostic and therapeutic protocols, leading to the establishment of the basis for, and utility in, seeking early at-risk markers in children and adolescents. This instrument initiative is based on the premise that expertise can accurately identify useful diagnostic markers and on the belief that technologies can now be developed to collect massive behavioral data in ways not previously done and discover behavioral patterns.<br/>The instrument is expected to<br/>- Provide extensive data collection associated with subjects diagnosed with the respective disorders (data useful not only to clinicians but also to computer vision and machine learning researchers among others),<br/>- Capture interactions, behaviors, and physiological reactions to real and/or synthetic multimodal stimuli (optical, acoustical, etc.),<br/>- Allow computer and information scientists to develop computational tools and algorithms to generate quantitative, adequate, and cost-effective norms for screening a broad population, <br/>- Enhance Cognitive Behavior Therapy (CBT) procedures and diagnostic protocols by integration of technologies that can excite or inhibit triggers for schizophrenic or OCD episodes,<br/>- Assess particular Augmented and Virtual Environments (AE/VEs) and social media devices (smart phones and tablets) and their impacts on the cognitive presence of normal versus afflicted subjects, and<br/>- Evaluate whether an enhanced cognitive presence via an AE/VE can increase or suppress (habituate) the intensity of behavioral symptoms detectable by sensors. <br/>Broader Impacts: Among these we have:<br/>- Creation of large and complex datasets that will enable computer scientists to apply the newest computational tools on them,<br/>Development of a potentially transformative technology-driven instrument for detecting early risk markers of OCD,<br/>- Exploration of a platform well-suited to new directions for a better characterization of mental disorders,<br/>- Systematic database development of quantified, multimodal data and a sounder and more precise basis for earlier detection,<br/>- Reduction of overall costs and a parallelizing reduction in the long-term costs due to previously delayed or incorrect diagnoses,<br/>- Reduction of anxiety, disruption, stress, and sometimes real tragedy on patients and their families, <br/>- Earlier detection and reduced need for drug-based, later stage interventions enabled by the ease of testing, and associated societal benefits, and<br/>- Student education and training in the use of the instrument."
"1637927","NRI: A Cognitive Navigation Assistant for the Blind","IIS","National Robotics Initiative","09/01/2016","07/27/2016","Kris Kitani","PA","Carnegie-Mellon University","Standard Grant","Wendy Nilsen","08/31/2019","$1,000,000.00","Manuela Veloso","kkitani@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","8086","$0.00","The focus of this project is on implementation of a navigation assistant that uses a collection of sensing modalities and algorithms to guide a blind person through the knowledge landscape (e.g., social context, visual landmarks, scene functionality) of an unfamiliar environment. The approach is based on a portfolio of complex processes that provide within a single framework a coherent account of the state of the world, with the help of novel techniques which meld information at various levels of abstraction. In the near term, project outcomes will directly improve the quality of life for those with visual impairments through public release of a smartphone app. In the longer term, the societal impact of this research will extend beyond improving sensory capabilities for the blind in that it describes an approach towards human augmentation through the use of machine intelligence. The work will directly shed light on the variety of environmental knowledge which can be automatically acquired using machine perception, and how that information can be conveyed through a physical co-robot interface. From an educational perspective, this work will develop important models for integrating knowledge obtained by intelligent machines into one source, and will also develop new theories regarding the translation of that rich knowledge in a manner which can be easily understood by the user.<br/><br/>Leveraging prior work, sensing modalities such as Bluetooth low energy beacons, depth sensors, color cameras and wearable inertial motion units will be used to enable continuous localization within a novel environment. An additional layer of higher-order algorithms will further build upon physical measurements of location to develop computational contextual awareness, enabling the navigation assistant to understand the knowledge landscape by identifying meaningful visual landmarks, modes of interaction (functionality) within the environment and social context. This knowledge structure will then be conveyed to the blind user to enable contextual hyper-awareness, that is to say a contextual understanding of the environment which goes beyond normative sensing capabilities, in order to augment the user's ability to navigate the knowledge landscape of the environment. The navigation assistant will be instantiated as two concrete manifestations: a compact wearable interface, and a physical robotic interface. The wearable interface will be a smartphone-based system that gives audio-based navigation feedback to facilitate the creation of a cognitive map. The robotic interface will be a wheeled hardware platform that guides the user through haptic feedback to further reduce the cognitive load of interpreting and following audio feedback. Both platforms will be refined and evaluated in real-world scenarios based on principles derived from rigorous user studies. Project outcomes will include a navigation assistant that can help a blind person walk a path through novel indoor or outdoor suburban environments to a desired destination. The two physical interfaces will also be used to develop working theories and models for co-robot scenarios that must take into account situational context and the preferential dynamics of the user."
"1637753","NRI: Collaborative Research: Accelerating Robotic Manipulation with Data-Enhanced Contact Mechanics","IIS","National Robotics Initiative","09/01/2016","07/27/2016","Alberto Rodriguez Garcia","MA","Massachusetts Institute of Technology","Standard Grant","Jeffrey Trinkle","08/31/2019","$430,000.00","","albertor@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","8086","$0.00","Robotic manipulation depends upon mechanical contact between robot and object. A better understanding of mechanical contact enables a wider range of more flexible manipulation techniques, which in turn enables the applications of greatest societal benefit such as eldercare, disaster response, or surgery. This project is developing a broader and more accurate understanding of frictional contact, using a fusion of physics and data. The project combines recent advances in a physics-based understanding of frictional contact with new machine learning techniques applied to a large corpus of experimental data. One operation of great interest is manipulation of an object held in the robot gripper, even when the gripper is very simple. Other operations of interest are handling objects in clutter, and manipulation of flexible objects, such as clothing.<br/><br/>The project is attacking several central challenges: modeling frictional contact, modeling deformation, measuring small motions and interaction forces, gathering large amounts of data, and developing techniques for learning in a closed-loop system. Parametric and semi-parametric models enable the project to apply engineering models enhanced with observation data, for both planning and control. New machine learning techniques such as predictive state representations (PSRs) enable identification and modeling of previously hidden state, as well as learning in closed-loop systems. New infrastructure enables gathering of relevant, precise data, on a large scale. The project is developing and employing a Robotic Manipulation Arena, with a unique combination of manipulation resources and instrumentation to provide high volumes of high quality experimental data. The primary outcomes are robust and practical contact models, so that robots can work more dexterously and opportunistically."
"1637758","NRI: Collaborative Research: Accelerating Robotic Manipulation with Data-Enhanced Contact Mechanics","IIS","National Robotics Initiative","09/01/2016","07/27/2016","Byron Boots","GA","Georgia Tech Research Corporation","Standard Grant","Jeffrey Trinkle","08/31/2019","$452,219.00","","bboots@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","8086","$0.00","Robotic manipulation depends upon mechanical contact between robot and object. A better understanding of mechanical contact enables a wider range of more flexible manipulation techniques, which in turn enables the applications of greatest societal benefit such as eldercare, disaster response, or surgery. This project is developing a broader and more accurate understanding of frictional contact, using a fusion of physics and data. The project combines recent advances in a physics-based understanding of frictional contact with new machine learning techniques applied to a large corpus of experimental data. One operation of great interest is manipulation of an object held in the robot gripper, even when the gripper is very simple. Other operations of interest are handling objects in clutter, and manipulation of flexible objects, such as clothing.<br/><br/>The project is attacking several central challenges: modeling frictional contact, modeling deformation, measuring small motions and interaction forces, gathering large amounts of data, and developing techniques for learning in a closed-loop system. Parametric and semi-parametric models enable the project to apply engineering models enhanced with observation data, for both planning and control. New machine learning techniques such as predictive state representations (PSRs) enable identification and modeling of previously hidden state, as well as learning in closed-loop systems. New infrastructure enables gathering of relevant, precise data, on a large scale. The project is developing and employing a Robotic Manipulation Arena, with a unique combination of manipulation resources and instrumentation to provide high volumes of high quality experimental data. The primary outcomes are robust and practical contact models, so that robots can work more dexterously and opportunistically."
"1637562","NRI: Collaborative Research: Scalable Robot Autonomy through Remote Operator Assistance and Lifelong Learning","IIS","National Robotics Initiative","09/01/2016","07/27/2016","Sonia Chernova","GA","Georgia Tech Research Corporation","Standard Grant","Tatiana D. Korelsky","08/31/2019","$266,167.00","","chernova@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","8086","$0.00","One of the most significant barriers to the wider adoption of autonomous robotic systems in commercial applications is the challenge of achieving 100% reliable autonomy in unconstrained human environments. One path toward more robust autonomy is to spend more time in research labs improving robot capabilities, delaying deployment until autonomy is entirely robust. Instead, it may be valuable to deploy robots out in the wild and adapt their behavior based on the rare examples, corner cases, and contingencies encountered after deployment in order to achieve near-term, fully reliable autonomy. This approach is specifically motivated by the call center model, in which robots are deployed at end-user sites and contact a remote human operator for assistance whenever an error is encountered. This project develops a system that enables robots to perform lifelong, incremental improvement from remote human assistance with the long-term goal of achieving full autonomy. This research program has significant broader impacts, making personal robots more accessible to everyday people, while also providing opportunities for human-robot interaction that are ideal for educational K-12 programs, as well as undergraduate and graduate education. <br/><br/>Towards these goals, novel algorithms, interfaces, and user studies are being developed to advance the state of the art in three key areas related to the call center model: (1) Robust, Multi-Sensory Task Outcome Detection: multimodal techniques for identifying conditions under which to seek assistance or deploy recovery behaviors; (2) Transparency Devices for Situated Awareness: visual and language interface modalities for increasing the situational awareness of the remote operator and allowing for intuitive interaction, leading to more efficient and correct recovery procedures; (3) Low-Level and High-Level Task Model Refinement: lifelong learning techniques for incorporating corrections and recovery procedures into existing task models, as well as active learning methods to collect more targeted data. The proposed approach is being evaluated on a variety of mobile manipulation tasks that a hotel concierge robot might perform, such as delivery tasks or preparing for and cleaning up after a conference banquet."
"1637961","NRI: Towards Dexterous Micromanipulation and Assembly","IIS","National Robotics Initiative","09/01/2016","07/27/2016","David Cappelleri","IN","Purdue University","Standard Grant","Jeffrey Trinkle","08/31/2019","$1,000,000.00","Song Zhang, Karthik Ramani","dcappell@purdue.edu","Young Hall","West Lafayette","IN","479072114","7654941055","CSE","8013","8086","$0.00","Robots that people are familiar with are large, roughly, human-sized. The theory and design tools for developing such macro-scale robots is well developed. By contrast, the theory and design tools applicable to tiny, or micro-scale, robots is nearly non-existent. The goal of this project is to enable the transition of robot manipulation technology from macro-scale robots to micro-scale robots. The expected project outcomes are: i.) controlled and predictable environments for micro-robotic manipulation and assembly; ii.) a new class of 3D vision-based micro-force sensors and a 3D multi-resolution vision system; and iii.) the identification of dexterous micro-manipulation primitives via human micro-teleoperation with new novel haptic probes. These results will enable the assembly of micro-scale systems that are currently not possible. Such systems are applicable across a wide range of domains, such as cm-to-mm scale robots, micro-sensors, steerable catheters, micro-fluidic, and energy harvesting devices. <br/><br/>At the micro-scale, surface forces dominate the interactions causing unpredictable forces. This project aims to lay the foundations for tools, such as simulators, motion planners, controllers, etc., to be developed for this unique micro-scale environment. The research approach is to reduce the uncertainty in forces present in the micro-world to enable dexterous micro-manipulation and assembly. A new class of manipulation substrates, fixtures, micro-parts, and manipulation tools to control and overcome the levels of adhesion forces present in the micro-world will be created. To enable force control, 3D vision-based micro-force sensing probes along with a multi-resolution 3D vision system to detect the micro-forces in real-time will be developed. Dexterous micro-manipulation primitives will be identified from a human tele-operating a multi-probe micro-manipulation system with micro-force feedback in an augmented reality system. How much and what types of micro-force feedback information is needed for the human to perform different tasks will be studied. These motion primitives together with physics-based simulators can reduce uncertainty in motion planners. The insights gained here will dictate the force-control algorithms implemented in future automated systems."
"1637908","NRI: Collaborative Research: Accelerating Robotic Manipulation with Data-Enhanced Contact Mechanics","IIS","National Robotics Initiative","09/01/2016","07/27/2016","Matthew Mason","PA","Carnegie-Mellon University","Standard Grant","Jeffrey Trinkle","08/31/2019","$422,050.00","","matt.mason@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","8086","$0.00","Robotic manipulation depends upon mechanical contact between robot and object. A better understanding of mechanical contact enables a wider range of more flexible manipulation techniques, which in turn enables the applications of greatest societal benefit such as eldercare, disaster response, or surgery. This project is developing a broader and more accurate understanding of frictional contact, using a fusion of physics and data. The project combines recent advances in a physics-based understanding of frictional contact with new machine learning techniques applied to a large corpus of experimental data. One operation of great interest is manipulation of an object held in the robot gripper, even when the gripper is very simple. Other operations of interest are handling objects in clutter, and manipulation of flexible objects, such as clothing.<br/><br/>The project is attacking several central challenges: modeling frictional contact, modeling deformation, measuring small motions and interaction forces, gathering large amounts of data, and developing techniques for learning in a closed-loop system. Parametric and semi-parametric models enable the project to apply engineering models enhanced with observation data, for both planning and control. New machine learning techniques such as predictive state representations (PSRs) enable identification and modeling of previously hidden state, as well as learning in closed-loop systems. New infrastructure enables gathering of relevant, precise data, on a large scale. The project is developing and employing a Robotic Manipulation Arena, with a unique combination of manipulation resources and instrumentation to provide high volumes of high quality experimental data. The primary outcomes are robust and practical contact models, so that robots can work more dexterously and opportunistically."
"1427300","NRI: Representing and Anticipating Actions in Human-Robot Collaborative Assembly Tasks","IIS","INFORMATION TECHNOLOGY RESEARC, National Robotics Initiative","08/01/2014","07/27/2016","Aaron Bobick","GA","Georgia Tech Research Corporation","Continuing grant","Reid Simmons","07/31/2017","$849,999.00","Irfan Essa, Henrik Christensen, Michael Stilman","afb@wustl.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","1640, 8013","8086","$0.00","For robots to effectively collaborate with humans on a variety of tasks, they must go beyond responding to human actions to anticipating them. This is especially true in the domain of collaborative assembly tasks where the robot assists a human worker by providing tools or parts as required. In this project, a method for formally specifying collaborative assembly tasks is being developed that allows a robot both to understand the action of the human as well as to determine which action the robot has to perform and when. <br/><br/>This project is making fundamental advances to enable task specification to be compiled or converted into a grammar-like description of the human activity. Given this description, a probabilistic inference method that integrates sensory information to analyze the actions of a human and predict which actions the human will take and when. The system learns the necessary perceptual detection information for human actions will be learned from small amounts of training examples of individual actions. By integrating these perceptual observations within a structured representation of the task derived from the specification, the robot can make effective predictions about the timing of human action and can thus anticipate when it will need to provide assistance and of what type. Given these probabilistic predictions the robot makes a plan of action that optimizes a collaboration measure such as how idle time of the human or overall task completion time. The broader impact of this project is along two dimensions. The first is within the small to medium enterprise manufacturing and assembly industry. Successful development of the technologies described is critical for human-robot collaboration in a variety of structured tasks in these domains. Second, the ability to successfully anticipate human behavior is essential for the general integration of assistive robotics into society."
"1638107","NRI: Collaborative Research: Scalable Robot Autonomy through Remote Operator Assistance and Lifelong Learning","IIS","National Robotics Initiative","09/01/2016","07/27/2016","Scott Niekum","TX","University of Texas at Austin","Standard Grant","Tatiana D. Korelsky","08/31/2019","$486,276.00","Andrea Thomaz","sniekum@cs.utexas.edu","101 E. 27th Street, Suite 5.300","Austin","TX","787121532","5124716424","CSE","8013","8086","$0.00","One of the most significant barriers to the wider adoption of autonomous robotic systems in commercial applications is the challenge of achieving 100% reliable autonomy in unconstrained human environments. One path toward more robust autonomy is to spend more time in research labs improving robot capabilities, delaying deployment until autonomy is entirely robust. Instead, it may be valuable to deploy robots out in the wild and adapt their behavior based on the rare examples, corner cases, and contingencies encountered after deployment in order to achieve near-term, fully reliable autonomy. This approach is specifically motivated by the call center model, in which robots are deployed at end-user sites and contact a remote human operator for assistance whenever an error is encountered. This project develops a system that enables robots to perform lifelong, incremental improvement from remote human assistance with the long-term goal of achieving full autonomy. This research program has significant broader impacts, making personal robots more accessible to everyday people, while also providing opportunities for human-robot interaction that are ideal for educational K-12 programs, as well as undergraduate and graduate education. <br/><br/><br/>Towards these goals, novel algorithms, interfaces, and user studies are being developed to advance the state of the art in three key areas related to the call center model: (1) Robust, Multi-Sensory Task Outcome Detection: multimodal techniques for identifying conditions under which to seek assistance or deploy recovery behaviors; (2) Transparency Devices for Situated Awareness: visual and language interface modalities for increasing the situational awareness of the remote operator and allowing for intuitive interaction, leading to more efficient and correct recovery procedures; (3) Low-Level and High-Level Task Model Refinement: lifelong learning techniques for incorporating corrections and recovery procedures into existing task models, as well as active learning methods to collect more targeted data. The proposed approach is being evaluated on a variety of mobile manipulation tasks that a hotel concierge robot might perform, such as delivery tasks or preparing for and cleaning up after a conference banquet."
"1325276","NRI PI Meeting 2013","IIS","National Robotics Initiative","03/15/2013","03/15/2013","Henrik Christensen","GA","Georgia Tech Research Corporation","Standard Grant","Satyandra Gupta","02/28/2014","$103,218.00","","hic@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","7556, 8086","$0.00","The objective of this proposal is to organize the first annual principal investigator meeting for the National Robotics Initiative (NRI). The NRI was launched in the summer of 2011 and there is a clear need to bring together the researchers of the projects awarded in 2012 and 2013 to provide cross-project coordination in terms of common intellectual challenges, methods for education and training, best practice in terms of transition of results and opportunities for broader dissemination of project status, activities and results.<br/><br/>The meeting will be organized as a two-day meeting in the vicinity of Washington DC which broadens the impact to other governmental agencies, as members will be invited due to proximity. The meeting will include all the PIs for the NRI initiative, and invitations to all the program managers for robotics related programs in DC in addition to organizers of the Robotics-VO (Virtual Organization). Finally, the meeting is organized to take place during fall of 2013 to allow the second cohort of NRI projects to participate, allowing greater cross-fertilization of topics and ideas."
"1317989","NRI: Small: Assistive Robots for Blind Travelers","CBET","Gen & Age Rel Disabilities Eng, National Robotics Initiative","09/01/2013","06/30/2016","Aaron Steinfeld","PA","Carnegie-Mellon University","Standard Grant","Alexander Leonessa","02/28/2019","$978,571.00","Aaron Steinfeld","steinfeld@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","ENG","5342, 8013","010E, 7923, 8086, 9251","$0.00","PI: Dias, M. Bernadine; Steinfeld, Aaron<br/>Proposal Number: 1317989<br/><br/>Summary: As robotics technology evolves to a stage where co-robots become a reality, we need to ensure they are equally capable of interacting with humans with disabilities. The proposed work addresses this challenge by exploring meaningful human-robot interaction in the context of assistive robots for blind travelers. For people with disabilities independent transportation remains a major barrier to employment and quality of life. Furthermore, emergency situations necessitating evacuation is one of the greatest fears they face. The key question we seek to answer in the proposed work is: what role can co-robots play in empowering people with disabilities to safely travel to and navigate unfamiliar environments? We hypothesize that co-robots can enhance the safety and independence of these travelers by assisting them to navigate unfamiliar urban environments effectively and providing support when evacuating. We begin our proposed work with a needs assessment to understand the preferences and challenges of blind travelers. The ultimate objective is to enhance the safety and independence of blind travelers.<br/><br/>Intellectual Merit: The proposed work explores three principal research areas applied to three scenarios relevant to assistive robots for blind travelers: (1) information exchange and object manipulation, (2) assistive localization, and (3) urban navigation and emergency building evacuation. The research areas we plan to explore are accessible interfaces, assistive interaction modalities, and effective cooperation mechanisms. Envisioned scenarios include robots assisting humans to localize within necessary resolution and context using a combination of perception, robot localization, and crowdsourcing, robots assisting humans to retrieve lost or fallen objects or locate objects or people of interest, robots assisting humans to interact with other aspects of the environment such as reading notices, and robots assisting humans during emergency evacuation of buildings. We will also explore means of these travelers ""teaching"" the robots to do tasks of interest to them. The robots will have to reason intelligently about task allocation among themselves and coordinating with humans when needed. Overall, the proposed research will significantly advance the knowledge of how assistive robots can meaningfully and effectively interact with travelers with disabilities. The uniqueness of the proposed research is captured in the accessibility of the interfaces, the richness of the interaction modalities, and the flexibility and range of the cooperation mechanisms. The combination of these contributions will significantly advance the state of the art in assistive technology as well as human-robot interaction.<br/><br/>Broader Impact: The team has a strong commitment to undergraduate research experience. Over 75% of the students mentored by the Principal Investigator and Co-Principal Investigator have been women, minorities, or people with disabilities. This commitment extends to the team's instructional activities. Team members regularly incorporate research findings into class presentations, guest lectures, and seminars. The team is also committed to community outreach. Both Dias and Steinfeld regularly speak to non-academics and will include aspects of this project in such talks. A final educational outcome will be several planned workshops at our partner organizations and the outcomes from the proposed work are expected to impact operations and methodologies used at these organizations. The assembled team of researchers and partner organizations further enhances the broader impact of this proposal. Principal Investigator Dias is one of the very few female robotics faculty members at the university and in the discipline. She is engaged in many mentoring and leadership activities to encourage and sustain the participation of women in computing and to address the needs of technologically underserved communities. The proposed team of undergraduate students, a graduate student, and a postdoctoral research assistant will also gain significant mentoring and education through their participation in this research. Industry interaction extends beyond regular contact due to faculty involvement in high profile centers. The research and evaluation program is specifically geared towards people with disabilities. Therefore, the contributions of the proposed work will make significant advancements towards realizing the vision of safe and independent travel for people with disabilities. The results of the proposed work will be disseminated broadly through a variety of avenues and all outcomes of the research will be made available in accessible formats to the community partners and their networks."
"1441258","NRI PI Meeting 2014- Workshop","IIS","National Robotics Initiative","05/15/2014","05/09/2014","Henrik Christensen","GA","Georgia Tech Research Corporation","Standard Grant","Reid Simmons","04/30/2015","$119,601.00","","hic@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","7556, 8086","$0.00","The objective of this award is to organize the annual principal investigator meeting for the National Robotics Initiative (NRI). The NRI was launched in the summer of 2011 and there is a clear need to bring together the researchers of the awarded projects to provide cross-project coordination in terms of common intellectual challenges, methods for education and training, best practice in terms of transition of results and opportunities for broader dissemination of project status, activities and results. The meeting will be organized as a two-day meeting in the vicinity of Washington DC which broadens the impact to other governmental agencies, as members will be invited due to proximity. The meeting will include all the PIs for the NRI initiative, and invitations to all the program managers for robotics related programs in DC in addition to organizers of the Robotics-VO (Virtual Organization). Finally, the meeting is organized to take place during fall of 2014 to allow the first three cohorts of NRI projects to participate, allowing cross-fertilization of topics and ideas."
"1427345","NRI: Considerate Co-robot Intelligence through Ubiquitous Human State Awareness","IIS","National Robotics Initiative","09/01/2014","06/20/2016","Weihua Sheng","OK","Oklahoma State University","Continuing grant","Jeffrey Trinkle","08/31/2017","$725,000.00","Guoliang Fan","weihua.sheng@okstate.edu","101 WHITEHURST HALL","Stillwater","OK","740781011","4057449995","CSE","8013","8086, 9150","$0.00","This project develops a new type of social intelligence for robot companions in assisted living environments. This new intelligence allows a robot to learn a person's daily activity and location without constantly following him or her. Such a human-aware capability frees the robot to do its daily routine work, while being able to attend the human more proactively and effectively. A large population can benefit from the outcome of this project, especially the elderly and patients who need assistance and companionship in their homes. In addition, people in their work places can also benefit from this project, since robot co-workers can provide routine service and emergency assistance with such human-awareness. The proposed educational activities will raise more awareness of robot research through curriculum enrichments in a series of undergraduate and graduate courses. Through the outreach activities, it will stimulate prospective and current college students to pursue degrees and careers in science and engineering. <br/><br/>The objective of this project is to develop the new theoretical/algorithmic framework and the open hardware/software platform for this type of considerate co-robot intelligence. The proposed method is based on ubiquitous human awareness-a robot capability of knowing a person's activity and location in an indoor environment without using onboard sensors. This capability is realized through wearable sensing and computing from a human-based perspective. The major research efforts consist of four parts: co-robot 3D semantic mapping; human activity and location inference; activity prediction and behavioral anomaly detection; experimental evaluation using open hardware/software platforms. A case study will be conducted to evaluate the effectiveness of the proposed considerate co-robot intelligence in elderly fall prevention, detection and intervention."
"1648643","Doctoral Student Career Development at the Workshop on the Algorithmic Foundations of Robotics (WAFR)","IIS","National Robotics Initiative","08/01/2016","07/19/2016","Pieter Abbeel","CA","University of California-Berkeley","Standard Grant","Jeffrey Trinkle","07/31/2017","$10,000.00","","pabbeel@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","8013","7556, 8086","$0.00","This grant will help support the travel costs for U.S. Ph.D. students to attend the International Workshop on Algorithmic Foundations of Robotics (WAFR). WAFR is a high-level, single-track, international workshop that brings roboticists and theoretical computer scientists together. The conference attracts an international crowd that includes academics, industry workers, entrepreneurs, and funding agency leaders. The meeting environment allows participants to exchange ideas, interact with leaders in the field, meet up-and-coming scientists, network, develop collaborations, and generate new ideas on the leading edge of robotic technology."
"1157954","CAREER: Development of a Robotic Prosthetic Hand with Programmable Passive Dynamics","IIS","INFORMATION TECHNOLOGY RESEARC, Cyber-Human Systems (CHS), ROBUST INTELLIGENCE, National Robotics Initiative","07/20/2011","04/28/2016","Ashish Deshpande","TX","University of Texas at Austin","Continuing grant","Jeffrey Trinkle","04/30/2017","$609,869.00","","ashish@austin.utexas.edu","101 E. 27th Street, Suite 5.300","Austin","TX","787121532","5124716424","CSE","1640, 7367, 7495, 8013","1045, 1187, 1640, 7367, 7495, 8086, 9251, CL10, 9150","$0.00","A prosthetic robot hand is designed and developed with programmable passive dynamics modeled after human hands. The goals include the development of mathematical models of variable passive dynamics in human hands, the study of the role of passive dynamics in hand operations, the design of novel joint mechanisms, development of a hand prototype and an EMG control interface, and the development of pedagogical laboratory modules for education.<br/><br/>The project develops technology to substantially improve rehabilitation and quality of life for persons with hand disabilities, including amputees returning from ongoing wars. This work addresses the acute need for substantial improvement in the functionality and control mechanisms for prosthetic hands. Most current prosthetic hands do not address the underlying design constraints of a useful prosthetic hand, namely, weight and size limitations, limited control, ease of control and suitable aesthetics. In addition, students learn this new subject and contribute to the advance of science and technology in this domain. Finally, programs are in place to recruit women, individuals with disabilities, and Native American students in engineering research and education."
"1317815","NRI: Small: Collaborative Planning for Human-robot Science Teams","IIS","National Robotics Initiative","10/01/2013","09/18/2013","Gaurav Sukhatme","CA","University of Southern California","Standard Grant","Reid Simmons","09/30/2017","$482,252.00","","gaurav@cs.usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8013","7923, 8086","$0.00","This project envisions the future of scientific exploration as a collaborative endeavor between human scientists and autonomous robotic systems. The key challenge to materializing this vision lies in combining the expert knowledge of the scientist with the optimization capabilities of the autonomous system. The scientist brings specialized knowledge and experience to the table, while the autonomous system is capable of processing and evaluating large quantities of data. This research leverages these complementary strengths to develop a collaborative system capable of guiding scientific exploration and data collection by integrating input from scientists into an autonomous learning and planning framework. This is achieved by combining probabilistic planning with inverse reinforcement learning to integrate human input and prior knowledge into a unified optimization framework in the context of scientific exploration. The project team is validating the approach in the challenging domain of autonomous underwater ocean monitoring. This domain is particularly well suited for the testing of human-robot collaboration due to the limited communication available underwater and the necessary supervised autonomy capabilities. By integrating feedback from the human user into an algorithmic planning framework, the goal is to improve the efficiency of scientific data collection and gather data about phenomena that were previously outside the reach of scientific investigation. The use of autonomous vehicles for scientific data collection is becoming increasingly prominent; however, the research community lacks a foundational understanding of the interactions between scientists and autonomous vehicles. This work focuses on principled methods for integrating human input into algorithmic optimization techniques moving towards the goal of supervised autonomy for robots.<br/><br/>This project has the potential to change the way scientific data are collected through the development of a foundational framework for human-robot scientific collaboration. Such a framework is expected to have broad implications throughout the fields of human-robot interaction and artificial intelligence. The proposed research is being integrated into the robotics and computer science curriculum at both the graduate and undergraduate levels. It is also being utilized for K-12 robotics outreach programs in Los Angeles. The algorithms created in this research are transitioned to field tests and operations via ongoing collaborations with the Monterey Bay Aquarium Research Institute (MBARI) and the Southern California Coastal Ocean Observing System (SCCOOS)."
"1317214","NRI: Small: Collaborative Research: Don't Read my Face: Tackling the Challenges of Facial Masking in Parkinson's Disease Rehabilitation through Co-Robot Mediators","IIS","National Robotics Initiative","09/15/2013","05/14/2015","Ronald Arkin","GA","Georgia Tech Research Corporation","Standard Grant","Reid Simmons","08/31/2018","$587,987.00","","arkin@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","7923, 8086, 9251","$0.00","The overarching scientific goal of this project is two-fold: (1) to develop a robotic architecture endowed with moral emotional control mechanisms, abstract moral reasoning, and a theory of mind that allow corobots to be sensitive to human affective and ethical demands, and (2) to develop a specific instance of the architecture for a co-robot mediator between people with ""facial masking"" due to Parkinson's disease (PD) that reduces their ability to signal emotion, pain, personality and intentions to their family caregivers, and health care providers who often misinterpret the lack of emotional expressions as disinterest and an inability to adhere to treatment regimen, resulting in stigmatization. To tackle these problems, the project brings together two roboticists with extensive prior experience in robot ethics and modeling emotions as well as implementing them in integrated autonomous robotic systems. The robotics expertise is combined with that of an expert in early PD rehabilitation and daily social life. The project will build on extensive software, hardware and data set resources, including complex robotic control architectures with ethical control mechanisms, personality and emotion models, and affect and natural language capabilities.<br/><br/>The general expected outcome of the project is an architecture for co-robots that can be adapted to a great variety of health care scenarios in an effort to enrich and dignify already stressed and stigmatized relationships between humans. The project also includes novel educational efforts such as a course in occupational therapy robotics as well as significant K?12 outreach through the Tufts Centers for STEM Diversity and for Engineering Education and Outreach, as well as various important community and public activities such as presentations on health care robotics to focus and patient groups."
"1527148","NRI: Real Time Observation, Inference and Intervention of Co-Robot Systems Towards Individually Customized Performance Feedback Based on Students' Affective States","IIS","National Robotics Initiative","09/01/2015","08/11/2015","Conrad Tucker","PA","Pennsylvania State Univ University Park","Standard Grant","John Krupczak","08/31/2018","$342,574.00","Timothy Brick","ctucker4@psu.edu","110 Technology Center Building","UNIVERSITY PARK","PA","168027000","8148651372","CSE","8013","8086","$0.00","This NSF National Robotics Initiative project will investigate the potential of a cycle of observation, inference and intervention by co-robot systems to enhance students' affective states and improve their performance on engineering laboratory tasks. Co-robots are robots that work side-by-side with humans, assisting them and adapting to their needs. The two-way exchange of knowledge between students and co-robots creates a reciprocal relationship, in which each party learns from the other in service of a common goal. Affective states, such as frustration and engagement, play a major role in students' performance on everyday learning tasks. A student who is overly stressed or distracted may commit errors that would be otherwise easy to avoid. A co-robot system that is cognizant of students' affective states can intervene to prevent these errors. The results of this project may provide a template for skill-based instruction on topics well beyond engineering. Currently, such learning requires extensive interactions between a student and an instructor, with the instructor providing intensive feedback at all times. In many cases, personality mismatches or other issues between instructor and student can lead to frustration, learning difficulties, and eventual dropout. Furthermore, one-on-one learning is limited by scalability challenges, as an increase in the number of students, without a proportional increase in trained instructors, can result in decreases in quality and quantity of instructor time allocated to each student. Co-robot learning systems will be able to mitigate these challenges by providing both real time and scalable feedback systems that adapt to the individual needs of students and help to minimize the amount of human instructor time required by each student. <br/><br/>This research will acquire facial, auditory, and body gesture data from students using the integrated visual, audio and depth sensory system of the co-robot. The system will make statistical inferences of students' affective states, based on machine learning classification of facial and body language data. Visual feedback will be used to present students with interventions (visual instructions and commentary) intended to enhance their affective state and improve their performance on laboratory tasks. The project will assess the impact of co-robots' ability to improve students' affective states and enhance students' performance on laboratory tasks over repeated iterations of learning and testing. This project will lead to a better understanding of how students interact and function during potentially stressful laboratory activities. The co-robot systems proposed in this work will help discover the correlations that exists between students' affect and task performance. Co-robots will actively adapt to the manner in which students learn complex engineering tasks and the affective states that accompany that learning. Co-robot systems that predict the effectiveness of specific intervention strategies for each student and situation will lead to individually-tailored student feedback that serves both students and instructors towards enhancing student performance over time. This proposal advances the impact of co-robots into educational research and practice and extends knowledge of how to succinctly represent the complexities of human behavior in digital form."
"1208637","NRI-Small: Robotic Treadmill Therapy for Lower Spinal Cord Injuries","IIS","National Robotics Initiative","09/01/2012","04/17/2015","John Hollerbach","UT","University of Utah","Standard Grant","Jeffrey Trinkle","08/31/2017","$916,999.00","Jake Abbott","jmh@cs.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8013","7923, 8086, 9251, 9150","$0.00","New rehabilitation therapies for patients with incomplete lower spinal cord injuries (SCI) will be developed using the Treadport, a robotic treadmill that provides a realistic walking experience in a safe and flexible virtual environment. The Treadport overcomes limitations of current rehabilitation treadmills, which are too dissimilar from everyday walking and therefore limit a patient's recovery. We will seek to improve a patient's walking speed and effort, resistance to falling by strengthening and training a patient to unexpected perturbations, and arm swing coordination which is critical for normal walking. The intellectual merit is the body-weight assisted robotic treadmill training, and arm swing assistance using a light-weight exoskeleton. The robotic treadmill will provide monitored assistance and virtual reality training scenarios not currently possible.<br/><br/>The broader impact is the development of new technology and scientific understanding that will improve the lives of SCI patients so that they have the most mobility possible given their injuries. The proposed research combines the disciplines of robotics, biomechanics, and physical therapy, and requires interdisciplinary training. The socially positive nature of this project is expected to be especially attractive to underrepresented populations in engineering and computer science, particularly women and people with disabilities.<br/><br/>This proposal addresses the co-worker theme of the National Robotics Initiative, particularly rehabilitation, orthotics and prosthetics."
"1427313","NRI: Biologically Inspired Feedback Control of Robots Interacting with Humans to Cooperate and Assist with Repetitive Movement Tasks","ECCS","National Robotics Initiative","09/01/2014","08/26/2014","Tetsuya Iwasaki","CA","University of California-Los Angeles","Standard Grant","Radhakisan S. Baheti","08/31/2017","$450,000.00","","tiwasaki@ucla.edu","11000 Kinross Avenue, Suite 211","LOS ANGELES","CA","900952000","3107940102","ENG","8013","8086","$0.00","Biologically inspired feedback control of robots interacting with humans to cooperate and assist with repetitive movement tasks<br/><br/>The project will address the fundamental problem of how to control the motion of a robot so that it can cooperatively work with humans to assist them in repetitive tasks. Oscillatory body movements constitute an elementary means for various tasks in human living. Such repetitive movements include essential life functions such as heart beat, breathing, eating (chewing), walking; basic daily tasks such as brushing teeth, washing face; house-hold chores such as cleaning windows, sweeping floor; health/entertainment activities such as dancing, swimming, cycling, rowing; and manufacturing labors such as moving objects in factory assembly lines. Robots and mechanical devices that assist such human movements would be found useful in a number of contexts. A robotic manipulator and a human arm may grab a common tool to work together on repetitive tasks where the former assists the latter by providing force and stability to reduce burden on the human. An exoskeleton may be worn to complement reduced capability of, or provide rehabilitations for, elderly people and patients with neurological disorders or physical disabilities. Thus, well-designed assistive devices for oscillatory movements would significantly contribute to improving quality of human life. Design of robotic mechanisms for such assistive devices is surely a challenging task. Equally challenging is the design of control algorithms that command the actuators and govern the motion of the robotic device. The state-of-the-art control technologies allow a designer to program a robot to achieve prescribed motion with speed, precision, and robustness, as seen for instance in industrial manipulators. However, if such robots interact with humans, they would be perceived as stiff, stubborn, or even dangerous, and are therefore not suitable as co-robots in direct support of humans. What is needed is control algorithms that make robots understand human intentions, cooperate with humans without insisting on their preprogramed operations, and assist with human tasks. Development of such algorithms will be the focus of this project.<br/><br/>This basic research aims to establish a systematic method for designing a feedback controller for a general robotic system interacting with a human to stabilize the oscillation intended by the human and to reduce the burden on the human by providing assistive forces. The control architecture is inspired by the central pattern generator (CPG) -- neuronal circuits that command muscle contractions to achieve rhythmic body movements during animal locomotion. CPGs are attractive for engineering applications due to its ability to conform their oscillations to natural dynamics of a varying environment through sensory feedback. This exploratory research will investigate the potential of the CPG architecture to provide a viable foundation for a new system design for achieving co-robots that assist humans to execute oscillation tasks. The controller is realized as an interconnection of identical units, emulating neuronal dynamics. The problem is formulated as the search for the interconnection such that the robot-human-CPG system has a stable limit cycle in which human decides an appropriate oscillation and CPG-controlled robot assists. The method of multivariable harmonic balance will be employed to obtain a convex characterization of feasible interconnection matrices that meet oscillation specifications. The approximate nature of the design method will be complemented by extensive simulations as well as physical experiments on robotic manipulators. While the central theme of control theory has been the regulation around an equilibrium point of a dynamical system, capability of generating coordinated autonomous oscillations can be extremely useful in many engineering applications. The basic research proposed here will provide an initial stepping stone toward a new paradigm for cooperative pattern generations by feedback control."
"1328816","NRI: Large: Collaborative Research: Fast and Accurate Infrastructure Modeling and Inspection with Low-Flying Robots","IIS","National Robotics Initiative","09/15/2013","06/15/2016","Jerome Hajjar","MA","Northeastern University","Standard Grant","Jeffrey Trinkle","08/31/2017","$328,050.00","","jf.hajjar@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173732508","CSE","8013","7925, 8086, 9251","$0.00","The goal of this project is to transform the efficiency, fidelity, and safety of current critical infrastructure inspection methods by combining human judgment with machine intelligence through the development of an autonomous robotic inspection assistant. The proposed work utilizes small aerial robots, coupled with three-dimensional imaging and the state-of-the-art in planning, modeling, and analysis to develop safe and efficient, high-precision assessment of structures. The key themes of the proposed work are: (1) rapid infrastructure modeling and analysis of large complex structures via a small autonomous aerial robot with 3D mapping capabilities; (2) immersive inspection and structural assessment to combine shape and appearance into an integrated representation amenable to structural health evaluation by an inspector; and (3) adaptive aerial vehicle motion plans that seek to learn from the experience of human inspectors and facilitate as autonomous inspection assistants. The proposed work is exploring the role of humans in the entire cycle from deployment of flying robots to registering data to the assessment. This project brings together members of participating communities and is developing curriculum to engage undergraduate and graduate students from robotics and civil engineering in the proposed research."
"1426998","NRI: Large-Scale Collaborative Semantic Mapping using 3D Structure from Motion","IIS","National Robotics Initiative, INFORMATION TECHNOLOGY RESEARC","09/01/2014","06/24/2016","Zsolt Kira","GA","Georgia Tech Applied Research Corporation","Continuing grant","Jie Yang","08/31/2017","$600,000.00","Frank Dellaert","zkira@gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013, 1640","8086","$0.00","The project develops techniques to advance the state of the art in tackling the challenges associated with creating such representations using robots, namely issues related to the scalability and semantic interpretability of such maps. The research activities include advancement of knowledge in multiple fields, such as computer vision, structure from motion, robotics, and semantic mapping. The results have the potential for many societal applications including city planning, asset management, creation of historical records, and support for autonomous driving. The demonstration of the developed theoretical techniques for real-time interaction between humans and robots facilitated by a semantic map enables even greater societal benefit, for example for emergency management, crime prevention, and traffic management. Direct educational impact is anticipated for graduate students and the results are disseminated through both publications and software, allowing the community to leverage the results.<br/><br/>This research program advances real-time large-scale distributed semantic mapping of outdoor environments. Specifically, the research team is enabling real-time large-scale semantic mapping by using unsupervised object discovery, obviating the need for large sets of annotated videos for each object category which becomes prohibitive when dealing with hundreds of object categories. The research team frames this process within the structure from motion optimization framework, thereby leveraging geometric and multi-view constraints and features to increase reliability of object track association as well as category clustering. In addition to address scalability, the project develops a distributed, multi-robot system, allowing large teams of air and ground vehicles to cooperatively build a map of large geographic areas in reasonable time frames. Furthermore, the project develops techniques to make the maps more semantically-meaningful and hence interpretable by humans. To accomplish this objective, the research team uses automatic techniques to attach semantic labels to objects discovered in an unsupervised manner. Moreover, humans can interact with the system at multiple levels. Human users can refine both the object categories and semantic labels to increase their accuracy, as well as designate dynamic targets of interest and task robots to track them."
"1305267","II-NEW: Robot Arms for Interactive Perception of Highly Non-Rigid Objects","CNS","COMPUTING RES INFRASTRUCTURE, IIS SPECIAL PROJECTS, National Robotics Initiative","09/15/2013","09/25/2013","Stanley Birchfield","SC","Clemson University","Standard Grant","Jeffrey Trinkle","08/31/2014","$94,585.00","Ian Walker","stb@clemson.edu","230 Kappa Street","CLEMSON","SC","296340001","8646562424","CSE","7359, 7484, 8013","7359, 9150","$0.00","This infrastructure proposal, acquiring two Kinova JACO Research arms to upgrade aging mobile manipulators, will enable fundamental research in interactive perception with non-rigid objects. Traditionally, robotics has focused upon rigid objects. The PI team proposes to enable study of the problem of sifting through a cluttered pile of non-rigid objects, many of which are occluding one another, and properly classifying and manipulating each object. The proposed work is not merely concerned with computing grasp points of rigid objects for which training examples are available for off-line learning; rather the focus is upon segmenting, grasping, and interacting with non-rigid objects for which models are difficult to construct. The aim is to merge ideas from interactive perception with those of the textile manipulationn to learn object properties over time and interact with objects in unstructured<br/>environments.<br/><br/>Non-rigid objects are important in a number of highly-visible industries, such as textiles and automotive manufacturing, so broader impacts are numerous. Moreover, automated laundry folding will have a significant impact on service robots, where products such as vacuum cleaning robots and lawn mowing robots have already reached the commercial sector. The findings of the research will be made available to the research community via conference proceedings and journal publications, and the research will be integrated into existing courses to reach undergraduate and graduate students. Presentations to nearby schools to introduce disadvantaged and underrepresented high school students to STEM careers is included in the outreach plan."
"1426756","NRI: Collaborative Research: Exploiting Granular Mechanics to Enable Robotic Locomotion","IIS","National Robotics Initiative","08/15/2014","08/08/2014","Robin Murphy","TX","Texas A&M Engineering Experiment Station","Standard Grant","Jeffrey Trinkle","07/31/2017","$180,163.00","","murphy@cse.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","8013","8086","$0.00","We need robots to extend our reach into dirty and dangerous environments. To do so, mobile robots must be able to locomote in messy unstructured terrains. Conventional mobile robots have not begun to display the multi-functionality of organisms that inhabit natural terrains. This is because mobile robots have been created in, and their models mainly validated on, clean hard laboratory floors, whereas biological organisms have evolved to contend with heterogeneous, dirty and unpredictable environments. One important example of such real world complex terrain, often overlooked by our community despite its ubiquity, involves loose granular materials commonly found in deserts, disaster sites, containers, and caves. Therefore creation of the next level of mobility to traverse dirty environments requires simultaneous advances in both robotics and physics, particularly regarding the interactions associated with desired behaviors. <br/><br/>The proposed work is built on a foundation of geometric mechanics, granular physics of intrusion and biological inspiration from desert-dwelling snakes. We use geometric mechanics, a field that applies principles from differential geometry to problems in classical mechanics, to design gaits for biologically inspired robots. We bring the benefits of the geometric tools to bear on granular environments: in even these mathematically ""messy"" systems, we can begin to efficiently analyze gaits. The key concept in this effort is that systems with complicated, nonlinear low-level physics often exhibit much ""cleaner"" high-level motion, often approximated by a kinematic relationship. Development of such high-level motion controllers will be aided by our ability to discover basic biological principles of locomotion in granular media. We will therefore develop computationally efficient analysis tools for granular materials and will develop techniques to study the locomotion of systems on the surface of granular media."
"1528121","NRI: Socially Aware, Expressive, and Personalized Mobile Remote Presence: Co-Robots as Gateways to Access to K-12 In-School Education","IIS","National Robotics Initiative","09/01/2015","08/06/2015","Maja Mataric","CA","University of Southern California","Standard Grant","Alexander Leonessa","08/31/2018","$600,000.00","Gisele Ragusa","mataric@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8013","010E, 8086","$0.00","Participating in the school environment is essential to children's social, emotional, and cognitive development and learning. It has long been recognized that the quality of a student's school experience is important not only for the academic and achievement outcomes, but for fostering self-esteem, self-confidence, and general psychological well-being. Yet annually 26.6% of America's children have health or behavioral challenges that cause them to miss significant amounts of school, and 13% of all US K-12 public school students receive interventions due to learning disabilities or emotional disturbances. This project focuses on the problem of using mobile remote presence co-robots as a means to provide numerous K-12 aged children who cannot be present in school access to the curricular and social learning experiences critical to their development and future outcomes. Using mobile remote presence for access to K-12 classrooms for homebound students may be a powerful gateway for minimizing the effects of physical separation from the school environment. This project develops methods that enable the creation of personalizable robots that allow shared autonomy, socially appropriate movement and socially expressive nonverbal communication in dynamic in-class K-12 environments, allowing children to be truly embodied in the classroom, even from a distance. The impact of this NRI project spans K-12 education at large, but also applies to general uses of mobile remote presence systems outside of the classroom setting, for both education and training. In addition, the project connects the research themes with outreach; it engages K-12 students and teachers in co-robot-themed activities and holds annual NRI-themed workshops at large-scale public venues. The broader outreach program is designed to train students in STEM, so they can become not only end users of robotics and other technologies but capable of developing such technologies themselves, thereby contributing to the US STEM workforce. <br/><br/>This proposal focuses on developing control algorithms for mobile remote presence (MRP) co-robot systems that will improve human access to a learning/training environment, focusing on homebound K-12 students, but with general implications to users of all ages and a variety of contexts. Work with MRP systems has identified key missing technical capabilities necessary for facilitating natural remote interaction and learning: 1) simple, socially-appropriate autonomous behavior and context awareness that reduces user cognitive load; 2) expressiveness for conveying the user's affect and communicative intent; and 3) the ability to personalize the way the user interacts through the MRP. This project addresses these challenges with participatory user-informed algorithm development, system integration, and evaluation. Specifically, it first develops an approach to automating and facilitating spatial and social context awareness for the operator and the MRP, and uses it to enable the two research thrusts, social appropriateness and expressiveness, with algorithmic methods for personalizing both. To ground the results in the selected real-world context, iterative design and evaluation is performed in the K-12 in-class setting, involving users across the age and education span, providing a test of the co-robot's relevance, effectiveness, and robustness. The project brings together a pair of interdisciplinary experts with a track record of successful past collaborations and three partners: industry, deployment, and outreach, committed to a project timeline with specific evaluable milestones."
"1525889","NRI: Collaborative Research: Versatile Locomotion: From Walking to Dexterous Climbing with a Human-Scale Robot","IIS","National Robotics Initiative","09/01/2015","08/06/2015","Mark Cutkosky","CA","Stanford University","Standard Grant","Jeffrey Trinkle","08/31/2018","$450,000.00","","cutkosky@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","8013","8086","$0.00","The project aims to give legged robots the skills to navigate a wide variety of terrain. This capability is needed to employ robots in applications such as search-and-rescue, construction, and exploration of remote environments on Earth and other planets. The multidisciplinary team, composed of researchers at Duke, Stanford, UC Santa Barbara, JPL, and Motiv Robotics, will develop a robot to climb a variety of surfaces ranging from flat ground to overhanging cliffs. Using an array of sensors, unique hands, and sophisticated algorithms, the robot will dynamically adopt walking, crawling, climbing, and swinging strategies to traverse wildly varied terrain. During the course of this research, the team hopes to achieve the milestone of the first demonstration of a human-scale rock climbing robot. The research is also expected to lead to insights into cognitive and biomechanical processes in human and animal locomotion.<br/><br/>Although rock climbing serves as an ideal proving ground for the work, this project conducts basic research to address more a general-purpose goal; namely, to provide the physical and cognitive skills for robots to adaptively navigate varied terrain. It takes a dexterous climbing approach, which uses non-gaited, coordinated sequences of contact to move the body, much as dexterous manipulation uses contact with the fingers and palm to move an object. It will apply principles from optimization, machine learning, bioinspiration, and control theory to make intellectual contributions in several domains, such as robot hand design, planning algorithms, balance strategies, and locomotion performance measurement. Novel grippers, sensor-based planning strategies, reactive maneuvers, and locomotion metrics will be developed during the course of this research."
"1524363","NRI: Collaborative Research: Human-Supervised Manipulation of Deformable Objects","IIS","Gen & Age Rel Disabilities Eng, National Robotics Initiative","08/01/2015","06/23/2016","M. Cenk Cavusoglu","OH","Case Western Reserve University","Standard Grant","Alexander Leonessa","07/31/2018","$421,191.00","","mcc14@case.edu","Nord Hall, Suite 615","CLEVELAND","OH","441064901","2163684510","CSE","5342, 8013","8086, 9251","$0.00","The goal of the proposed work is to develop algorithms that enable human-supervised robotic manipulation of deformable objects under significant uncertainty. Manipulation of deformable objects is essential in surgery, which involves complex manipulations of delicate and highly deformable tissue under substantial uncertainty, and in manufacturing, where many assembly tasks involve flexible parts and materials (e.g. cables, textiles, and composites). Recent advances in robot hardware (e.g. the da Vinci and Baxter robots) have made robotic manipulation of deformable objects physically possible but robots still lack the algorithms necessary to perform practical tasks in this domain when there is significant model uncertainty. This project will have broad societal impact through its applications in surgical and manufacturing robotics. The ability to robustly manipulate deformable structures is an important precursor technology towards realizing intelligent robotic surgical assistants. Robotics and its medical applications have the potential to inspire children to pursue careers in STEM fields and meet the needs of America's growing health-care and manufacturing robotics industry. Integration of the research activities with education will emphasize actively involving undergraduates in research activities and introducing new lecture material and projects into undergraduate and graduate courses. Also, special emphasis will be given to recruit qualified students from under-represented groups.<br/><br/>The purpose of the proposed research is to develop algorithms that enable human-supervised robotic manipulation of deformable objects under substantial uncertainty. Specifically, the research will focus on developing adaptive complexity models for modeling deformable object dynamics and associated uncertainty, planning algorithms for integrated exploration and task execution, control algorithms for robust manipulation of deformable objects under uncertainty, and algorithms for effective human supervision of robotic manipulation of deformable objects. The intellectual merit of the project comes from its fundamental contributions to robotic modeling, planning, and control algorithms for manipulation of deformable objects. The research will have impact in the fields of robotic manipulation, motion planning under uncertainty, and compliant manipulation control."
"1526960","NRI: Active Tendon-Driven Orthosis for Prehensile Manipulation After Stroke","IIS","National Robotics Initiative","08/01/2015","08/12/2015","Matei Ciocarlie","NY","Columbia University","Standard Grant","Reid Simmons","07/31/2018","$599,386.00","Joel Stein","mtc2103@columbia.edu","2960 Broadway","NEW YORK","NY","100276902","2128546851","CSE","8013","8086","$0.00","This project develops wearable and functional hand orthoses for stroke survivors, to act as both assistive and training devices. For daily living, such a device could provide assistance with manipulation tasks, increasing the sense of control and the ability to live independently. For rehabilitation, a wearable orthosis could enable functional training on real-world manipulation tasks, extend training beyond the small number of sessions performed in a clinical setting, and provide distributed training during the course of daily activities rather than block training in designated therapeutic sessions. From a healthcare perspective, this work will enable a shift from devices used in a clinic under medical supervision towards unobtrusive functional orthoses used during daily life. From an engineering perspective, it will advance towards a new class of active wearable systems, controllable by natural movement or similarly intuitive interfaces. <br/><br/>New approaches will be required to build a device that is simultaneously dexterous (for complex manipulation), non-intrusive (for daily living) and intuitive to control. To achieve this, we investigate ways to assist joint movement using a network of artificial tendons routed on the surface of the hand and arm, without adding large structural components such as an exoskeleton. A key tenet of our approach is that a hand orthosis can provide meaningful assistance with daily manipulation tasks even when using a number of actuators far smaller than the number of joints in the hand. We will also investigate multiple control methods, including unimanual operation (where endogenous movement of the paretic limb is detected and enhanced) and bilateral operation (where movement of the less affected limb is used to generate control signals for the paretic one)."
"1522240","NRI: RUI: Autonomous Vehicles: Ethics, Design, and Risk","IIS","National Robotics Initiative","07/15/2015","07/21/2015","Patrick Lin","CA","California Polytechnic State University Foundation","Standard Grant","Frederick M Kronz","06/30/2017","$164,436.00","","palin@calpoly.edu","One Grand Ave","San Luis Obispo","CA","934070830","8057562982","CSE","8013","8086, 9229","$0.00","This collaborative research project will examine ethical issues that arise in connection with autonomous vehicles (such as self-driving cars), from programming choices to guarding against abuse. Collaborators include engineers, philosophers, and legal experts. Autonomous vehicles are starting to emerge, and they are predicted to solve a lot of problems, especially traffic accidents and fatalities. But they will not be foolproof, meaning that (just like any other technology) they will be subject to error, misuse, and failure, and some designs may be better or more responsible than others. Even if autonomous vehicles can save thousands of lives each year, industry and researchers are not thereby released from the obligation to consider ethical, legal, and policy implications of the bringing about of unnecessary and wrongful deaths. The loss of lives in this way could have a devastating, high profile effect on the nascent industry. The results of this project will benefit both industry and the academia. Deliverables will include a public report, a book, university-level courses, conference talks, and academic as well as media publications. Beyond the autonomous driving industry, this project provides research and materials in robot ethics and technology ethics more broadly, as pressure mounts to integrate ethics into engineering and science programs.<br/><br/>The PI and his team will examine the different ways that an autonomous vehicle could be involved in an accident, as well as crash-avoidance and crash-optimization options to mitigate the harm that occurs in unavoidable crashes. They will also investigate the technical and policy feasibility of various ethical frameworks to guide actions by autonomous vehicles by drawing from classical dilemmas in ethics, ongoing work in technology ethics, and real engineering in academic and industry labs on the front lines. They will also attend to broader issues, such as how autonomous cars should handle medical emergencies, criminal conduct, and other common situations. The project is aligned with the NSF's mission, as well as the call of its NRI program for research to gain a better understanding of the long-term social, behavioral and economic implications of co-robots. Because autonomous vehicles are poised to be the first robots to be integrated with society on a large scale, they will set the tone for the entire social or co-robotics industry. It is crucially important to engage ethics ahead of these developments to consider the issues and scenarios in advance of potentially high-profile accidents."
"1427193","NRI: Collaborative Research: Optimal Interaction Design Framework for Powered Lower-Extremity Exoskeletons","IIS","COLLABORATIVE RESEARCH, IIS SPECIAL PROJECTS, National Robotics Initiative","09/01/2014","06/05/2015","Joo Kim","NY","New York University","Standard Grant","Ephraim P. Glinert","08/31/2017","$597,176.00","","joo.h.kim@nyu.edu","70 WASHINGTON SQUARE S","NEW YORK","NY","100121019","2129982121","CSE","7298, 7484, 8013","5952, 8086, 9251","$0.00","The goal of this project is to establish a user-centered optimal design framework for customized lower-extremity exoskeletons, in which human-exoskeleton physical interactions and dynamics can be predicted and optimized to provide design requirements. The objectives are to establish: (1) mathematical models of disability parameters, performance indices, and desired/task-failure motions; (2) formulations for constraint status, contact load, and coupled dynamics; (3) optimization method of contact load distribution; (4) sensor integration for required data and experimental validation; and (5) prototype design modification, fabrication, and usability tests. This research will open a new paradigm for systematic characterization of disability parameters and the desired motions from engineering perspectives, leading to user-specific mathematical models. The potential to reduce the need for involving human subjects as part of the design iteration loop will result in accelerated development and better performing assistive devices at reduced cost. Given the growing number of individuals who would benefit from customized exoskeletons as assistive devices, this project will have broad social impact by resolving major hurdles to their widespread use. This research will be integrated into comprehensive education and outreach plans for minority students and individuals with disabilities.<br/><br/>The algorithm with controlled infeasibility will provide physically valid solutions of task failure as well as desired motions for integrated human-exoskeleton systems. The concurrent formulations for constraint status/loads and coupled dynamics will resolve the problems of incorporating physical interactions into optimal motions. As a novel design method, this project will introduce optimal contact load distribution subject to exoskeleton dynamics and transformation of a complex design into a dynamically equivalent model. The feedback loops in the design framework will serve as self-evaluation/contingency plans. This research will transform exoskeleton technologies through user-centered design and predictive evaluations by systematically considering end-user requirements and limitations right from the beginning and at each stage of design. Project outcomes will represent a significant breakthrough that will bring exoskeleton technologies to the next level by (a) functioning as a central hub that systematically connects and integrates relevant disciplines; and (b) providing customized design, reduced design cycle, optimized systems with light weight and natural motion, and improved user comfort and safety."
"1527232","NRI: Deep Learning Unmanned Aircraft Systems for High-Throughput Agricultural Disease Phenotyping","IIS","National Robotics Initiative","08/01/2015","10/20/2015","Hod Lipson","NY","Cornell University","Standard Grant","Jeffrey Trinkle","07/31/2018","$1,149,273.00","Michael Gore, Rebecca Nelson","hod.lipson@columbia.edu","373 Pine Tree Road","Ithaca","NY","148502820","6072555014","CSE","8013","8086","$0.00","An estimated 13% of crops are lost globally to plant diseases. Disease detection, identification, and tracking is performed today by crop scouts, a process that is expensive, slow and difficult, and is impractical to expand to cover all crops. The project involves developing AI-drones that work side-by-side with farmers and identify specific diseases and assess their progress. The use of intelligent drones for crop monitoring will allow farmers to respond quickly to emergent diseases, nutrient stress, and other potentially devastating damages without the prohibitive expense of hiring a crop scout. This ability could increase productivity, and may also help predict, track and respond to epidemics for national and global food security. In addition, this technology will also be used for ongoing collection of precise plant performance data for breeding resistance.<br/><br/>The central hypothesis of this proposal is that drones equipped with trained Convolutional Neural Networks can provide a transformative increase in actionable crop disease identification. A secondary hypothesis is that the proposed phenotyping at the individual plant level will also provide unprecedented resolution of data for future modeling, breeding, and data-driven yield optimization. In order to test this hypothesis, we will develop a UAS platform to collect images over university owned experimental crops, and aim to develop AI to identify pathologies at an accuracy that is on par with human experts. The UASs will consult human experts in ambiguous cases and gradually learn to make decisions autonomously. The key challenge will be development of AI that can reliably diagnose disease with a good accuracy of detection to false alarms. Automatic identification of disease is a challenging machine vision task given the complexity of images, exacerbated by variable lighting and weather conditions, and navigation/stability control."
"1426945","NRI: Information-Theoretic Trajectory Optimization for Motion Planning and Control with Applications to Space Proximity Operations","IIS","National Robotics Initiative","09/01/2014","08/21/2014","Panagiotis Tsiotras","GA","Georgia Tech Research Corporation","Standard Grant","Radhakisan S. Baheti","08/31/2017","$700,000.00","Evangelos Theodorou","p.tsiotras@ae.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","8086","$0.00","Robotic operations in space are indispensable for many missions both in Earth orbit and beyond. Satellite servicing and refueling, space station resupply with consumables, removal of space debris, spacecraft structural integrity inspection, crew assistance, as well as support for deep space missions to Mars and other planets and comets, all require the assistance of highly accurate, reliable and autonomous (or semi-autonomous) space robots. To date, most robotic operations in space are performed in a closely supervised mode by a human operator. This limits both the flexibility and the type of missions that can be performed (for example, the time for light to travel to and from Mars takes about 15 minutes, making ""real-time"" remote control impossible). This research aims at developing the necessary theory and algorithms to be able to utilize active exploration using robust, reliable sensing and planning of a free-flying space robots in the vicinity of another body, in order to perform proximity operations (including autonomous rendezvous and docking in space). One of the challenges in these types of problems is the uncertainty in understanding the surroundings in order to plan suitable control actions. In order to handle these challenges we utilize novel tools and methodologies from the field of stochastic optimal control along with new advances describing the spacecraft attitude dynamics and kinematics of spacecraft in orbit. In order to ensure that the algorithms we develop perform in real-life as expected, the theoretical results will be experimentally validated on a high-fidelity 5-dof spacecraft simulator facility. This work will have an immediate impact on the US capabilities to perform monitoring and servicing of satellites in space routinely, by advancing the state-of-the-art in perception and path-planning of orbiting spacecraft in the vicinity of another body, man-made or natural. Although the emphasis of this work is primarily on space robotic applications, the same techniques can be used in all similar problems where an intelligent agent needs to navigate autonomously in an uncertain and dynamic environment.<br/><br/>The proposed research tackles a fundamental problem in autonomous/robotic systems, namely, the integrated sensing and planning under uncertainty. The current paradigm in the literature utilizes perceptual cues (especially those based solely on visual information) essentially as surrogates of full-state feedback estimators, thus enforcing an artificial separation of perception and control action. This dichotomy between sensory data acquisition/processing, and control/actuation strategies - deeply rooted in the community from its wide applicability to the stabilization of linear systems subject to additive noise (?separation principle?) - is unsuitable for this problem, where information gathering (perception/sensing) is tightly coupled with motion (control). To overcome the aforementioned limitations, in this work it is proposed to use tools from stochastic optimal control in order to extract actionable information from raw sensory inputs. A key ingredient of the proposed approach is to keep track of the first and second order statistics of the estimation error and treat them as the state, so that control actions depend on both of them. The result is a new, computationally more efficient, methodology to maximize information gathering during the exploration phase and to optimize over distributions of trajectories during the execution phase."
"1426828","NRI: Collaborative Research: Minimally Invasive Robotic Non-Destructive Evaluation and Rehabilitation for Bridge Decks (Bridge-MINDER)","IIS","National Robotics Initiative, IIS SPECIAL PROJECTS, COLLABORATIVE RESEARCH","08/01/2014","11/06/2015","Jingang Yi","NJ","Rutgers University New Brunswick","Standard Grant","Reid Simmons","07/31/2017","$628,499.00","Nenad Gucunski, Hung La","jgyi@rutgers.edu","33 Knightsbridge Road","Piscataway","NJ","088543925","8489320150","CSE","8013, 7484, 7298","8086, 5936","$0.00","Bridges are critical components of modern transportation infrastructure. Bridges deteriorate over time as a result of material aging, excessive use and overloading, environmental conditions, inadequate maintenance, and deficiencies in inspection and evaluation. Building on the recent advances in robotics and automation technologies, the objective of this project is to develop a novel Minimally Invasive robotic Non-Destructive Evaluation and Rehabilitation for bridge decks. <br/><br/>The project develops new robotic system, algorithms and control schemes for sustainable civil infrastructure, including (1) development of a novel robotics-assisted in-traffic system for highly-efficient, safety-guaranteed bridge deck inspection and diagnosis; (2) development of a new robotic rehabilitation system for delivering fast, cost-effective, minimally invasive repair and maintenance for bridge decks; (3) a human-in-the-loop robotic coordination for minimally traffic interference; and (4) experimental platforms and field performance evaluation for the proposed system. The project also provides insights to advance real time scene understanding, multiple robot coordination, and human-robot collaboration in complex robotic systems. If successful, this project can drastically reduce bridge maintenance cost and mitigate negative impact caused by closing the bridge traffic flow under traditional bridge inspection and repair. The project outcomes, including source code, datasets, and publications, are to be shared among research community and the general public. The project also includes a number of integrated research and education programs to attract students from underrepresented groups into engineering and involve students into robotics research."
"1426961","NRI: Autonomous Synthesis of Haptic Languages","IIS","National Robotics Initiative","08/01/2014","08/04/2014","Todd Murphey","IL","Northwestern University","Standard Grant","Jeffrey Trinkle","07/31/2017","$585,217.00","J. Edward Colgate","t-murphey@northwestern.edu","1801 Maple Ave.","Evanston","IL","602013149","8474913003","CSE","8013","8086","$0.00","This project develops algorithms that enable a robot to physically explore its environment using touch and to construct a language that it can use to describe that environment. The steps include exploring an environment while actively seeking information and then detecting potential elements of a language to describe what was touched. A secondary phase involves taking the set of language elements and compressing the language itself so that sensing, storage, and communication are all more efficient and more robust. The work will use a robot equipped with a robotic arm, hand, and fingertip sensors to describe objects and surfaces it encounters, all without any information about the objects provided beforehand. The importance of the work stems from the need for robots to operate in environments where touch is the only reliable sensory source. For instance, underwater applications often have limited visibility and dexterous manipulation can suffer from visual occlusion due to the hand itself. This research will enable robots to be more responsive to touch and more reliable in vision-impoverished environments.<br/><br/>A key technical tool used in this work is ergodic control, a computational technique that finds exploration strategies matching desired statistics. Symbol detection involves finding definitions of dynamic sensor evolution that minimize measures of variability. Language minimization depends on computing the entropy of a language, and finding the minimal language that has the same level of expressiveness. These three mathematical and algorithmic components need to be used in parallel during language creation, and they each have to respect physical limitations on the part of the robot (e.g., computational limitations and physical limitations). Software will be shared through the Robot Operating System (ROS) and TREP (physical simulation and optimal control software)."
"1426968","NRI: Collaborative Research: Human-Supervised Perception and Grasping in Clutter","IIS","COLLABORATIVE RESEARCH, IIS SPECIAL PROJECTS, National Robotics Initiative","08/15/2014","07/05/2016","Holly Yanco","MA","University of Massachusetts Lowell","Standard Grant","Ephraim P. Glinert","01/31/2018","$561,607.00","","holly@cs.uml.edu","600 Suffolk Street","Lowell","MA","018543643","9789344723","CSE","7298, 7484, 8013","5948, 8086, 9251","$0.00","One of the basic building blocks in semi-autonomous manipulation is the ability for a robot to grasp an object that a human operator indicates. There are many tasks where the natural way for a human and robot to work together is for the human to point out the approximate locations of objects to be grasped and for the robot to generate the precise motions necessary to achieve the grasp. This core ""auto-grasp"" functionality is critical to providing assistive manipulation for the disabled and elderly, as well as for a variety of military, police, space, or underwater applications. But implementing auto-grasp capability can be challenging in situations where the environment is cluttered, or when it is difficult to determine the grasp intention of the human. In this collaborative project that combines expertise from two institutions, the PIs will tackle situations where it is necessary for the robot to actively explore or ""interrogate"" the environment in order to figure out what the human intends to grasp and how the robot should do it. To these ends, the PIs will investigate a modified approach to planning under uncertainty known as belief space planning. Belief space planning is well-suited to active localization for grasping, because it is a single framework in which the algorithm can reason about perception-oriented and goal-orientation parts of the task. The PIs will use belief space planning to localize graspable geometries in the environment, known as grasp affordances, in a region indicated by the user. They will also explore different ways in which a human can interact with the system in order to control the grasping. The application focus of the work will be in assistive manipulation, where a person who is elderly or disabled operates an assistive robot arm mounted on an electric wheelchair or scooter. User studies will determine the best methods for the target population to operate the system. The project will contribute to the opportunities available for undergraduates and high school students in the PIs' institutions, and it will also be integrated as appropriate into the curricula of the courses they teach.<br/><br/>This research contains two key innovations that the PIs expect will make robot grasping more robust. The first is to incorporate ideas from belief space planning into the reach and grasp planning process. Because belief space planning can reason about how the robot's own ""state of information"" is expected to change in the future, it is capable of producing plans that acquire task-relevant information in the course of performing a task. The second innovation is a new approach to perception-for-grasping that localizes grasp affordance geometries in the neighborhoods of objects of potential interest. Not only is this grasp affordance approach helpful to the belief space planner, but the PIs' preliminary work indicates that this approach can be accurate and very fast (10Hz). Finally, the connection between the user interface and uncertainty in the location of the grasp target will also be explored, the plan being to model human behavior as an uncertain system where hidden variables describe user intention."
"1427260","NRI: Collaborative Research: Human-Centered Modeling and Control of Cooperative Manipulation with Bimanual Robots","IIS","National Robotics Initiative","08/01/2014","08/10/2014","Ruzena Bajcsy","CA","University of California-Berkeley","Standard Grant","Reid Simmons","07/31/2017","$672,719.00","","bajcsy@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","8013","8086","$0.00","This proposal addresses modeling and control aspects of human-robot interaction by considering constraints imposed by an individual's physiology. The project is motivated by increasing demand for automation in unstructured environments that require high-level cognitive processing and complex decision-making which cannot yet be fully automated. By taking human-centric approach, data-driven musculoskeletal models are incorporated into the robot interaction model to account for differences of individuals. <br/><br/>Each cooperative activity is divided into action primitives requiring different control strategies while estimating human intent from various sensors. The framework is based on theory of hybrid systems that provides provable safety and stability criteria. The outcome of this research will facilitate methodology for safer and more reliable human-robot interaction and advance state-of-the-art in human movement analysis and control theory. The broader impacts of this research will be realized through new insights into understanding of human intent and haptic cooperation applicable to general human-machine interaction. With increasing interest in service robotics safe and reliable interaction will be the key to successful introduction of robots in human-occupied environments. The potential economic impact of robots engaged in services and manufacturing alongside humans are significant due to increased productivity and reduced costs. Another emerging area is rehabilitation and assistive robotics. The developed data-driven musculoskeletal models will also be applicable to quantification of physical impairments and estimation of muscular stress in healthcare and ergonomics. This interdisciplinary research provides excellent opportunities for undergraduate and graduate students to be engaged in analytical challenges, laboratory demonstrations of theoretical results, and experimental evaluations."
"1409549","RI: Medium: Collaborative Research: Experience-Based Planning: A Framework for Lifelong Planning","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","08/01/2014","06/20/2016","Maxim Likhachev","PA","Carnegie-Mellon University","Standard Grant","Hector Munoz-Avila","07/31/2017","$363,995.00","","maxim@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","7495, 8013","7495, 7924, 9251","$0.00","Robots need to improve their behavior over time, yet produce consistent behavior in order to allow humans to predict their actions, which is necessary to develop trust in their behavior or even cooperate with them. Furthermore, many tasks repeat, such as opening drawers. This project develops technology that addresses these issues by viewing planning as a lifelong process and exploiting the structure of human environments for efficiency, for example that drawers typically open in similar ways.<br/><br/>This research collaboration is developing a framework for lifelong planning based on experience graphs that aims to improve performance of planning over time by exploiting past experiences when solving similar planning tasks. The concept is novel because experiences are used to guide the heuristic search as opposed to be used for mere replay or adaptation. The idea that makes this possible is a novel heuristic search-based framework that can take advantage of prior experiences and still provide rigorous guarantees on completeness and path quality. The team studies how experiences can be utilized effectively during planning, how planning should gather experiences, how it should prune redundant experiences and how it can obtain experiences from demonstrations. Applications include everyday household tasks and low-volume manufacturing tasks. The software developed in this collaborative research is being integrated into the SBPL library, one of the core libraries in ROS. The project also incorporates educational activities as well as activities that help to bridge the research communities in robotics and artificial intelligence, two separate communities despite their common interest in autonomous systems."
"1427329","NRI: Dexterous Manipulation Attained Using Task-Specific Admittance Realized with Variable Impedance Actuation","IIS","National Robotics Initiative","08/01/2014","07/21/2014","Joseph Schimmels","WI","Marquette University","Standard Grant","Jeffrey Trinkle","07/31/2017","$749,981.00","Shuguang Huang","j.schimmels@marquette.edu","P.O. Box 1881","Milwaukee","WI","532011881","4142887200","CSE","8013","8086","$0.00","This project establishes a sound engineering framework for designing next-generation robots and intelligent robotic devices that can be used to perform difficult manipulation tasks quickly and reliably. The framework is useful both for traditional robots already installed in manufacturing plants and future ""soft robots"" that will make human-robot collaboration safe. The focus is on constrained manipulation problems such as assembly and repair tasks, many of which are dull, difficult, and dangerous. The results will allow robots to be more effectively used in a wide range of domains such as service, healthcare, space exploration, and manufacturing.<br/><br/>The project yields a means to simultaneously control both a robot's position and its elastic behavior using kinematically redundant arms driven by actuators with controllable stiffness. It will be demonstrated that proper design of the robot path plan and admittance plan will ensure that constraints imposed by manipulation tasks are accommodated and high-level task objectives are reliably satisfied despite uncertainies (e.g., the door is opened, the assembly is completed properly). The four planned major outcomes of this research program are: 1) general procedures for selecting the optimal manipulation plan (identifying the optimal motion and admittance plans) using numerical procedures to ensure that the specified manipulation task progresses quickly, appropriately, and reliably despite uncertainties, 2) procedures for realizing a specified admittance plan involving the use of a redundant serial manipulator with variable impedance actuation (a series elastic actuator with controllable stiffness) at each joint, 3) a demonstration of improved robot dexterity in constrained manipulation tasks using redundant serial manipulators with variable impedance actuation to verify the manipulation plan selection methods and validate the overall manipulation approach, and 4) a library of constrained generic task categories with associated optimal manipulation plans."
"1208385","NRI-Small: Collaborative Research: A Design Methodology for Multi-fingered Robotic Hands with Second-order Kinematic Constraints","IIS","National Robotics Initiative, EXP PROG TO STIM COMP RES, IIS SPECIAL PROJECTS, COLLABORATIVE RESEARCH","09/01/2012","05/28/2015","Alba Perez Gracia","ID","Idaho State University","Standard Grant","Reid Simmons","08/31/2017","$493,364.00","","perealba@isu.edu","921 South 8th Avenue, Stop 8046","Pocatello","ID","832090002","2082822592","CSE","8013, 9150, 7484, 7298","7923, 8086, 9150, 5952","$0.00","This project, developing a systematic methodology for the design of multi-fingered robotic hands and grasping devices for a desired kinematic task, represents a novel formalization of the kinematic synthesis of articulated systems as a tree structure. The kinematic task is to be defined as positions and higher motion derivatives of the fingers, with accelerations related to the contact geometry at the fingertips for grasping actions. This research team aims to develop multi-fingered grasping devices for human-robot and anthropomorphic tasks, however the method will be a general tool for the design of any kind of multiple-finger grasping device.<br/><br/>This research has a number of broader impacts affecting both the academic community and society at large. First, the project will directly result in a design tool for multi-fingered robotic hands to enable the automatic transformation from task specifications to design alternatives ? an important development in its own right. This design tool will increase the ability of industry to design high performance, cost-effective multi-fingered robotic hands and other end effectors. This directly impacts manufacturing by speeding the development of end-of-arm tooling, with secondary benefits to the cost and quality of the final product. This will assist the U.S. to maintain its leadership and encourage the creation of high-quality jobs. The proposed curriculum additions resulting from this project will produce competent engineers for industry with a greater ability of approaching and solving design problems."
"1527432","NRI: Robust Stochastic Control for Agile Aerial Manipulation","IIS","National Robotics Initiative","09/01/2015","08/05/2015","Marin Kobilarov","MD","Johns Hopkins University","Standard Grant","Jeffrey Trinkle","08/31/2018","$496,093.00","","marin@jhu.edu","3400 N CHARLES ST","Baltimore","MD","212182608","4105168668","CSE","8013","8086","$0.00","A new class of flying robots are beginning to, not only navigate and observe, their surroundings, but also reach and manipulate objects in places that are difficult for humans to go. Such systems will assist people through manipulation in unsafe or remote locations, and will automate manual labor-intensive tasks such as package delivery, agricultural inspection, and infrastructure repair. Current aerial manipulator prototypes lack the control fidelity to ensure reliability and efficiency that is expected from such operations. To overcome these limitations, the proposed project develops novel control techniques that exploit the capabilities of the aerial vehicle. If successful, this research project will enable agile and safe aerial manipulation in extreme environments that is presently impossible or infeasible using standard methods. <br/><br/>The goal of this research is the realization of planning and control methods with built-in robustness for robots that can interact with and manipulate the environment in autonomous and human-assisted modes. This is accomplished by posing the coupled perception-control problem as a statistical learning problem and adaptively computing decision policies to optimize future performance and minimize probability of safety violation. At the core of the approach lies a provably-stable adaptive control methodology equipped with probabilistic robustness guarantees in terms of maximum expected cost and probability of collision. These bounds correspond to concentration-of-measure inequalities derived through Bayesian probably-approximately-correct analysis. Two experimental platforms provide proof-of-concept for: 1) an autonomous ""Air-gripper"" for repetitive tasks such as load delivery, crop sampling, and remote cleaning; 2) co-robotic ""hands in the sky"" in direct assistance to a human operator enabling access to dangerous or difficult-to-access places, e.g. for inspection and repair in extreme environments, during rescue or security-sensitive missions. The implemented techniques are generally applicable and will be released as open-source ROS-compatible software."
"1554961","EAGER: Reflection and Diffraction Sound Signals for Non-Field-of-View Target Estimation","IIS","National Robotics Initiative","09/01/2015","09/01/2015","Tomonari Furukawa","VA","Virginia Polytechnic Institute and State University","Standard Grant","Jie Yang","08/31/2017","$165,252.00","","tomonari@vt.edu","Sponsored Programs 0170","BLACKSBURG","VA","240610001","5402315281","CSE","8013","7916, 8086","$0.00","This project studies the diffraction and reflection signals in sound source localization. The study could enable the capability of tracking and localizing human partners outside of the robots' Field-Of-View (FOV) to the co-robots for human-robot interaction. The project estimates an object outside of FOV using auditory sensors and enhances perception capabilities of robots. The project integrates research and education by training graduate and undergraduate students and outreach local K-12 students.<br/><br/>This research proves the early-stage concept of the project that estimates the location of a Non-FOV (NFOV) target by learning from humans and utilizing the physics of sound wave propagation associated with the NFOV target. The research is to introduce the capability of localizing human partners outside of the robots' FOV to the co-robots for Human-Robot Interaction. The research team approaches the problem by deterministically formulating first-arrival diffraction and reflection signals and identifying signal directions. The project develops an approach that auditorily estimates the location of an NFOV target by learning from humans and utilizing the physics of sound wave propagation associated with the NFOV target. The project further evaluates the approach in unknown indoor environments by using both visual and auditory sensors."
"1527202","NRI: Novel Prosthetic Arm Control Based on a Low-Dimensional Internal Musculoskeletal Biomechanical (LIMB) Model","IIS","Gen & Age Rel Disabilities Eng, National Robotics Initiative","08/01/2015","07/05/2016","He Huang","NC","North Carolina State University","Standard Grant","Alexander Leonessa","07/31/2018","$889,387.00","Jonathan Stallings","hhuang11@ncsu.edu","CAMPUS BOX 7514","RALEIGH","NC","276957514","9195152444","CSE","5342, 8013","010E, 8086, 9251","$0.00","Upper limb amputation is a major cause of disability for nearly 160,000 Americans, many of whom could benefit from emergent sophisticated robotic, multifunctional prosthetic arms/hands. In these advanced prostheses, movements are typically controlled by interpreting the user's electromyographic (EMG) signals from residual or reinnervated muscles. State-of-the-art pattern recognition (PR) has been the most promising EMG control interface for multifunctional artificial arms. However, EMG PR-based control algorithms often require lengthy and frequent algorithm training and lack reliability when the external loading or arm posture changes. This is partly because EMG PR is data-driven and does not account for the behavior of the underlying neural or biomechanical system from which the EMG signals are sourced. The objective of this project is to develop a novel EMG control of multifunctional transradial (TR) prostheses based on a systematic study of neuromuscular control and biomechanical roles of residual muscles in TR amputees. This research can potentially enhance the health, function, and quality of life of upper limb amputees. This project's concept, methods, and frameworks for enhancing EMG-based prosthesis control may be extended to other assistive robotics to benefit other patient populations such as stroke survivors. This project will impact STEM education by promoting project-based cross-training among K-12, undergraduate, and graduate students in underrepresented groups including females, minorities, and students with disabilities. The research may also impact the neuroscience and movement science communities by elucidating the control mechanism of the arm/hand and unveiling new knowledge of neuroplasticity and the internal model in upper limb amputees. <br/><br/>At the core of the multifunctional prosthesis control is a musculoskeletal model of the missing limb that will be used to interpret intended joint motions from EMG signals. The intellectual merit of this project includes a new concept for the control of robotic, multifunctional prosthetic arms/hands. The PIs' musculoskeletal model-based interface is fundamentally different from existing data-driven, EMG PR-based control because it interprets EMG signals and decodes user movement intent in a more biological way. Additionally, this effort will result in new knowledge regarding the neuroplasticity, neuromuscular control, and perceived biomechanical roles of residual muscles in upper limb amputees, which has not been systematically investigated based on the investigators' knowledge. Ultimately, the project will result in a new prosthesis control that, compared to state-of-the-art EMG PR-based control, may require significantly fewer and shorter calibrations, provide more intuitive, robust control (against posture changes, external loading, etc.), and enable multi-joint coordinated prosthesis operations. The investigators expect that the research may transform the way in which upper limb amputees operate multifunctional prostheses in daily life."
"1527183","NRI: Collaborative Research: Targeted Observation of Severe Local Storms Using Aerial Robots","IIS","National Robotics Initiative","01/01/2016","08/13/2015","Christopher Weiss","TX","Texas Tech University","Standard Grant","Ralph Wachter","12/31/2018","$346,246.00","","chris.weiss@ttu.edu","349 Administration Bldg","Lubbock","TX","794091035","8067423884","CSE","8013","8086","$0.00","This project addresses the development of self-deploying aerial robotic systems that will enable new in-situ atmospheric science applications. Fixed-wing aerial robotic technology has advanced to the point where platforms fly persistent sampling missions far from remote operators. Likewise, complex atmospheric phenomena can be simulated in near real-time with increasing levels of fidelity. Furthermore, cloud computing technology enables distributed computation on large, dynamic data sets. Combining autonomous airborne sensors with environmental models dispersed over multiple communication and computation channels enables the collection of information essential for examining the fundamental behavior of atmospheric phenomena. The aerial robotic system proposed here will close significant capability gaps in conventional platform?s abilities to collect the data necessary to answer a wide range of scientific questions. The motivating application for this work is improvement in the accuracy and lead-time of tornado warnings.<br/><br/>The proposed project draws on techniques in the areas of robotics, unmanned systems, networked control, wireless communication, active sensing, and atmospheric science to realize the vision of bringing cloud robotics to the clouds. The autonomous self-deploying aerial robotic systems is comprised of multiple robotic sensors and distributed computing nodes including: multiple fixed-wing unmanned aircraft, deployable Lagrangian drifters, mobile Doppler radar, mobile command and control stations, distributed computation nodes in the field and in the lab, a net-centric middleware connecting the dispersed elements, and an autonomous decision-making architecture that closes the loop between sensing in the field and new online numerical weather prediction tools."
"1542307","2015 International Workshop on Robotics and Interactive Technologies For Neuroscience and Rehabilitation","CBET","Gen & Age Rel Disabilities Eng, PERCEPTION, ACTION & COGNITION, COLLABORATIVE RESEARCH, ROBUST INTELLIGENCE, National Robotics Initiative","05/15/2015","05/13/2015","Ferdinando Mussa-Ivaldi","IL","Rehabilitation Institute of Chicago","Standard Grant","Alexander Leonessa","04/30/2017","$44,529.00","","sandro@northwestern.edu","345 East Superior Street","Chicago","IL","606112654","3122384534","ENG","5342, 7252, 7298, 7495, 8013","004E, 010E, 137E, 5920, 7495, 7556, 8086, 8089","$0.00","1542301<br/>Mussa-Ivaldi, Ferdinando A.<br/><br/>Since its inception, Robotics has had a profound impact on our understanding of how the brain constructs movements, processes sensory information and adapts to variable environments. In more recent times it has entered the clinical domain of sensory-motor rehabilitation for survivors of stroke and other disabling neurological conditions. <br/><br/>The proposed workshop is intended to organize a vigorous debate between experts in robotics, in rehabilitation research and in the basic neuroscience of the sensory-motor system with the dual purpose of enhancing the integration of these disciplines and developing new international collaborations between centers of excellence for Neurotechnologies and Rehabilitation in the United States and Italy. <br/>Therefore, the general goal of the workshop will be to promote the progress of science in the critical area where robotics and neuroscience are going to meet and interact, with a specific emphasis on state of the art advances in the national health, in relation with the increasing percentage of senior citizens in all industrialized countries of the world. Moreover, the project will advance the rehabilitation field strengthening the leadership of US research.<br/><br/>The debate on the hot issues of Advanced Robotics and Robotic Neurorehabilitation will be organized by scientists from two institutions, namely the Rehabilitation Institute of Chicago (RIC) - the leading US hospital for rehabilitation research - and the Italian Institute of Technology (IIT) - the leading Italian research center for a broad spectrum of advanced technologies, including neurorehabilitation, robotics and nanotechnology. The interaction between these two research centers is intended to be a seed for developing a broader collaborative network of US and Italian researchers in neurorehabilitation science and technology. The significance of this initiative stems from its potential to enhance and maintain the close contiguity of insights from basic neuroscience with the ingenuity of rapid technological developments in the clinical domain. It is expected that this tight interaction will serve to promote the recovery of lost abilities and independence in the disabled population, while expanding the intellectual and educational domains for current and future generations of researchers in neural engineering.<br/><br/>The three-days workshop will include four discussion sessions, on the following themes:<br/><br/>1. Sensory-motor control and haptic interactions.<br/>2. Learning and motor cognition.<br/>3. Human/machine interactions.<br/>4. Neuroprosthetics and rehabilitation<br/><br/>Each theme will be introduced by brief presentations from IIT and RIC experts, followed by more detailed demonstrations and descriptions of key scientific and technological issues. A group of panelists will guide the discussion, inclusive of all participants, during and at the end of the meeting, by pointing at specific unresolved problems and future challenges. Participation of junior scientists will be particularly encouraged both during the meeting and through the preparation of articles that will be collected in the submission for a special issue of Frontiers Research Topics. On the third day, principal investigators from both institutions will visit the IIT laboratories. There, a discussion of the perspective collaborations that have emerged in the previous two days will conclude the workshop."
"1227495","NRI-Large: Collaborative Research: Purposeful Prediction: Co-robot Interaction via Understanding Intent and Goals","IIS","National Robotics Initiative","10/01/2012","06/04/2015","James Bagnell","PA","Carnegie-Mellon University","Continuing grant","Ephraim P. Glinert","09/30/2017","$2,175,468.00","Anind Dey, Martial Hebert","dbagnell@ri.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","7367, 7925, 8086, 9251","$0.00","In order for robots to collaborate with humans, they need to be able to accurately forecast human intent and action. People act with purpose: that is, they make sequences of decisions to achieve long-term objectives. For instance, in driving from home to a store, people carefully plan a sequence of roads that will get them there efficiently. In predicting a person's next decision, algorithms must be developed that reflect these purposeful actions. <br/><br/>Currently, robots are unable to anticipate human needs and goals, and this represents a fundamental barrier to their large-scale deployment in the home and workplace. The aim of this project is to develop a new science of purposeful prediction that can be applied to human-robot interaction across a wide variety of domains. The work draws on recent techniques based on Inverse Optimal Control and Inverse Equilibria Theory that enable statistically sound reasoning about observed deliberate behavior. These new methods provide the foundations of a theoretical framework that integrates traditional decision making techniques like optimal control, search and planning with probabilistic methods that reason about uncertainty and hidden information, particularly about goals, utility and intent. <br/><br/>Intellectual merit: The project will provide a general framework that allows robots to anticipate and adapt to the activities of their human co-workers based on perceptual cues. The investigators will develop the theory, a computational toolbox, and, in collaboration with industrial partners, prototype deployments of these new methods for the prediction of peoples' behavior in a diverse set of robotics domains from computer vision to motor control. The project is transformative in that it combines a novel theoretical/algorithmic framework with extensive support in terms of volume of data and validation infrastructure in the context of many applications. <br/><br/>Broader impacts: A revolution in personal robotics in both the home and workplace depends on the ability to forecast human activities and intents; small- and medium- scale manufacturing will make a leap forward through agile robotic systems intelligent enough to understand and assist their co-workers in flexible assembly tasks; and robust models of pedestrian and vehicular traffic flow will enable more effective driver warning systems and safer autonomous mobile robots. Purposeful prediction technology is an important step towards enabling such understanding of actions and intents in these arenas. The research work will involve the training and mentoring of undergraduate, masters and doctoral students as well as post-doctoral fellows in this emerging multi-disciplinary research area at the intersection of computer and cognitive sciences and robotics."
"1551312","EAGER: Functional Imitation of Observed Tasks by Co-Robots","IIS","National Robotics Initiative","09/01/2015","08/11/2015","Manfred Huber","TX","University of Texas at Arlington","Standard Grant","Jeffrey Trinkle","08/31/2017","$139,968.00","Gergely Zaruba, Farhad Kamangar","huber@cse.uta.edu","1 UNIVERSITY OF TEXAS AT","Arlington","TX","760190145","8172722105","CSE","8013","7916, 8086","$0.00","Assistive and service robots have made significant strides and have the potential to be transformative in many fields, including health care, aging, disability management, and work in dangerous environments. For this, however, it is important that these robots can be ""programmed"" and used by largely untrained users, including caregivers or elderly persons. This project aims to develop a novel approach to allow robots of widely varying designs to perform assistive and supportive tasks by observing demonstrations performed by a human. Rather than copying movement, which would require that the robot resembles the human, the proposed approach uses these demonstrations to ""infer"" the important aspects of the task and translate them into a strategy that can be executed by the robot and in varying situations and settings.<br/><br/>The proposed approach treats imitation not as copying of observed movements but rather as learning to replicate the function of the demonstration. For this, it transforms observations into a hierarchical Markov task model using learned models of observed environmental dynamics. This probabilistic task model is then mapped onto a hierarchical Semi-Markov Decision model of the robot's behavioral capabilities using an adaptive similarity function that represents the correspondence between attributes in the two models as well as the importance of particular attributes for successful task performance. The cost function is adapted during imitation using Reinforcement Learning and qualitative feedback from the user, allowing the system to improve and personalize its imitation capabilities. This project develops a proof-of-concept system and evaluates it on a wheeled mobile manipulator in the context of common household tasks."
"1208500","NRI-Small: Spacial Primitives for Enabling Situated Human-Robot Interaction","IIS","National Robotics Initiative","08/01/2012","06/30/2016","Maja Mataric","CA","University of Southern California","Standard Grant","William Bainbridge","07/31/2017","$758,000.00","Clifford Nass","mataric@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","CSE","8013","7923, 8086, 9251","$0.00","To enable natural and productive human-robot interaction (HRI), a co-robot must both understand and control ""proxemics"" -- the social use of space -- in order to communicate in ways commonly used and understood by humans. This project focuses on answering the question: How do social (speech and gesture), environmental (loud noises and low lighting), and personal (hearing and visual impairments) factors influence positioning and communication between humans and co-robots, and how should a co-robot adjust its social behaviors to maximize human perception of its social signals?<br/><br/>The project will develop principled computational models for the recognition and control of proxemic co-robot behavior in HRI using both telepresence and autonomous co-robots. The research will establish a foundational component of HRI for co-robotics, with specific impact on special needs users in socially assistive contexts -- particularly the elderly, both aging in place and in institutions -- with the goal of mitigating isolation and depression, and encouraging exercise and socialization. <br/><br/>Broader impacts: The work will inform robot design and control, and provide software and a corpus of public HRI data for use by researchers worldwide. Beyond robotics, the project promises to inform, validate, and extend longstanding research in the social sciences. This project also includes a strong public and K-12 outreach component consisting of weaving the HRI themes being developed into annual regional and international outreach events. The events feature large-scale open houses and educational workshops with interactive demonstrations and hands-on activities that highlight human factors in computational systems as an effective means of increasing interest in STEM-related activities."
"1316271","NRI: Small: Interleaved Continuum-Rigid Manipulation - Enabling High-Performance and Inherent-Safety in Minimally-Invasive Surgical Procedures","IIS","National Robotics Initiative","09/01/2013","08/13/2013","Michael Zinn","WI","University of Wisconsin-Madison","Standard Grant","Jeffrey Trinkle","08/31/2017","$495,154.00","","mzinn@wisc.edu","21 North Park Street","MADISON","WI","537151218","6082623822","CSE","8013","7923, 8086","$0.00","This project investigates a fundamentally new robotic manipulation approach for minimally-invasive surgical procedures. The approach combines flexible, actively actuated continuum segments with small, rigid-link joints -- retaining the safety advantages of flexible continuum manipulators while achieving the performance advantages of traditional rigid manipulators. The combination of performance and inherent safety is an essential element in the emerging area of co-robotic surgical applications. The overall approach is referred to as interleaved continuum-rigid manipulation. While interleaved continuum-rigid manipulation offers an attractive, intuitive approach, its apparent simplicity belies the obstacles which must be overcome prior to successful realization. This project will address these obstacles though a coordinated effort in design, modeling, and control. The design effort will seek to address the significant technical challenges of the interleaved manipulation approach including rigid-link joint actuation challenges and rigid-link joint to flexible segment drive-train coupling. The modeling investigation will establish a toolset from which the underlying behaviors of the interleaved manipulation approach can be understood. The controls investigation will seek to develop control strategies appropriate for the hybrid flexible-rigid system envisioned. Evaluation will include performance and safety-metric evaluation as well as bench top clinical evaluation, focusing on the execution of simulated clinical tasks.<br/><br/>If successful, the results of the research will lead to improvements in manipulation capability for use in highly technical and safety critical minimally-invasive co-robotic surgical procedures. Specifically, interleaved continuum-rigid manipulation will enable levels of performance required for cooperative manipulation tasks in emerging co-robotic surgical techniques while maintaining safety. New classes of interventional techniques for neurological, cardiac, and other high risk procedures will be possible, resulting in improved outcomes and reduced morbidity. In addition, the extension to larger scale manipulators could have significant impact in other co-robotic application areas including search and rescue robotics, light-manufacturing, and home and healthcare assistive robotics. To facilitate wide dissemination of the project results, major findings will be published in both conference and peer-reviewed journals. Additionally, detailed results, including design and analysis data, will be made available online. Finally, the project will have a strong training focus by providing an integrated research and educational environment for graduate and undergraduate student researchers - including those in underrepresented groups through established University fellowship programs."
"1341984","Travel Support for Mobile Micro-Robotics Challenge at IEEE ICRA 2013","IIS","National Robotics Initiative","06/01/2013","05/24/2013","Dan Popa","TX","University of Texas at Arlington","Standard Grant","Gregory Chirikjian","08/31/2014","$10,000.00","","dan.popa@louisville.edu","1 UNIVERSITY OF TEXAS AT","Arlington","TX","760190145","8172722105","CSE","8013","7495, 7556","$0.00","This project proposes to provide competitive travel support for student teams wishing to compete in the Mobile Micro-Robotics Challenge. The challenge involves advances in the design and fabrication of microelectromechanical systems (MEMS) enabling the development of mobile micro-robots that can autonomously navigate and manipulate in controlled environments. It is expected that this technology will be critical in applications as varied as intelligent sensor networks, in vivo medical diagnosis and treatment, and adaptive microelectronics. However, many challenges remain, particularly with respect to locomotion, power storage, embedded intelligence, and motion measurement, therefore, it is valuable to help enable participation by defraying travel costs. <br/><br/>The broader impacts of this grant include the ability of research teams to showcase working demonstrations that support the challenge themes of micro-scale actuation, manipulation and locomotion. Travel assistance will increase the exposure of US student teams to international competitors, likely increasing the knowledge and competitiveness of the US-based teams since this event has traditionally been dominated by teams from Europe. The multi-day competitions will be held during the IEEE ICRA conference providing an excellent opportunity to engage a broad technical audience. The competitions are open to the general public to raise awareness of the state-of-the-art. The competitions provide an opportunity for students and leaders to learn how micro-robotics can play an important role in society."
"1317803","NRI: Small: Virtualized Robot Test and Integration Laboratory","IIS","National Robotics Initiative, ","09/15/2013","03/08/2016","Alonzo Kelly","PA","Carnegie-Mellon University","Standard Grant","Jeffrey Trinkle","08/31/2017","$1,093,640.00","","alonzo@ri.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013, P154","7923, 8086","$0.00","The Virtual Robot Test and Integration Laboratory (VRTIL) addresses the need to verify robot software using real data, produced by real sensors, in all its richness and imperfection. The project is embedding robot software in a synthetic environment in order to mimic its sensor interfaces. In an advance over the state of the art, these interfaces are windows into data bases that store the actual sensor data that a robot did read when it was in a particular state.<br/>The project goal is to reproduce the real world interface as exactly as possible using data. Two basic tools enable such ?virtualization? of a real robot in a real environment:<br/><br/>* Virtualized reality provides a means to produce synthetic views from virtual viewpoints that are near to an actual viewpoint for which a real image is available;<br/><br/>* High fidelity synthesized motion gathers large amounts of data to calibrate a model that is valid across all of state space.<br/><br/>Combining these ideas will result in a software test environment for robot software that has the realism of data logs and the responsiveness of a simulator. This may fundamentally transform the development of robots by lowering the cost and time of testing and debugging."
"1317813","NRI: Small: Online Training of Hierarchical Multirobot Teams","IIS","National Robotics Initiative","09/01/2013","05/10/2016","Sean Luke","VA","George Mason University","Standard Grant","Reid Simmons","08/31/2016","$515,998.00","","sean@cs.gmu.edu","4400 UNIVERSITY DR","FAIRFAX","VA","220304422","7039932295","CSE","8013","7923, 8086, 9251","$0.00","This project involves a new approach to robot learning from demonstration which allows experimenters to teach multiple robots nontrivial, heterogenous collective behaviors in real-time. Training multiple robots is challenging because while the experimenter may know what emergent collective behavior he wishes to achieve, it is unlikely that he knows what individual robot behaviors, and their interactions, will achieve it. This project decomposes robot teams or swarms into a social hierarchy of leaders and subordinates. Small groups are then trained at the bottom of the hierarchy relatively simple collective behaviors, then groups of leaders are trained progressively with more complex and abstract behaviors, until all trained top-level root behaviors have been learned. The project examines homogeneous behaviors within robot groups, heterogeneous behaviors among the robots, and mixtures of the two at any level in the hierarchy. The method builds on prior work in single-robot behavior training, which used decomposed hierarchies of behaviors consisting of finite-state automata and learned transition functions. Beyond robotics, the project also allows the development of group behaviors for virtual agents found in animation and in emerging agent-based models in the social sciences and biology. The work integrates with a teaching and research robotics laboratory, with agent-based modelers in the social sciences, and with robotics education efforts in K-12 underprivileged schools."
"1316809","NRI: Small: Collaborative Research: Don't Read my Face: Tackling the Challenges of Facial Masking in Parkinson's Disease Rehabilitation through Co-Robot Mediators","IIS","National Robotics Initiative","09/15/2013","09/02/2013","Matthias Scheutz","MA","Tufts University","Standard Grant","Reid Simmons","08/31/2018","$949,924.00","Linda Tickle-Degnen","matthias.scheutz@tufts.edu","20 Professors Row","Medford","MA","021555807","6176273417","CSE","8013","7923, 8086","$0.00","The overarching scientific goal of this project is two-fold: (1) to develop a robotic architecture endowed with moral emotional control mechanisms, abstract moral reasoning, and a theory of mind that allow corobots to be sensitive to human affective and ethical demands, and (2) to develop a specific instance of the architecture for a co-robot mediator between people with ""facial masking"" due to Parkinson's disease (PD) that reduces their ability to signal emotion, pain, personality and intentions to their family caregivers, and health care providers who often misinterpret the lack of emotional expressions as disinterest and an inability to adhere to treatment regimen, resulting in stigmatization. To tackle these problems, the project brings together two roboticists with extensive prior experience in robot ethics and modeling emotions as well as implementing them in integrated autonomous robotic systems. The robotics expertise is combined with that of an expert in early PD rehabilitation and daily social life. The project will build on extensive software, hardware and data set resources, including complex robotic control architectures with ethical control mechanisms, personality and emotion models, and affect and natural language capabilities.<br/><br/>The general expected outcome of the project is an architecture for co-robots that can be adapted to a great variety of health care scenarios in an effort to enrich and dignify already stressed and stigmatized relationships between humans. The project also includes novel educational efforts such as a course in occupational therapy robotics as well as significant K?12 outreach through the Tufts Centers for STEM Diversity and for Engineering Education and Outreach, as well as various important community and public activities such as presentations on health care robotics to focus and patient groups."
"1317976","NRI: Small: Dexterous Manipulation with Underactuated Hands: Strategies, Control Primitives, and Design for Open-Source Hardware","IIS","National Robotics Initiative","09/15/2013","09/03/2013","Aaron Dollar","CT","Yale University","Standard Grant","Jeffrey Trinkle","08/31/2018","$1,197,571.00","","aaron.dollar@yale.edu","Office of Sponsored Projects","New Haven","CT","065208327","2037854689","CSE","8013","7923, 8086","$0.00","The proposed work will contribute to the fundamental understanding of dexterous manipulation, especially in how low-dimensional hands that have traditionally been used for pure grasping tasks can be leveraged for excellent manipulation capabilities. The PI team will investigate how the passive mechanics of compliant and underactuated hands can be utilized for manipulation strategies include vibrational stick-slip, equilibrium point manipulation, and finger gaiting, as well as develop low-level planning and control schemes to implement those for a wide range of manipulation movements. This work will lead to the development of general-purpose robot hands having low-dimensional actuation and sensing. Furthermore, the focus on design for open-source dissemination through rapid prototyping techniques will result in novel fabrication strategies in furtherance of fast innovation in a range of fields.<br/><br/>A key thread of the proposed work is that simple hand designs and control schemes will be made freely available through an open-source repository. The hands will be able to be easily and inexpensively fabricated with rapid prototyping techniques, lower the entry barrier from expense hand hardware and encouraging continual design improvements. Additionally, the team will kick-start a user community around those hands through an on-site tutorial at Yale for approximately 20 students and postdocs from top manipulation research groups. For the education and outreach component of this proposal, a major thrust involves enhancing RoboticsCourseWare.org, the PI's open-access repository for robotics teaching materials, in order to highlight open-source and other inexpensive hardware platforms for educational purposes. A strong focus on involving a diverse group of undergraduates and high school students in the research plan is also highlighted."
"1328268","NRI: Large: Collaborative Research: Human-robot Coordinated Manipulation and Transportation of Large Objects","IIS","National Robotics Initiative","10/01/2013","09/25/2013","Paul Oh","PA","Drexel University","Standard Grant","Jie Yang","09/30/2014","$299,998.00","","paul@coe.drexel.edu","1505 Race St, 8th Floor","Philadelphia","PA","191021119","2158955849","CSE","8013","7298, 7925, 8086","$0.00","Motivated by the complementary abilities of humans and humanoids, the objective of this proposal is to develop the science and technology necessary for realizing human-robot cooperative object manipulation and transportation. The key concepts that this research seeks to promote are adaptability to human activity under minimal communication, and robustness to variability and uncertainty in the environment, achieved through a layered representation and deliberate processing of the available information. Moreover, this project aims to make maximum use of a minimal set of sensors to plan and control the actions of the robot, while ensuring safe and efficient cooperative transportation. The embodiment of this research is a humanoid co-worker that bears most of the load, when helping a person to carry an object, without requiring excessive communication, or prior training on the part of the human.<br/><br/>By introducing concrete methods for human-robot physical collaboration in semi-structured environments, this project enables a unique synergy between robots and humans that has the potential to increase productivity, and reduce accidents and injuries. In doing so, it also promotes the advancement of new practical applications of robots in construction, manufacturing, logistics, and home services. By developing open-source, portable algorithms for humanoid robots and mobile manipulators, this effort results in cost and time savings for researchers, developers, educators, and end-users in robotics. Finally, through an aggressive educational and community outreach plan, and by actively engaging K-12 students in an exciting RoboTech Fellows program, this project seeks to increase diversity and attract underrepresented groups to STEM."
"1328722","NRI: Large: Collaborative Research: Human-robot Coordinated Manipulation and Transportation of Large Objects","IIS","National Robotics Initiative","10/01/2013","07/07/2015","Stergios Roumeliotis","MN","University of Minnesota-Twin Cities","Continuing grant","Jie Yang","09/30/2018","$1,499,987.00","Demoz Gebre-Egziabher","stergios@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","8013","7298, 7925, 8086","$0.00","Motivated by the complementary abilities of humans and humanoids, the objective of this proposal is to develop the science and technology necessary for realizing human-robot cooperative object manipulation and transportation. The key concepts that this research seeks to promote are adaptability to human activity under minimal communication, and robustness to variability and uncertainty in the environment, achieved through a layered representation and deliberate processing of the available information. Moreover, this project aims to make maximum use of a minimal set of sensors to plan and control the actions of the robot, while ensuring safe and efficient cooperative transportation. The embodiment of this research is a humanoid co-worker that bears most of the load, when helping a person to carry an object, without requiring excessive communication, or prior training on the part of the human.<br/><br/>By introducing concrete methods for human-robot physical collaboration in semi-structured environments, this project enables a unique synergy between robots and humans that has the potential to increase productivity, and reduce accidents and injuries. In doing so, it also promotes the advancement of new practical applications of robots in construction, manufacturing, logistics, and home services. By developing open-source, portable algorithms for humanoid robots and mobile manipulators, this effort results in cost and time savings for researchers, developers, educators, and end-users in robotics. Finally, through an aggressive educational and community outreach plan, and by actively engaging K-12 students in an exciting RoboTech Fellows program, this project seeks to increase diversity and attract underrepresented groups to STEM."
"1346800","Workshop for Women in Machine Learning","IIS","INFO INTEGRATION & INFORMATICS, ROBUST INTELLIGENCE, National Robotics Initiative","09/01/2013","08/19/2013","Katherine Heller","NC","Duke University","Standard Grant","Jie Yang","09/30/2016","$40,002.00","","kheller@gmail.com","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","7364, 7495, 8013","7495, 7556","$0.00","Since 2006, the annual workshop for Women in Machine Learning (WiML) has brought together female researchers in industry and academia, postdoctoral fellows, and graduate students from the machine learning community to exchange research ideas and build mentoring and networking relationships. The one-day workshop has been especially beneficial for junior graduate students, giving them a supportive environment in which to present their research (in many cases, for the first time) and enabling them to meet peers and more senior researchers in the field of machine learning. The networking opportunities provided by the workshop have also helped senior graduate students find jobs following graduation. <br/><br/>Intellectual Merit: This workshop will advance machine learning knowledge and foster collaboration within the machine learning community. As invited speakers, established researchers at top universities and research labs will teach workshop participants about cutting-edge ideas from diverse areas of machine learning. Students will present their own research and receive valuable feedback from both senior researchers and their peers. By enabling women at all stages of their careers in machine learning to exchange research ideas and form new relationships, we expect that new connections and research collaborations will be established, thereby advancing the state-of-the-art of the field. <br/><br/>Broader Impact: This workshop will provide a forum for female graduate students, postdoctoral fellows, junior and senior faculty, and industry and government research scientists to exchange research ideas and establish networking and mentoring relationships. Undergraduates, particularly those who are interested in pursuing graduate school or industry positions in machine learning, are also welcome to attend. Bringing together women from different stages of their careers gives established researchers the opportunity to act as mentors, and enables junior women to find female role models working in the field of machine learning. The workshop will also benefit the wider machine learning community: Firstly, the WiML website, which lists all previous workshop presenters, serves as a useful resource for organizations looking for female invited speakers. Secondly, co-locating with a major machine learning conference enhances the visibility of female researchers among the wider machine learning community. Thirdly, travel funding provided to workshop participants also facilitates their travel to the co-located conference, which for some participants would otherwise not be possible. Finally, all workshop materials (slides, abstracts, etc.) will be made available on the workshop website in order to ensure broad dissemination."
"1354321","EAGER: Individualized Musculoskeletal Modeling for Diagnosis, Rehabilitation and Real-time Feedback","IIS","National Robotics Initiative","09/15/2013","09/06/2013","Ruzena Bajcsy","CA","University of California-Berkeley","Standard Grant","Gregory Chirikjian","08/31/2015","$193,786.00","","bajcsy@eecs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","8013","7916, 8086","$0.00","The goal of this project is to vastly improve the way musculosketal modeling is performed and utilized by creating a data driven model that builds on the product of exponentials formulation for joints. By using parameter fitting, a kinematic chain of joints can be fitted the individual rather than scaling a generic template which does not take into account the large diversity in body shapes and sizes. The team will look at a structured, yet data driven approach to modeling a person, making it possible to compare joint ranges and muscular limitations both contralaterally, as well as against their peers. It is also possible to compare a patient and against their past history allowing for better understanding and diagnosis within a specified patient groups (scoliosis, the elderly, hip replacement recovery) as well as with the general population. The PI proposes a hybrid optimal control approach for determining muscular activation based on segmenting different dynamical modes. These modes can take into account changes in mass or geometric constraints such as assistive devices (e.g. crutches walkers or exoskeletons). They propose to apply these methods of musculoskeletal modeling to the upper limbs in the elderly group who experiences muscle weakness, joint damage and may have artificial prostheses.<br/><br/>The tools developed under this proposal can be used to analyze any number of biological creatures by modeling their joints in a similar manner. It expands to a wider robotic community where robotic kinematic chains can be directly compared to biological chains. This can be used for teleoperation of robotic devices where particular joints can be mapped between each other. It also adds to the tools that can be used to design assistive exoskeletons and prosthetic devices, as it allows biological and mechanical joints to be modeled in together- potentially improving the methods of controlling these devices. The project team consists of a PI from Computer Science and two consultants, one from Mechanical Engineering (UC Berkeley) and a MD from UCDMC who have extensive experience in workspace assessment techniques, human modeling, human-machine interaction, and control. Research findings will also be outreached to K-12 students and their parents though various official events at the University. The Center for Information Technology Research in the Interest of Society (CITRIS) at UC Berkeley provides a unique environment and opportunity for the investigators to interact and share research findings with other researchers, students, and broader public."
"1327614","NRI: Large: Collaborative Research: Human-robot Coordinated Manipulation and Transportation of Large Objects","IIS","National Robotics Initiative","10/01/2013","09/25/2013","Ioannis Poulakakis","DE","University of Delaware","Standard Grant","Jie Yang","09/30/2018","$400,000.00","","poulakas@udel.edu","210 Hullihen Hall","Newark","DE","197162553","3028312136","CSE","8013","7298, 7925, 8086","$0.00","Motivated by the complementary abilities of humans and humanoids, the objective of this proposal is to develop the science and technology necessary for realizing human-robot cooperative object manipulation and transportation. The key concepts that this research seeks to promote are adaptability to human activity under minimal communication, and robustness to variability and uncertainty in the environment, achieved through a layered representation and deliberate processing of the available information. Moreover, this project aims to make maximum use of a minimal set of sensors to plan and control the actions of the robot, while ensuring safe and efficient cooperative transportation. The embodiment of this research is a humanoid co-worker that bears most of the load, when helping a person to carry an object, without requiring excessive communication, or prior training on the part of the human.<br/><br/>By introducing concrete methods for human-robot physical collaboration in semi-structured environments, this project enables a unique synergy between robots and humans that has the potential to increase productivity, and reduce accidents and injuries. In doing so, it also promotes the advancement of new practical applications of robots in construction, manufacturing, logistics, and home services. By developing open-source, portable algorithms for humanoid robots and mobile manipulators, this effort results in cost and time savings for researchers, developers, educators, and end-users in robotics. Finally, through an aggressive educational and community outreach plan, and by actively engaging K-12 students in an exciting RoboTech Fellows program, this project seeks to increase diversity and attract underrepresented groups to STEM."
"1317718","NRI: Small: Understanding neuromuscular adaptations in human-robot physical interaction for adaptive robot co-workers","IIS","National Robotics Initiative","09/15/2013","09/03/2013","Jun Ueda","GA","Georgia Tech Research Corporation","Standard Grant","Jeffrey Trinkle","08/31/2017","$1,199,427.00","Minoru Shinohara, Karen Feigh","jun.ueda@me.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","7923, 8086","$0.00","The goal of this award is to develop theories, methods, and tools to understand the mechanisms of neuromotor adaptation in human-robot physical interaction. Human power-assisting systems, e.g., powered lifting devices that aid human operators in manipulating heavy or bulky loads, require physical contact between the operator and machine, creating a coupled dynamic system. This coupled dynamic has been shown to introduce inherent instabilities and performance degradation due to a change in human stiffness; when instability is encountered, a human operator often attempts to control the oscillation by stiffening their arm, which leads to a stiffer system with more instability. The project will establish control algorithms for robot co-workers that proactively adjust the contact impedance between the operator and robotic manipulator for achieving higher performance and stability. This research will 1) understand the association between neuromuscular adaptations and system performance limits, 2) develop probabilistic methods to classify and predict the transition of operator's cognitive and physical states from physiological measures, and 3) integrate this knowledge into a structure of shared human-robot and demonstrate the efficacy in a powered lifting device with real-world constraints at vehicle assembly facilities. <br/><br/>If successful, the research will benefit the communities interested in the adaptive shared control approach for advanced manufacturing and process design, including automobile, aerospace, and military. Such next-generation manufacturing is expected to improve productivity and reduce assembly time as well as physical burden of assembly line workers. Research outcomes will be integrated into current courses at both graduate and undergraduate levels. Students will be recruited from interdisciplinary and multicultural groups including under-represented groups. K-12 outreach will be carried out in conjunction with Georgia Tech Student and Teacher Enhancement Partnership Program and a summer robot camp in a local non-profit association. An online portal is maintained for dissemination."
"1317749","NRI: Small: Robotic Scouts: Augmenting Human Perception for Underground Rescue","IIS","National Robotics Initiative","09/15/2013","05/09/2014","William Whittaker","PA","Carnegie-Mellon University","Standard Grant","Jeffrey Trinkle","08/31/2016","$608,000.00","Srinivasa Narasimhan","red@frc.ri.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","7923, 8086, 9251","$0.00","Robots are potential tools for life saving in underground rescue operations like mine disasters. Human rescuers are thwarted by roof falls, explosion dangers, quality of air, visibility through smoke and dust, mental stress and physical endurance. The inability of human rescuers to cover sufficient distance in a short time often has fatal consequences for accident victims. Robots which autonomously scout ahead of human rescuers and summarize environmental information can greatly enhance situational awareness, enabling teams to push forward without delay. This proposal envisions immersive, robotically-created 3D models, fused from many sensor sources and created through smoke, dust, mud, flood and fire. These expansive models provide rescuers with visual clarity that are uncorrupted by environmental condition. Illumination artifacts, sensor noise and errors are removed by fusion and intelligent view planning of radical new modalities including LIDAR, RADAR, multispectral imaging and actively-illuminated RGB sensing. <br/><br/>While the world has often been captivated by high-visibility mine accidents, robots are tangible tools with visible results that have the opportunity to become the centerpiece of any rescue effort. The disrupting effect of the first trapped miner found and human life saved by a rescue robot would inspire countless people to appreciate the evolving role of science and technology in our lives. Prior underground robotics work by this team have generated considerable press and initiated acceptance of robotic technology in even the most change-adverse industries. The investigators will continue to push for robots through media appearances, lab and mine tours for school children and live demonstrations at science museums. This project will support education at all levels, which includes supporting postdoctoral and graduate research in robotics at CMU, employing undergraduate REU interns, and developing curriculum for the Mobile Robot Design course at CMU. Contributions from this research will impact automation in industries ranging from underground production to civic inspection, and defense."
"1328805","NRI: Large: Collaborative Research: Human-robot Coordinated Manipulation and Transportation of Large Objects","IIS","National Robotics Initiative","10/01/2013","01/04/2016","R. Vijay Kumar","PA","University of Pennsylvania","Standard Grant","Jie Yang","09/30/2018","$700,000.00","Daniel Lee, Katherine Kuchenbecker","Kumar@seas.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","8013","7298, 7925, 8086","$0.00","Motivated by the complementary abilities of humans and humanoids, the objective of this proposal is to develop the science and technology necessary for realizing human-robot cooperative object manipulation and transportation. The key concepts that this research seeks to promote are adaptability to human activity under minimal communication, and robustness to variability and uncertainty in the environment, achieved through a layered representation and deliberate processing of the available information. Moreover, this project aims to make maximum use of a minimal set of sensors to plan and control the actions of the robot, while ensuring safe and efficient cooperative transportation. The embodiment of this research is a humanoid co-worker that bears most of the load, when helping a person to carry an object, without requiring excessive communication, or prior training on the part of the human.<br/><br/>By introducing concrete methods for human-robot physical collaboration in semi-structured environments, this project enables a unique synergy between robots and humans that has the potential to increase productivity, and reduce accidents and injuries. In doing so, it also promotes the advancement of new practical applications of robots in construction, manufacturing, logistics, and home services. By developing open-source, portable algorithms for humanoid robots and mobile manipulators, this effort results in cost and time savings for researchers, developers, educators, and end-users in robotics. Finally, through an aggressive educational and community outreach plan, and by actively engaging K-12 students in an exciting RoboTech Fellows program, this project seeks to increase diversity and attract underrepresented groups to STEM."
"1427081","NRI: Collaborative Research: Human-Supervised Perception and Grasping in Clutter","IIS","COLLABORATIVE RESEARCH, IIS SPECIAL PROJECTS, National Robotics Initiative","08/15/2014","06/05/2015","Robert Platt","MA","Northeastern University","Standard Grant","Ephraim P. Glinert","07/31/2017","$807,978.00","","r.platt@neu.edu","360 HUNTINGTON AVE","BOSTON","MA","021155005","6173732508","CSE","7298, 7484, 8013","5947, 8086, 9251","$0.00","One of the basic building blocks in semi-autonomous manipulation is the ability for a robot to grasp an object that a human operator indicates. There are many tasks where the natural way for a human and robot to work together is for the human to point out the approximate locations of objects to be grasped and for the robot to generate the precise motions necessary to achieve the grasp. This core ""auto-grasp"" functionality is critical to providing assistive manipulation for the disabled and elderly, as well as for a variety of military, police, space, or underwater applications. But implementing auto-grasp capability can be challenging in situations where the environment is cluttered, or when it is difficult to determine the grasp intention of the human. In this collaborative project that combines expertise from two institutions, the PIs will tackle situations where it is necessary for the robot to actively explore or ""interrogate"" the environment in order to figure out what the human intends to grasp and how the robot should do it. To these ends, the PIs will investigate a modified approach to planning under uncertainty known as belief space planning. Belief space planning is well-suited to active localization for grasping, because it is a single framework in which the algorithm can reason about perception-oriented and goal-orientation parts of the task. The PIs will use belief space planning to localize graspable geometries in the environment, known as grasp affordances, in a region indicated by the user. They will also explore different ways in which a human can interact with the system in order to control the grasping. The application focus of the work will be in assistive manipulation, where a person who is elderly or disabled operates an assistive robot arm mounted on an electric wheelchair or scooter. User studies will determine the best methods for the target population to operate the system. The project will contribute to the opportunities available for undergraduates and high school students in the PIs' institutions, and it will also be integrated as appropriate into the curricula of the courses they teach.<br/><br/>This research contains two key innovations that the PIs expect will make robot grasping more robust. The first is to incorporate ideas from belief space planning into the reach and grasp planning process. Because belief space planning can reason about how the robot's own ""state of information"" is expected to change in the future, it is capable of producing plans that acquire task-relevant information in the course of performing a task. The second innovation is a new approach to perception-for-grasping that localizes grasp affordance geometries in the neighborhoods of objects of potential interest. Not only is this grasp affordance approach helpful to the belief space planner, but the PIs' preliminary work indicates that this approach can be accurate and very fast (10Hz). Finally, the connection between the user interface and uncertainty in the location of the grasp target will also be explored, the plan being to model human behavior as an uncertain system where hidden variables describe user intention."
"1451327","Workshop: Locomotion and Manipulation: Why the Great Divide?","IIS","National Robotics Initiative","09/01/2014","08/25/2014","Koushil Sreenath","PA","Carnegie-Mellon University","Standard Grant","Jeffrey Trinkle","08/31/2015","$50,000.00","","koushils@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","7556, 8086","$0.00","Getting robots to perform useful work in uncertain environments is still a grand challenge in robotics. One of the biggest possible impacts of this ability would be to enable the development of new co-robots that are nurses, handymen, and butlers for the elderly and infirm. Other important applications would be co-robots that can help find and evacuate the injured during first responses to disasters, improvements in design and control of active prosthetic devices and exoskeletons, and locomotion and manipulation behaviors for collaborative human-robot work. This proposal requests funds to organize a workshop focused on developing new planning and control methods to extend robots? abilities to transport themselves to work sites and then to perform useful physical work. <br/><br/>A two-day workshop is proposed to encourage collaboration between the research communities of robotic manipulation and locomotion, both in academia and industry. The workshop is motivated by the fact that, although the underlying physical principles driving locomotion and manipulation are similar, the techniques developed by the two communities are not. The workshop will analyze the reasons for these differences and explore ideas to bring them closer, with the goal of cross-pollinating advances in both fields. In addition, the workshop will consider the impact of industrial efforts to address these problems."
"1328018","NRI: Large: Collaborative Research: Human-robot Coordinated Manipulation and Transportation of Large Objects","IIS","National Robotics Initiative","10/01/2013","09/25/2013","Steven Lavalle","IL","University of Illinois at Urbana-Champaign","Standard Grant","Jie Yang","09/30/2018","$300,000.00","","lavalle@cs.uiuc.edu","SUITE A","CHAMPAIGN","IL","618207473","2173332187","CSE","8013","7298, 7925, 8086","$0.00","Motivated by the complementary abilities of humans and humanoids, the objective of this proposal is to develop the science and technology necessary for realizing human-robot cooperative object manipulation and transportation. The key concepts that this research seeks to promote are adaptability to human activity under minimal communication, and robustness to variability and uncertainty in the environment, achieved through a layered representation and deliberate processing of the available information. Moreover, this project aims to make maximum use of a minimal set of sensors to plan and control the actions of the robot, while ensuring safe and efficient cooperative transportation. The embodiment of this research is a humanoid co-worker that bears most of the load, when helping a person to carry an object, without requiring excessive communication, or prior training on the part of the human.<br/><br/>By introducing concrete methods for human-robot physical collaboration in semi-structured environments, this project enables a unique synergy between robots and humans that has the potential to increase productivity, and reduce accidents and injuries. In doing so, it also promotes the advancement of new practical applications of robots in construction, manufacturing, logistics, and home services. By developing open-source, portable algorithms for humanoid robots and mobile manipulators, this effort results in cost and time savings for researchers, developers, educators, and end-users in robotics. Finally, through an aggressive educational and community outreach plan, and by actively engaging K-12 students in an exciting RoboTech Fellows program, this project seeks to increase diversity and attract underrepresented groups to STEM."
"1349982","EAGER: SupraPed: Biped Robot Support Expansions for Rough Terrains","IIS","National Robotics Initiative","09/01/2013","08/29/2013","Oussama Khatib","CA","Stanford University","Standard Grant","Gregory Chirikjian","08/31/2014","$100,000.00","","ok@robotics.stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","8013","7916, 8086","$0.00","Biped humanoid robots hold the potential to perform human-like locomotion and manipulation tasks in a variety of situations where either skilled humans are unavailable or where environmental conditions prevent human intervention. However, while bipedal motion maximizes maneuverability in tight workspaces, it simultaneously reduces locomotion stability. To initiate a new direction of ongoing research in legged robot motion control, this work will enable humanoid robots to transform into tripeds or quadrupeds or, more generally, ""SupraPeds"". To control the potentially numerous contact points on SupraPeds, we will also develop a software system that implements generic multi-contact control for arbitrary humanoids, which will enable autonomous balancing while satisfying contact force constraints. <br/><br/>This concept is intuitive to humans as everyone has experienced the need for an extra hand or two to maintain balance or navigate difficult terrain. The PIs will capitalize on this aspect to communicate to high school students, undergraduates, and graduate students, as well as educators nationwide, through live and online courses and seminars. To broaden the participation of under-represented groups, we will participate in a Research Experience for Undergraduates (REU) program especially for community colleges and minority-serving institutions. The results of the research will be published in open-access conferences and journals whenever possible, and the sensing and control software will be documented and published on public websites as open-source code. The applications of the work will impact fields such as search and rescue and scientific exploration."
"1450655","Doctoral Student Workshop on Algorithmic Foundations of Robotics","IIS","National Robotics Initiative","08/15/2014","08/12/2014","Nancy Amato","TX","Texas A&M Engineering Experiment Station","Standard Grant","Gregory Chirikjian","07/31/2015","$10,000.00","Samuel Rodriguez","amato@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","8013","7495, 7556","$0.00","This award supports travel for US PhD students attending WAFR 2014 held in Istanbul, Turkey, from August 3 to August 5, 2014. The goal of the workshop is to highlight the research on the design and analysis of robot algorithms from both theoretical and practical angles. This gives an opportunity for the students to interact with senior faculty and get feedback on their work. One of the most important aspects of WAFR is its informal atmosphere, which allows a frank exchange of new, previously unpublished ideas; this is a critical component of graduate student development. In particular, WAFR has been an occasion for graduate students to meet and interact with more senior researchers who many times are not accessible to students at other major robotics conferences. The WAFR 2014 organizing committee will select participants and recipients of this travel support based on their contribution to the workshop. This award aims to have a diverse representation of participants in terms of gender, ethnic background, academic institution, and geographic location."
"1421034","RI: Small: From Impact to Impulsive Manipulation","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","07/15/2014","07/20/2015","Yan-Bin Jia","IA","Iowa State University","Standard Grant","Jeffrey Trinkle","06/30/2017","$524,009.00","","jia@iastate.edu","1138 Pearson","AMES","IA","500112207","5152945225","CSE","7495, 8013","7495, 7923, 9251, 9150","$0.00","When people work, they often take advantage of impacts between objects, for example, a worker impacts a nail with a hammer to drive it in. These impacts generate what are known as ""impulsive forces"" to do work. People take advantage of smaller impulsive forces to complete many common daily tasks too; inserting a key into a key hole, opening a stuck window, and cracking an egg. While the impacts between a plate and a table is much lighter than that applied to a nail, it is nonetheless just as useful. Today's robots have not been designed to take advantage of impulsive forces to accomplish tasks. The PI conducts an investigation into several fundamental and experimental issues surrounding impulsive robotic manipulation and develops methods for their use in robot control. Project results are advancing the theory of multi-body impacts and widening the capabilities of robots. Direct applications of the results are leading to faster and more agile robots for civil and military services and improved efficiency of industrial processes.<br/><br/>The technical goal of this project is to gain in-depth understanding about control of collision outcomes and, more importantly, integration of impact planning with motion planning in robot manipulation. The project consists of three phases. The first phase advances the Principal Investigator's recent work on compliant and multiple impacts to modeling of n-body simultaneous collisions, tackling issues that include state space condensation, impulse growth in high-dimensional space, sensitivity to physical parameters, and area and nonlinear contacts. In parallel with the analysis is the development of a graphical interface called MultiCollide for interactive collision simulation. The second phase investigates how a manipulator can impart via impact a desired motion to an object, solving an inverse impact problem that connects impact dynamics with trajectory planning and manipulator dynamics. The final phase focuses on two impulsive manipulation tasks that respectively address the following important issues: how to leverage contact compliance and geometry, and how to interleave impact planning with robot motion planning."
"1426703","NRI: Collaborative Research: Efficient Algorithms for Contact-Aware State Estimation","IIS","National Robotics Initiative","08/15/2014","08/11/2014","Michael Kaess","PA","Carnegie-Mellon University","Standard Grant","Jie Yang","07/31/2017","$400,000.00","","kaess@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","8086","$0.00","This project addresses the difficult theoretical, computational, and applied challenges required to exploit a deep mathematical relationship between recent advances in machine perception/estimation algorithms and recent advances in algorithms for planning/controlling systems undergoing frictional contact. It explores the immediate applications of these algorithms to perception, robotic object manipulation and parts assembly, and humanoid robots performing complex, multi-contact, whole-body maneuvers. In order to showcase the generality of approach, and simultaneously reach out to the important under-represented minority population, the research team employs a new hands-on short-course curriculum in which students apply the proposed algorithms to predict the outcome of games using visual tracking. The course is being developed in partnership with the MIT Office of Minority Education (OME).<br/><br/>The project brings together expertise in simultaneous localization and mapping (SLAM), robot manipulation, robotic automation, legged robots, and optimization and nonlinear control, leading to a cross-fertilization of ideas and techniques. The research team exploits sparsity in the complementarity formulations of contact in Lagrangian dynamics. The project explores a new algebraic approach to nonlinear estimator design. The project produces new theorems, new algorithms, and experimental results on real robots. The project also represents a new partnership with our industrial collaborator, ABB Robotics. The developed algorithms facilitate a broad range of new applications in which perception and control systems monitor and manipulate physical interactions with the world. From palm-sized smart devices to environmental monitoring, sensors are becoming ubiquitous; to reach their full potential these sensor networks must be able to reason about contact - the basic building block of physical interaction."
"1464219","CRII: RI: Semiparametric Approaches to Learning Robot Dynamics","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","07/01/2015","05/27/2015","Byron Boots","GA","Georgia Tech Research Corporation","Standard Grant","Jeffrey Trinkle","06/30/2016","$75,000.00","","bboots@cc.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","7495, 8013","7495, 8086, 8228","$0.00","Robotics is revolutionizing quality of life in a wide range of domains from healthcare to automobile safety. Parametric modeling techniques are fundamental to robotic prediction and control in these domains, employed with great success when a robot's interaction with an environment can be precisely characterized by Newtonian physics. As complex robotic technology moves into natural and human environments, it is becoming more difficult to robustly characterize these interactions a priori with parametric models. As a result, machine learning is an increasingly important tool: models of complicated and noisy dynamics can be directly learned from a robot's interaction with its environment. In particular, nonparametric learning has shown exceptional promise, often outperforming parameterized, physics-based models when applied to difficult modeling problems. However, nonparametric approaches also have practical drawbacks: they do not incorporate prior knowledge such as physics-based insights and constraints, they are data-intensive, and they often incur significant computational cost. <br/><br/>This is an exploratory investigation of how nonparametric statistical models can be better integrated with parametric physics-based models for robot prediction and control. The focus of this project is on developing new semiparametric models that elegantly compose parametric and nonparametric components for accurate, robust modeling."
"1427399","NRI: Co-Robots for COMPUGIRLS - Culturally Responsive Robotics Education for Underrepresented Girls","IIS","National Robotics Initiative","09/01/2014","08/20/2014","Andrew Williams","WI","Marquette University","Standard Grant","John Krupczak","02/28/2017","$500,000.00","Kimberly Scott","andrew.williams@marquette.edu","P.O. Box 1881","Milwaukee","WI","532011881","4142887200","CSE","8013","8086","$0.00","Robotics in the U.S. represents a multi-billion industry that is growing yearly. Currently a gap exists in the degree to which underrepresented populations participate in the fields of science, technology, engineering and mathematics. The gap is especially severe in the area of robotics. This project lead by Marquette University is developing a culturally-relevant curricula to teach girls to program co-robots or humanoid robots that work in collaboration with people. The focus of the co-robots for COMPUGIRLS program is to expose girls from under-resourced areas to co-robotics using effective, culturally-relevant curricula and instructional approaches. This robotics education program is designed to expand the already extant NSF-funded informal education program, COMPUGIRLS, through supplementing this program?s curriculum with humanoid co-robotic activities. Project activities will be facilitated with the support of engineering majors from Arizona State University serving as near peer mentors. Collaborations between high school students and undergraduates are expected to increase girls' interest in robotics-related careers, and will yield co-robot projects with features that allow the co-robots to interface with girls and members in their communities.<br/><br/>The co-robots for COMPUGIRLS program offers an approach with a high potential to engage girls in the emerging field of co-robotics. The functioning of co-robots involves collaboration and interaction with people. Applications are expected to grow in significance in the future in such diverse areas as healthcare, agriculture, education, and advanced manufacturing. However, most current K-12 robotics activities are focused on task-based robot-to-robot competitions and electro-mechanical robots. These existing robotics programs face challenges in including girls, particularly those from underrepresented populations. The objectives of this project are to develop and study a culturally-responsive co-robotics curriculum in the context of COMPUGIRLS and involve girls in effective humanoid robotics programming. The approach taken is based on earlier COMPUGIRLS research results. The elements of cultural responsiveness consists of engaging girls at the interface of social and technical asset building in ways that require them to reflect on their intersectional identities both technical and social. Developing girls as technosocial change agents with skills in using technology to improve communities was found to be an important element in engaging girls in technological endeavors. This project will employ a mixed methods research design to determine the impact of engaging underrepresented girls in culturally responsive humanoid robotics using culturally relevant instruction and interactions. The research will advance the discipline of co-robotics by considering cultural aspects currently underexplored in robotics education."
"1451230","Grant for Doctoral Consortium of the 2014 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2014)","IIS","National Robotics Initiative","09/01/2014","08/25/2014","Matthew Spenko","IL","Illinois Institute of Technology","Standard Grant","Jeffrey Trinkle","01/31/2015","$25,000.00","","mspenko@iit.edu","10 West 35th Street","Chicago","IL","606163717","3125673035","CSE","8013","7556, 8086","$0.00","This proposal will support U.S. Ph.D. students working in robotics the opportunity to share their knowledge and interact with each other and more senior researchers, to learn about different sub-fields within robotics, to meet potential employers, and to become apprised of new technology that will be demonstrated by vendors. This goal will be accomplished by partially supporting the travel costs for U.S. Ph.D. students to attend the International Conference on Intelligent Robots and Systems (IROS). IROS, sponsored by IEEE and the Robotics Society of Japan (RSJ), is one of the two premier annual conferences, along with IEEE ICRA, in the robotics field. The location of IROS rotates among North America, Europe, and Asia. The conference attracts an international crowd that includes academics, industry workers, entrepreneurs, and funding agency leaders. The meeting is a unique environment that allows participants to exchange ideas, interact with leaders in the field, meet up-and-coming engineers, network, develop collaborations, and generate new ideas on the leading edge of robotic technology."
"1449502","EAGER: Building a ROS Education Ecosystem","CNS","SPECIAL PROJECTS - CISE, IUSE, National Robotics Initiative","09/01/2014","04/17/2015","William Smart","OR","Oregon State University","Standard Grant","Kamau Bobb","02/28/2017","$238,994.00","","bill.smart@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","1714, 1998, 8013","7916, 8086, 9000, 9251","$0.00","Oregon State University proposes to develop and evaluate a model for making modern robotics software techniques, platforms, and models accessible for education. The Open Source Robotics Foundation's (OSRF) Robot Operating System (ROS) is a widely used software architecture for academic robotics research. <br/><br/>The project will: <br/>1. develop and curate open-source curricular materials that will allow educators to more easily deploy ROS in their classes at all levels; <br/>2. ensure that these materials are accessible to both the students in a formal education setting and also to non-traditional learners in an informal setting; <br/>3. provide concrete advice and guidelines for overcoming the common logistical problems of deploying and running a robotics class with ROS; <br/>4. work with robot retailers to develop a ""classroom pack"" of hardware and software that integrates with the curricular materials to reduce the overhead of starting a new class using ROS; and <br/>5. start to build a community of educators and learners, at all levels, invested in using ROS in their classes and other educational activities."
"1421168","RI: Small: Robot Developmental Learning of Skilled Actions","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","09/01/2014","08/22/2014","Benjamin Kuipers","MI","University of Michigan Ann Arbor","Standard Grant","Jeffrey Trinkle","08/31/2017","$446,823.00","","kuipers@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","7495, 8013","7495, 7923, 8086","$0.00","The goal of this project is to show how a robot --- using a continuous stream of visual and tactile data --- can learn to work at a human level of skill in tasks normally done by humans. To function at a human level, it must be able to plan with ""object-level"" abstractions such as putting a red block into the box, and it must also be able to grasp objects and move them while avoiding bumping into things and causing damage to its surroundings. This project is inspired by human cognitive development. A baby learns about objects and actions by bootstrapping from early regularities and unreliable actions to hierarchies of more complex and reliable actions. The hypothesis to be tested is that this bootstrap learning approach allows a robot to achieve human levels of skillful and robust action in a wide range of human-dominated environments.<br/><br/>This project draws on extensive prior work on foundational knowledge representations and machine learning. Learning begins by detecting low-level contingencies --- regularities among observed events --- and refining them into increasingly accurate predictive rules, that can be used to define reliable actions. For a given rule, a simple MDP model is formulated, and reinforcement learning methods learn a policy for accomplishing an action at the next level of the action hierarchy. Learned actions are initially unreliable, but policies and actions improve with experience. Attention is focused where learning is likely to be most productive by intrinsic motivation methods that reward actions that result in successful learning, including the important special case of rewarding attempts to imitate the successful actions of other agents."
"1427213","NRI: Collaborative Research: Optimal Interaction Design Framework for Powered Lower-Extremity Exoskeletons","IIS","National Robotics Initiative","09/01/2014","09/05/2014","Peter Neuhaus","FL","Florida Institute for Human and Machine Cognition, Inc.","Standard Grant","Ephraim P. Glinert","08/31/2017","$458,431.00","","pneuhaus@ihmc.us","40 S. Alcaniz St.","Pensacola","FL","325026008","8502024473","CSE","8013","8086","$0.00","The goal of this project is to establish a user-centered optimal design framework for customized lower-extremity exoskeletons, in which human-exoskeleton physical interactions and dynamics can be predicted and optimized to provide design requirements. The objectives are to establish: (1) mathematical models of disability parameters, performance indices, and desired/task-failure motions; (2) formulations for constraint status, contact load, and coupled dynamics; (3) optimization method of contact load distribution; (4) sensor integration for required data and experimental validation; and (5) prototype design modification, fabrication, and usability tests. This research will open a new paradigm for systematic characterization of disability parameters and the desired motions from engineering perspectives, leading to user-specific mathematical models. The potential to reduce the need for involving human subjects as part of the design iteration loop will result in accelerated development and better performing assistive devices at reduced cost. Given the growing number of individuals who would benefit from customized exoskeletons as assistive devices, this project will have broad social impact by resolving major hurdles to their widespread use. This research will be integrated into comprehensive education and outreach plans for minority students and individuals with disabilities.<br/><br/>The algorithm with controlled infeasibility will provide physically valid solutions of task failure as well as desired motions for integrated human-exoskeleton systems. The concurrent formulations for constraint status/loads and coupled dynamics will resolve the problems of incorporating physical interactions into optimal motions. As a novel design method, this project will introduce optimal contact load distribution subject to exoskeleton dynamics and transformation of a complex design into a dynamically equivalent model. The feedback loops in the design framework will serve as self-evaluation/contingency plans. This research will transform exoskeleton technologies through user-centered design and predictive evaluations by systematically considering end-user requirements and limitations right from the beginning and at each stage of design. Project outcomes will represent a significant breakthrough that will bring exoskeleton technologies to the next level by (a) functioning as a central hub that systematically connects and integrates relevant disciplines; and (b) providing customized design, reduced design cycle, optimized systems with light weight and natural motion, and improved user comfort and safety."
"1449029","AAAI-15 Support for Robotic Activities","IIS","National Robotics Initiative","09/01/2014","08/28/2014","Sandip Sen","CA","Association for the Advancement of Artificial Intelligence","Standard Grant","Jeffrey Trinkle","08/31/2015","$26,000.00","","sandip@utulsa.edu","2275 E BAYSHORE RD STE 160","East Palo Alto","CA","943033224","6503283123","CSE","8013","7556, 8086","$0.00","The AI and Robotics research communities have benefitted from collaborations over the past several decades. However, the recent acceleration in the development of new algorithmic techniques in both fields and new hardware in robotics provide many opportunities for high-impact collaborations. The PIs are requesting funds to take advantage of this opportunity by holding the first of what is expected to be a series of meetings to encourage collaboration between these two communities. The first meeting will be held at the most important AI conference, AAAI-2015, in Austin TX. There will be events that bring some of the most accomplished researchers from Robotics to AAAI and also events that will be exciting school children and undergraduate students."
"1634433","NRI: Collaborative Research: Enabling Risk-Aware Decision Making in Human-Guided Unmanned Surface Vehicle Teams","ECCS","National Robotics Initiative","01/06/2016","03/14/2016","Satyandra Gupta","CA","University of Southern California","Standard Grant","Radhakisan S. Baheti","08/31/2018","$526,167.00","","guptask@usc.edu","University Park","Los Angeles","CA","900890001","2137407762","ENG","8013","092E, 8086","$0.00","Over the last ten years, substantial progress has been made in the development of small low-cost unmanned surface vehicles (USVs). There are a number of civilian applications where deploying a human-robot team consisting of several small USVs and one or more human supervisors can significantly reduce costs, improve safety, and increase operational efficiencies. Representative applications include remote/persistent ocean sensing, marine search and rescue, maritime operations in congested port environments, and industrial offshore supply and support. USVs face unique challenges that are not experienced by robots operating indoors, such as: the need to adhere to marine navigation rules (COLREGs); local current, wave and wind conditions that can severely reduce the dynamic range of sensors and actuators; frequent communication interruptions; and risk and urgency due to rapidly changing situations during outdoor on-water operations. This research aims to develop decision making foundations for enabling teams of humans and USVs to perform complex collaborative tasks. Advances in this area could be extremely important from both a regulatory and practical standpoint for the future deployment of USV systems. Results from this research will enable leveraging the tremendous potential of USVs by reducing the cost of deployment and operational risks in civilian applications. The integration of the research with graduate and undergraduate courses will enhance the robotics and ocean engineering curricula and enrich learning experiences of the participating students. Outreach activities will educate and inform K-12 students about career opportunities in marine robotics. <br/><br/>The overall goal of the proposed effort is to make advances in risk-informed decision making so that teams of USVs and human supervisors can work cooperatively on a wide variety of missions. The proposed work will develop a comprehensive distributed decision making approach by leveraging the latest advances in task coordination and assignment, planning, reactive behaviors, and control to enable the deployment of human-guided USV teams in civilian applications. Progress in these constituent components will be pursued to ensure that they are consistent with each other and to explicitly account for risk during decision making. This research will develop methodologies to model team missions to ensure that all phases of decision making will have the required information for making informed decisions. Decision making methodologies will be developed for sparse advisory control of USV teams to mitigate risks and for coordinating and assigning tasks to different USVs in the team. Algorithms will also be developed for risk-aware deliberative trajectory planning and generating and executing reactive behaviors for mitigating risks. The methods developed will be validated through on-water field experiments."
"1427547","NRI: Collaborative Research: Modeling and Verification of Language-based Interaction","IIS","National Robotics Initiative","08/15/2014","03/10/2015","Nicholas Roy","MA","Massachusetts Institute of Technology","Standard Grant","Jeffrey Trinkle","07/31/2017","$525,000.00","","nickroy@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","8086","$0.00","Many autonomous systems today, such as personal or service robots, are designed primarily to perform tasks independently and in isolation. Integrating these robots with human partners can often result in poor performance, as the robot does not know how to interpret human interaction, and cannot merge information from this interaction with a model that guarantees robot performance. This research brings together key elements that are just now reaching a sufficient level of maturity for integration: firstly, natural language processing and probabilistic modeling to capture human input, and secondly probabilistic synthesis and verification of the combined human-robot systems to ensure correct performance. The outcome will be theory and software to enable correct, effective and natural interactions between robots and humans to be realized. This research will impact most future autonomous systems which require interactions with humans, including service, personal and planetary robots. <br/><br/>The goal of this research is to develop models and algorithms for synthesizing and verifying an integrated human-plus-robot system based on natural language interaction. Algorithms are being developed for probabilistic modeling and inference of natural language, including the grounding of the constituents of the language into the physical world and the human's expectations. These models will enable the development of a distribution over specifications for control synthesis, which will in turn enable the development and verification of correct-by-construction controllers to a particular level of probability. The out years will consider interactive human-robot dialogue to resolve conflicts, and ""open world"" scenarios to enable on-line learning of new models over time. It is expected that this research will enable high reliability and performance in many autonomous systems because of the inherent interaction with humans. Outcomes include open source data and software; community workshops; and undergraduate and graduate student education in the unique area of language, modeling and verification for robotics."
"1450153","NRI: Large: Collaborative Research: Human-robot Coordinated Manipulation and Transportation of Large Objects","IIS","National Robotics Initiative","07/01/2014","05/10/2016","Paul Oh","NV","University of Nevada Las Vegas","Standard Grant","Jie Yang","09/30/2018","$307,998.00","","paul@coe.drexel.edu","4505 MARYLAND PARKWAY","LAS VEGAS","NV","891541055","7028951357","CSE","8013","7298, 7925, 8086, 9251","$0.00","Motivated by the complementary abilities of humans and humanoids, the objective of this proposal is to develop the science and technology necessary for realizing human-robot cooperative object manipulation and transportation. The key concepts that this research seeks to promote are adaptability to human activity under minimal communication, and robustness to variability and uncertainty in the environment, achieved through a layered representation and deliberate processing of the available information. Moreover, this project aims to make maximum use of a minimal set of sensors to plan and control the actions of the robot, while ensuring safe and efficient cooperative transportation. The embodiment of this research is a humanoid co-worker that bears most of the load, when helping a person to carry an object, without requiring excessive communication, or prior training on the part of the human.<br/><br/>By introducing concrete methods for human-robot physical collaboration in semi-structured environments, this project enables a unique synergy between robots and humans that has the potential to increase productivity, and reduce accidents and injuries. In doing so, it also promotes the advancement of new practical applications of robots in construction, manufacturing, logistics, and home services. By developing open-source, portable algorithms for humanoid robots and mobile manipulators, this effort results in cost and time savings for researchers, developers, educators, and end-users in robotics. Finally, through an aggressive educational and community outreach plan, and by actively engaging K-12 students in an exciting RoboTech Fellows program, this project seeks to increase diversity and attract underrepresented groups to STEM."
"1132487","Travel Support for NIST Micro-Robotics Challenge at IEEE ICRA 2011","IIS","Manufacturing Machines & Equip, ROBUST INTELLIGENCE, National Robotics Initiative","04/15/2011","08/15/2013","Dan Popa","TX","University of Texas at Arlington","Standard Grant","Richard Voyles","03/31/2013","$21,500.00","","dan.popa@louisville.edu","1 UNIVERSITY OF TEXAS AT","Arlington","TX","760190145","8172722105","CSE","1468, 7495, 8013","082E, 083E, 1468, 7495, 7556, 8086","$0.00","This project supports student travel to the NIST Micro-Robotics Challenge which involves advances in the design and fabrication of microelectromechanical systems (MEMS) enabling the development of mobile micro-robots that can autonomously navigate and manipulate in controlled environments. It is expected that this technology will be critical in applications as varied as intelligent sensor networks, in vivo medical diagnosis and treatment, and adaptive microelectronics. Students explore micro-robot locomotion, power storage, embedded intelligence, and motion measurement. The NIST performance-based competitions for mobile micro-robots: 1) motivates researchers to accelerate micro-robot development, 2) reveals the most pressing technical challenges, and 3) evaluates the most successful methods for locomotion and manipulation at the micro-scale (e.g., actuation techniques for crawling).<br/><br/>In the competition research teams showcase working demonstrations that support the challenge themes of micro-scale actuation, manipulation and locomotion. This funding increases the competitiveness of US teams, as this event has traditionally been dominated by teams from Europe. The multi-day competitions are held during the IEEE ICRA conference providing an excellent opportunity to engage a broad technical audience. These are also be open to the general public to raise awareness of the state-of-the-art. The competitions provide an opportunity for students and leaders to learn how micro-robotics can play an important role in society."
"1150223","CAREER: Modeling and Design of Composite Swarming Behaviors","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","03/01/2012","01/12/2016","Nikolaus Correll","CO","University of Colorado at Boulder","Continuing grant","Jeffrey Trinkle","02/28/2017","$545,645.00","","ncorrell@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","7495, 8013","1045, 7495, 9251, CL10","$0.00","This project, developing models for complex swarming behaviors by incrementally starting from the properties of an individual agent, which includes noisy sensors and actuators and stochastic behavior, and validating the models by systematic experimentation with physical systems, will bridge the gap between analysis by simple primitives and their composition into complex swarming behaviors. Specifically, this research will consist of a comprehensive study of containment, partitioning, and re-configuration swarming primitives, and then show how they can be composed into complex swarming behaviors such as pattern recognition, sensor-based motion, and adaptive shape change. These primitives and behaviors have been chosen because they challenge existing modeling approaches and lead to independent contributions addressing grand challenge applications by themselves.<br/><br/>Inspired by the robust and scalable operation of bees, termites, ants and multi-cellular organisms, the funding of this proposed work will lead to a methodology to compose stochastic swarming behaviors in a principled way using probabilistic models. These models will be grounded in the probabilistic behavior of an individual agent's sensing, actuation, communication and computational properties by modeling the system at multiple levels of abstraction going back and forth from physical experimentation, kinematic models, and stochastic simulations to macroscopic difference equations. By grounding the models in physical experiments, models will be able to serve as design tool for not only the computational, but also the physical properties of an individual agent.<br/><br/>Broader Impacts: A principled methodology for modeling and designing swarming systems will create the foundation for designing and deploying swarm robotic systems for search and rescue, environmental response, precision agriculture, and surveillance, among others. At the same time, the ability to design swarms might also shed light on the workings of biological and chemical systems, such as social insects, multi-cellular systems, and self-assembly, enabling the design of complex multi-cellular systems with arbitrary functionality. Finally, algorithms and systems resulting from this research lend themselves to artistic installations, drawing a broad public into the fascination of swarming systems and nurture an understanding for the distributed nature that is common to all living systems. The proposed research will be deeply integrated with education. Specific offerings developed in this integrated research and education plan include: a modular robotic activity that introduces computational thinking to 4th graders, a crash-course on embedded systems geared at 1st year students from under-represented groups, a comprehensive robotics curriculum for upperclassmen, and an interdisciplinary graduate seminar on ""Swarm Intelligence"" and ""Self-Assembly"" that will actively involve students from other disciplines and encourage them to apply the proposed multi-level modeling methodology to their research."
"1518652","RAPID: Teleoperated Robot Systems in Support of Health Care Workers","IIS","INFORMATION TECHNOLOGY RESEARC, National Robotics Initiative","12/15/2014","05/06/2015","William Smart","OR","Oregon State University","Standard Grant","Jeffrey Trinkle","11/30/2016","$79,002.00","","bill.smart@oregonstate.edu","OREGON STATE UNIVERSITY","Corvallis","OR","973318507","5417374933","CSE","1640, 8013","001Z, 1640, 7914, 8086, 9251","$0.00","The expanded Ebola Testing Unit will allow health care workers to reduce their exposure to highly contagious pathogens, such as Ebola Virus Disease, by creating a physical separation between them and the pathogen. The remote-controlled robot system will allow the health care worker to perform some tasks without having the be physically close to sources of potential infection and will, as a result, dramatically reduce their risks of contracting the disease they are treating. The proposed system will both reduce the risk of infection to health care workers, and will increase the quality of care that patients receive: by reducing the time requirements of donning and doffing protective clothing, health care professionals can concentrate on patient care. Ultimately, the work proposed here will save lives, both in the current Ebola outbreak, and also in future outbreaks of highly-infectious diseases.<br/><br/>The approach will focus on three elements: tele-operated mobile manipulation, protective equipment for easy decontamination of the robot, and a person tracking system for Ebola treatment facilities. A simulated Ebola testing unit will be demonstrated within six months. It will integrate new hardware, address the issues of operation over wireless and identify the robot tasks in consultation with health care professionals. Protection and decontamination systems will be evaluated and the effectiveness of the complete system evaluation. The system will then be integrated in a broader facility in collaboration with WPI."
"1526200","NRI: Collaborative Research: Targeted Observation of Severe Local Storms Using Aerial Robots","IIS","National Robotics Initiative","01/01/2016","08/13/2015","Dezhen Song","TX","Texas A&M Engineering Experiment Station","Standard Grant","Ralph Wachter","12/31/2018","$224,242.00","","dzsong@cs.tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","8013","8086","$0.00","This project addresses the development of self-deploying aerial robotic systems that will enable new in-situ atmospheric science applications. Fixed-wing aerial robotic technology has advanced to the point where platforms fly persistent sampling missions far from remote operators. Likewise, complex atmospheric phenomena can be simulated in near real-time with increasing levels of fidelity. Furthermore, cloud computing technology enables distributed computation on large, dynamic data sets. Combining autonomous airborne sensors with environmental models dispersed over multiple communication and computation channels enables the collection of information essential for examining the fundamental behavior of atmospheric phenomena. The aerial robotic system proposed here will close significant capability gaps in conventional platform?s abilities to collect the data necessary to answer a wide range of scientific questions. The motivating application for this work is improvement in the accuracy and lead-time of tornado warnings.<br/><br/>The proposed project draws on techniques in the areas of robotics, unmanned systems, networked control, wireless communication, active sensing, and atmospheric science to realize the vision of bringing cloud robotics to the clouds. The autonomous self-deploying aerial robotic systems is comprised of multiple robotic sensors and distributed computing nodes including: multiple fixed-wing unmanned aircraft, deployable Lagrangian drifters, mobile Doppler radar, mobile command and control stations, distributed computation nodes in the field and in the lab, a net-centric middleware connecting the dispersed elements, and an autonomous decision-making architecture that closes the loop between sensing in the field and new online numerical weather prediction tools."
"1522853","NRI-Large: Collaborative Research: Multilateral Manipulation by Human-Robot Collaborative Systems","IIS","National Robotics Initiative","09/01/2014","02/03/2015","Jacob Rosen","CA","University of California-Los Angeles","Continuing grant","Jeffrey Trinkle","09/30/2016","$511,045.00","","rosen@seas.ucla.edu","11000 Kinross Avenue, Suite 211","LOS ANGELES","CA","900952000","3107940102","CSE","8013","7925, 8086","$0.00","This project addresses a large space of manipulation problems that are repetitive, injury-causing, or dangerous for humans to perform, yet are currently impossible to reliably achieve with purely autonomous robots. These problems generally require dexterity, complex perception, and complex physical interaction. Yet, many such problems can be reliably addressed with human/robot collaborative (HRC) systems, where one or more humans provide needed perception and adaptability, working with one or more robot systems that provide speed, precision, accuracy, and dexterity at an appropriate scale, combining these complementary capabilities.<br/><br/>The project focuses on multilateral manipulation, which arises when a human controls one or more robot manipulators in partnership with one or more additional controllers (humans or autonomous agents). Complex operations in surgery and manufacturing can benefit from the extra degrees of freedom provided by more than two hands, and training often depends on hands-on interaction between expert and apprentice. Example applications include surgical operations, which typically involve several physicians and assistants, and other medical tasks such as turning a patient in bed and wrapping a cast to constrain a hand. Multilateral manipulation also applies in manufacturing, for example for threading wires or cables, aligning gaskets to obtain a tight seal, and in many household situations, such as folding tablecloths, wrapping packages, and zipping overfilled suitcases so they will fit inside diabolically-designed overhead airline compartments. Multilateral manipulation often arises with deformable materials or multi-jointed objects with more than six degrees of freedom (DOF). The extra DOFs in materials introduce challenges such as computational complexity, but they also can accommodate minor inconsistencies through redundancy and provide system damping. This project advances the fundamental science of multilateral manipulation guided by specific applications from surgery and manufacturing.<br/><br/>Broader Impacts: Multilateral manipulation systems have the potential to improve healthcare, improve American competitiveness and product quality in manufacturing, and open the door to new service robot applications in the home. The project will be guided by an Advisory Board of experts from industry and medical practice. Project results will be disseminated through yearly conference workshops, open-source software tools integrated into common robotics software environments such as Robot Operating System (ROS), and the investigators' research and course webpages, to encourage integration of our approach into research projects and courses at many institutions. Outreach programs, public lab tours, and mentoring of minority students will broaden participation of underrepresented groups in engineering. These activities will encourage participation in STEM activities and provide student and postdoctoral researchers with mentoring experience."
"1528145","NRI: Robust and Low-Cost Smart Skin with Active Sensing Network for Enhancing Human-Robot Interaction","IIS","National Robotics Initiative","10/01/2015","08/27/2015","Fu-Kuo Chang","CA","Stanford University","Standard Grant","Jeffrey Trinkle","09/30/2018","$900,000.00","Marco Pavone","fkchang@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","8013","8086","$0.00","Tactile sensing is ubiquitous in nature - arguably even more essential than vision. Most animals have thousands of cutaneous sensors over their bodies for touch, temperature, etc. But even the most sophisticated robots have relatively few tactile sensors and, after 30 years of research, tactile sensing lags behind computer vision. This project aims at the development of a novel artificial skin mimicking the human skin that can be fitted into any robotic hand providing information-rich ""sense of touch."" This technology leads to the development of extremely sensitive robotic skins with unprecedented tactile sensing capabilities. As such, this work enables a plethora of robotic applications where tactile sensing is of utmost importance, ranging from robotic caregivers to medical robotics and autonomous exploration. The methodology in this project may revolutionize the way future robots are designed enabling their broad applicability. This effort represents a major milestone in endowing robots with the sensory information required to carry out tasks in human-centered environments.<br/><br/>The advent of microprocessors for touch sensing, spurred by the smart phone industry, has helped to address the wiring problem with local processing of information and communication. However, there remain the critical problems of fabricating a multi-functional artificial skin that can conformally cover arbitrary surfaces, diagnosing in real-time, the contact state, and gathering a large amount of data for high-resolution tactile sensing, while minimizing power consumption. Overall, this effort addresses tactile sensing from a system-level point of view. The approach involves the development of advanced manufacturing technologies from leveraging nonstandard CMOS/MEMS/NEMS fabrication processes to produce a low-cost and robust artificial skin outfitted with multi-modal micro-sensors. The tactile sense of touch is achieved via an innovative micro-contact sensing technique based on ultrasound waves generated from embedded sensors to identify local contact/slip conditions. Finally, the validation and performance evaluation is demonstrated through a series of graded tactile sensing experiments."
"1527113","NRI: Collaborative Research: Targeted Observation of Severe Local Storms Using Aerial Robots","IIS","National Robotics Initiative","01/01/2016","08/13/2015","Adam Houston","NE","University of Nebraska-Lincoln","Standard Grant","Ralph Wachter","12/31/2018","$425,652.00","","ahouston2@unl.edu","2200 Vine St, 151 Whittier","Lincoln","NE","685031435","4024723171","CSE","8013","8086, 9150","$0.00","This project addresses the development of self-deploying aerial robotic systems that will enable new in-situ atmospheric science applications. Fixed-wing aerial robotic technology has advanced to the point where platforms fly persistent sampling missions far from remote operators. Likewise, complex atmospheric phenomena can be simulated in near real-time with increasing levels of fidelity. Furthermore, cloud computing technology enables distributed computation on large, dynamic data sets. Combining autonomous airborne sensors with environmental models dispersed over multiple communication and computation channels enables the collection of information essential for examining the fundamental behavior of atmospheric phenomena. The aerial robotic system proposed here will close significant capability gaps in conventional platform?s abilities to collect the data necessary to answer a wide range of scientific questions. The motivating application for this work is improvement in the accuracy and lead-time of tornado warnings.<br/><br/>The proposed project draws on techniques in the areas of robotics, unmanned systems, networked control, wireless communication, active sensing, and atmospheric science to realize the vision of bringing cloud robotics to the clouds. The autonomous self-deploying aerial robotic systems is comprised of multiple robotic sensors and distributed computing nodes including: multiple fixed-wing unmanned aircraft, deployable Lagrangian drifters, mobile Doppler radar, mobile command and control stations, distributed computation nodes in the field and in the lab, a net-centric middleware connecting the dispersed elements, and an autonomous decision-making architecture that closes the loop between sensing in the field and new online numerical weather prediction tools."
"1525045","NRI: Collaborative Research: Targeted Observation of Severe Local Storms Using Aerial Robots","IIS","National Robotics Initiative","01/01/2016","08/13/2015","Ibrahim Isler","MN","University of Minnesota-Twin Cities","Standard Grant","Ralph Wachter","12/31/2018","$292,000.00","","isler@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","8013","8086","$0.00","This project addresses the development of self-deploying aerial robotic systems that will enable new in-situ atmospheric science applications. Fixed-wing aerial robotic technology has advanced to the point where platforms fly persistent sampling missions far from remote operators. Likewise, complex atmospheric phenomena can be simulated in near real-time with increasing levels of fidelity. Furthermore, cloud computing technology enables distributed computation on large, dynamic data sets. Combining autonomous airborne sensors with environmental models dispersed over multiple communication and computation channels enables the collection of information essential for examining the fundamental behavior of atmospheric phenomena. The aerial robotic system proposed here will close significant capability gaps in conventional platform?s abilities to collect the data necessary to answer a wide range of scientific questions. The motivating application for this work is improvement in the accuracy and lead-time of tornado warnings.<br/><br/>The proposed project draws on techniques in the areas of robotics, unmanned systems, networked control, wireless communication, active sensing, and atmospheric science to realize the vision of bringing cloud robotics to the clouds. The autonomous self-deploying aerial robotic systems is comprised of multiple robotic sensors and distributed computing nodes including: multiple fixed-wing unmanned aircraft, deployable Lagrangian drifters, mobile Doppler radar, mobile command and control stations, distributed computation nodes in the field and in the lab, a net-centric middleware connecting the dispersed elements, and an autonomous decision-making architecture that closes the loop between sensing in the field and new online numerical weather prediction tools."
"1521919","Doctoral Consortium at the 2015 International Conference on Robotics and Automation (ICRA 2015)","IIS","National Robotics Initiative","01/01/2015","01/23/2015","Maria Gini","MN","University of Minnesota-Twin Cities","Standard Grant","Jeffrey Trinkle","09/30/2016","$25,000.00","","gini@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","8013","7556, 8086","$0.00","This proposal will support U.S. Ph.D. students working in robotics the opportunity to share their knowledge and interact with each other and more senior researchers, to learn about different sub-fields within robotics, to meet potential employers, and to become apprised of new technology that will be demonstrated by vendors. This goal will be accomplished by partially supporting the travel costs for U.S. Ph.D. students to attend the International Conference on Robotics and Automation (ICRA), which is one of the two flagship conference of the IEEE Robotics and Automation Society. The conference attracts an international group of attendees that includes academics, industry workers, entrepreneurs, and funding agency leaders."
"1526515","NRI: Collaborative Research: Unified Feedback Control and Mechanical Design for Robotic, Prosthetic, and Exoskeleton Locomotion","IIS","National Robotics Initiative","09/01/2015","08/31/2015","Koushil Sreenath","PA","Carnegie-Mellon University","Standard Grant","Radhakisan S. Baheti","08/31/2018","$484,988.00","","koushils@cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","092E, 8086","$0.00","There is a pressing need for wearable robots, e.g., prostheses and exoskeletons, which improve the quality of life for individuals with limited mobility - devices that work symbiotically with human users to achieve stable, safe and efficient locomotion. At present, approximately 4.7 million people in the United States would benefit from an active lower-limb exoskeleton due to the effects of stroke, polio, multiple sclerosis, spinal cord injury, and cerebral palsy, and by 2050 an estimated 1.5 million people in the United States will be living with a major lower-limb amputation. Yet current wearable robotic devices do not address this growing population's needs since they are bulky, heavy, noisy, and require large batteries for even short duration use, while implementing predominately hierarchical control algorithms. Impeding innovation in this domain is the expensive and slow traditional design-build-test approach that ignores the tight coupling between hardware specifications and control algorithm performance. The vision of this work is to provide a methodology---inspired by advancements in robotic locomotion---that allows lower-limb prostheses and exoskeletons to meet real-world requirements through the co-design of the electromechanical and feedback systems. The transformative nature of this work, therefore, stems from its ability to realize wearable robots that synergize with humans to achieve increased mobility, providing a template for the growing robotic assistive device industry and potentially improving the quality of life of millions. <br/><br/>To realize the vision of this work, the overarching research goal is to create a new unified control and design framework that will allow for the efficient and stable locomotion of robots, prostheses, and exoskeletons. A key aspect of this control methodology is the ability to continuously mediate between different objectives enforcing stability and safety in an efficient manner through force-based interactions among (wearable) robotic devices, their environment and the user. The resulting framework will be utilized via control-in-the-loop mechanical design of prostheses and exoskeletons with stringent design requirements, tested experimentally on a novel humanoid robot, and clinically evaluated through human subject trials. This work is, therefore, guided by the following specific goals: (1) develop a unified online optimization-based control framework for (wearable) robotic locomotion that efficiently mediates stability, safety and force constraints, (2) create a feedback loop between formal control synthesis and the mechanical design of wearable robots that satisfy stringent performance requirements, (3) accelerate clinical testing by translating controllers formally and experimentally from bipedal humanoid robots to prostheses and exoskeletons. As a result of these research goals, this work has the potential to create the next generation of robotic systems that enable stable, safe and efficient human mobility."
"1526667","NRI: Collaborative Research: Co-Exploration using Science Hypothesis Maps","IIS","National Robotics Initiative","09/01/2015","08/07/2015","David Wettergreen","PA","Carnegie-Mellon University","Standard Grant","Jeffrey Trinkle","08/31/2018","$857,917.00","","dsw@ri.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","8086","$0.00","This project studies a new approach to exploration. In this approach, the explorer's description of where to go is not prescribed by a path, but instead by a model of what the explorer believes. In the context of a human scientist and robotic explorer working together, this approach transforms the co-robotic relationship into a collaboration in which the human and robot work together to fill in gaps in knowledge to make discoveries. The science hypothesis map is the structure in which human scientists initially describe their belief about the world and in which the belief state evolves as the robotic explorer collects information. <br/><br/>This project addresses a pressing and understudied area of human/robot interaction: it directly tackles the challenges of human scientist interaction with remote robotic explorer by crafting a representation of the environment that is expressive and intuitive for the human while quantifying information to enable meaningful robotic decisions. The hypothesis map formulation is a novel and radical departure from our prior work which has focused on human goal selection and robotic autonomy. It has the potential to revolutionize exploration robotics by changing the nature of the relationship between human and robot from supervisory control to shared information discovery."
"1519031","RAPID: Robotic Nursing Assistant for Contagious Patient","IIS","INFORMATION TECHNOLOGY RESEARC, National Robotics Initiative","12/15/2014","12/17/2014","Yi-Je Lim","MA","Hstar Technologies Co","Standard Grant","Gregory Chirikjian","07/31/2015","$117,572.00","","ylim@hstartech.com","82 Guggins lane","Boxborough","MA","017191517","6172295748","CSE","1640, 8013","001Z, 1640, 7914, 8086","$0.00","Ebola virus spreads through human-to-human transmission via direct contact with the blood, secretions, organs or other bodily fluids of infected people, and with surfaces and materials contaminated with these fluids. Hstar Technologies is developing and validating the Advanced Mobile Lifting and Transferring Robotic Nursing Assistant for Contagious Patients as a viable means for transporting patient without direct contact by care givers. A unique linkage system allows powerful, safe, and flexible lifting and positioning of the patient with a minimal number of actuators. The lifting mechanism is mounted on a low profile, stable and flexible footprint robotic mobile base for various terrains and among buildings. A simple, intuitive user interface allows control by a single caregiver with minimal training beside the robot or remotely.<br/><br/>This research includes developing feasible and novel cooperative nurse-robot protocols that will result in healthcare enhancement for both patients and caregivers. This project utilizes rapidly deployable, articulated robotic arms with gentle conveyor belts in reaction of urgent outbreak of contagious disease. Hstar Technologies is drawing upon its expertise in autonomous mobile platform, human robot interaction, operation control, robotic dexterous manipulation, and haptics."
"1527919","NRI: Collaborative Research: Targeted Observation of Severe Local Storms Using Aerial Robots","IIS","National Robotics Initiative","01/01/2016","08/13/2015","Eric Frew","CO","University of Colorado at Boulder","Standard Grant","Ralph Wachter","12/31/2018","$611,859.00","Brian Argrow","eric.frew@colorado.edu","3100 Marine Street, Room 481","Boulder","CO","803031058","3034926221","CSE","8013","8086","$0.00","This project addresses the development of self-deploying aerial robotic systems that will enable new in-situ atmospheric science applications. Fixed-wing aerial robotic technology has advanced to the point where platforms fly persistent sampling missions far from remote operators. Likewise, complex atmospheric phenomena can be simulated in near real-time with increasing levels of fidelity. Furthermore, cloud computing technology enables distributed computation on large, dynamic data sets. Combining autonomous airborne sensors with environmental models dispersed over multiple communication and computation channels enables the collection of information essential for examining the fundamental behavior of atmospheric phenomena. The aerial robotic system proposed here will close significant capability gaps in conventional platform's abilities to collect the data necessary to answer a wide range of scientific questions. The motivating application for this work is improvement in the accuracy and lead-time of tornado warnings.<br/><br/>The proposed project draws on techniques in the areas of robotics, unmanned systems, networked control, wireless communication, active sensing, and atmospheric science to realize the vision of bringing cloud robotics to the clouds. The autonomous self-deploying aerial robotic systems is comprised of multiple robotic sensors and distributed computing nodes including: multiple fixed-wing unmanned aircraft, deployable Lagrangian drifters, mobile Doppler radar, mobile command and control stations, distributed computation nodes in the field and in the lab, a net-centric middleware connecting the dispersed elements, and an autonomous decision-making architecture that closes the loop between sensing in the field and new online numerical weather prediction tools."
"1522904","NRI: Collaborative Research: RobotSLANG: Simultaneous Localization, Mapping, and Language Acquisition","IIS","National Robotics Initiative","09/01/2015","08/06/2015","Jason Corso","MI","University of Michigan Ann Arbor","Standard Grant","Ephraim P. Glinert","08/31/2018","$649,999.00","","jjcorso@eecs.umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","CSE","8013","8086","$0.00","Humans and robots alike have a critical need to navigate through new environments to carry out everyday tasks. A parent and child may be touring a college campus; a robot may be searching for survivors after a building has collapsed. In this collaboration by faculty at two institutions, the PIs envision human and robotic partners sharing common perceptual-linguistic experiences and cooperating in mundane tasks like janitorial work and home care as well as in critical tasks like emergency response or search-and-rescue. But while mapping and navigation are now commonplace for mobile robots, when considering human-robot collaboration for even simple tasks one is confronted by a critical barrier: robots and people do not share a common language. Human language is rich in linguistic elements for describing our spatial environment, the objects and places within it, and navigable paths through it (e.g., ""go down the hallway and enter the third door on the right.""). Robots, on the other hand, inhabit a metric world of occupied and unoccupied discretized grid cells, wherein most objects are devoid of meaning (semantics). The PIs' goal in this project is to overcome this limitation by conjoining the well understood problem of simultaneous localization and mapping (SLAM) with that of language acquisition, in order to enable robots to learn to communicate with people in English about navigation tasks. The PIs will spur interest in this novel research area within the scientific community by means of an Amazing Race challenge problem modeled after the reality television show of the same name, which will place robots and human-robot teams in unknown environments and charge them with completing a specific task as quickly as possible. Other outreach activities will include visits to K-12 schools with demonstrations. <br/><br/>This work will focus on simultaneous localization, mapping, and language acquisition, a field of inquiry that remains untouched. The crucial principles are that semantics are formulated as a cost function, which in turn specifies a joint distribution over many variables including those capturing sensory input, language, the environment map, and robot motor control. The cost function and joint distribution support standard inference of many forms, such as command following. More importantly, they support multidirectional inference over multiple variable sets jointly, such as simultaneous mapping and language interpretation. Within this innovative multivariate optimization-based framework, the PIs plan a thorough experimental regimen including both synthetic and real-world datasets of challenging environments, grounding the semantics of natural language in spatial maps of the realistic visual world and robot motor control, while navigating along particular paths or to arrive at particular destinations in (possibly novel) environments that are mapped not only in a geometric sense but also with linguistic underpinning to these particular paths and destinations. The language approach is compositional and uses spatially-grounded representations of nouns (objects/places) and prepositions (relations between them). These spatially-grounded representations will be modeled in the context of mapping. Furthermore, the PIs will consider realistic environments and adapt visual models thereof according to the joint model. The PIs are aware of no other work that jointly models mapping, vision, and language acquisition."
"1526986","NRI: Simulation Guided Design To Optimize the Performance of Robotic Lower Limb Prostheses","IIS","National Robotics Initiative","09/01/2015","08/12/2015","Frank Sup IV","MA","University of Massachusetts Amherst","Standard Grant","Jeffrey Trinkle","08/31/2018","$630,331.00","Brian Umberger","sup@umass.edu","Research Administration Building","AMHERST","MA","010039242","4135450698","CSE","8013","8086","$0.00","The goal of this project is to use sophisticated computer models of the human body to help design the next generation of robotic prosthetic technologies that maximize mobility for lower-limb amputees. The major outcome of this project will be an improved approach for designing prosthetic devices that reduce loading on the body and make walking easier. The success of this project will improve the quality of life for lower limb amputees by increasing their mobility and their ability to participate in the activities of daily life. The proposed research is especially relevant for older amputees whose residual limbs cannot tolerate significant loading. This award will also support the training of the next generation of engineers and scientists and the development of an innovative STEM robotics program targeted toward middle and high school teachers and their students.<br/><br/>This project is focused on creating a new design process for assistive robotic devices that aid people with mobility impairments. Using modeling and simulation, optimal robot forms and controls will be identified. The case study for this project is the development of prostheses for below-knee amputees. The guiding principle of the project is to consider the complete and altered anatomy of the person and develop solutions that are not limited to anthropomorphic mimicry. Detailed musculoskeletal models and optimal control simulations will be used to guide the robotic prosthesis development process towards optimized loading conditions while minimizing metabolic energy consumption. In this process, the optimal prosthesis form and specifications are initially unknown and are generated through predictive simulations. The results will then be reverse-engineered to develop robotic ankle prostheses that enable these optimal gait patterns for below-knee amputees. This approach will later be extended to maximize the performance of other co-robot systems such as exoskeletons and other rehabilitation robots."
"1527826","NRI: Collaborative Research: Versatile Locomotion: From Walking to Dexterous Climbing with a Human-Scale Robot","IIS","National Robotics Initiative","09/01/2015","05/02/2016","Kris Hauser","NC","Duke University","Standard Grant","Jeffrey Trinkle","08/31/2018","$480,712.00","","kris.hauser@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","8013","8086, 9251","$0.00","The project aims to give legged robots the skills to navigate a wide variety of terrain. This capability is needed to employ robots in applications such as search-and-rescue, construction, and exploration of remote environments on Earth and other planets. The multidisciplinary team, composed of researchers at Duke, Stanford, UC Santa Barbara, JPL, and Motiv Robotics, will develop a robot to climb a variety of surfaces ranging from flat ground to overhanging cliffs. Using an array of sensors, unique hands, and sophisticated algorithms, the robot will dynamically adopt walking, crawling, climbing, and swinging strategies to traverse wildly varied terrain. During the course of this research, the team hopes to achieve the milestone of the first demonstration of a human-scale rock climbing robot. The research is also expected to lead to insights into cognitive and biomechanical processes in human and animal locomotion.<br/><br/>Although rock climbing serves as an ideal proving ground for the work, this project conducts basic research to address more a general-purpose goal; namely, to provide the physical and cognitive skills for robots to adaptively navigate varied terrain. It takes a dexterous climbing approach, which uses non-gaited, coordinated sequences of contact to move the body, much as dexterous manipulation uses contact with the fingers and palm to move an object. It will apply principles from optimization, machine learning, bioinspiration, and control theory to make intellectual contributions in several domains, such as robot hand design, planning algorithms, balance strategies, and locomotion performance measurement. Novel grippers, sensor-based planning strategies, reactive maneuvers, and locomotion metrics will be developed during the course of this research."
"1546747","RSS 2015 Workshop on Women in Robotics","IIS","National Robotics Initiative","09/01/2015","07/24/2015","Julie Shah","MA","Massachusetts Institute of Technology","Standard Grant","Jeffrey Trinkle","08/31/2016","$10,032.00","","arnoldj@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","7556, 8086","$0.00","Robotics is undergoing tremendous growth over a wide range of important areas, from transportation, to manufacturing, entertainment, space exploration, health-care and education. While the field is progressing forward, both in research findings and industrial markets, the percentage of female roboticists continues to lag far behind their male counterparts. This project will strengthen the presence of women in robotics by organizing ""RSS 2015 Workshop on Women in Robotics"" and partially defraying the cost of women to attend. The workshop will be held at the Robot Science and Systems (RSS) conference on July 16th in Rome, Italy."
"1526534","NRI: Collaborative Research: Unified Feedback Control and Mechanical Design for Robotic, Prosthetic, and Exoskeleton Locomotion","IIS","National Robotics Initiative","09/01/2015","08/31/2015","Levi Hargrove","IL","Rehabilitation Institute of Chicago","Standard Grant","Radhakisan S. Baheti","08/31/2018","$302,981.00","","l-hargrove@northwestern.edu","345 East Superior Street","Chicago","IL","606112654","3122384534","CSE","8013","092E, 8086","$0.00","There is a pressing need for wearable robots, e.g., prostheses and exoskeletons, which improve the quality of life for individuals with limited mobility - devices that work symbiotically with human users to achieve stable, safe and efficient locomotion. At present, approximately 4.7 million people in the United States would benefit from an active lower-limb exoskeleton due to the effects of stroke, polio, multiple sclerosis, spinal cord injury, and cerebral palsy, and by 2050 an estimated 1.5 million people in the United States will be living with a major lower-limb amputation. Yet current wearable robotic devices do not address this growing population's needs since they are bulky, heavy, noisy, and require large batteries for even short duration use, while implementing predominately hierarchical control algorithms. Impeding innovation in this domain is the expensive and slow traditional design-build-test approach that ignores the tight coupling between hardware specifications and control algorithm performance. The vision of this work is to provide a methodology---inspired by advancements in robotic locomotion---that allows lower-limb prostheses and exoskeletons to meet real-world requirements through the co-design of the electromechanical and feedback systems. The transformative nature of this work, therefore, stems from its ability to realize wearable robots that synergize with humans to achieve increased mobility, providing a template for the growing robotic assistive device industry and potentially improving the quality of life of millions. <br/><br/>To realize the vision of this work, the overarching research goal is to create a new unified control and design framework that will allow for the efficient and stable locomotion of robots, prostheses, and exoskeletons. A key aspect of this control methodology is the ability to continuously mediate between different objectives enforcing stability and safety in an efficient manner through force-based interactions among (wearable) robotic devices, their environment and the user. The resulting framework will be utilized via control-in-the-loop mechanical design of prostheses and exoskeletons with stringent design requirements, tested experimentally on a novel humanoid robot, and clinically evaluated through human subject trials. This work is, therefore, guided by the following specific goals: (1) develop a unified online optimization-based control framework for (wearable) robotic locomotion that efficiently mediates stability, safety and force constraints, (2) create a feedback loop between formal control synthesis and the mechanical design of wearable robots that satisfy stringent performance requirements, (3) accelerate clinical testing by translating controllers formally and experimentally from bipedal humanoid robots to prostheses and exoskeletons. As a result of these research goals, this work has the potential to create the next generation of robotic systems that enable stable, safe and efficient human mobility."
"1513221","RAPID: Tele-Nursing Robots for Remote Treatment of Ebola Patients","IIS","INFORMATION TECHNOLOGY RESEARC, National Robotics Initiative","12/01/2014","12/02/2014","Kris Hauser","NC","Duke University","Standard Grant","Gregory Chirikjian","11/30/2015","$73,025.00","","kris.hauser@duke.edu","2200 W. Main St, Suite 710","Durham","NC","277054010","9196843030","CSE","1640, 8013","001Z, 7914, 8013, 1640","$0.00","Healthcare workers in the Ebola crisis are at risk for infection due to routine interactions <br/>with patients. These workers wear protective gear to protect them while handling contaminated material. But this gear can be challenging to remove safely. This award envisions that remote-controlled robots could perform common nursing duties inside hazardous clinical areas. If successful, this approach could significantly reduce the frequency with which healthcare workers are exposed to contagions and other biohazards. This project will develop a prototype mobile manipulator robot, a human operator?s console, and operator assistance algorithms to automate or partially-automate tedious and error-prone tasks. Medical professionals will be consulted at the start of the study to establish a prioritized list of clinical tasks for Ebola treatment, and at the end of the study to evaluate the prototype.<br/><br/>The research will explore the tele-nursing concept across the fields of mechanical design, sensors, motion planning and control, human-robot interaction, and robot system integration. Tele-operation of complex tasks, in particular, bimanual manipulation in highly non-engineered environments, is still a major research challenge, while contribution will be made also in algorithms for operator assistance for human-guided manipulation. The research is directly applicable to other infectious disease scenarios as well. Improving operator interfaces may also further our knowledge of human-controlled robots in other fields, such as household service, space robots, and material handling in industry and logistics."
"1540080","2015 National Robotics Initiative PI Meeting","IIS","National Robotics Initiative","08/15/2015","08/05/2015","Martial Hebert","PA","Carnegie-Mellon University","Standard Grant","Jie Yang","07/31/2016","$122,876.00","Daniel Huber","martial.Hebert@cs.cmu.edu","5000 Forbes Avenue","PITTSBURGH","PA","152133815","4122689527","CSE","8013","7556, 8086","$0.00","This project is to organize and execute the annual principal investigator (PI) meeting for the National Robotics Initiative (NRI). The workshop will bring together all the PIs engaged in the NRI for a two day workshop to discuss research, educational initiatives, and methods for transition of results. <br/><br/>The workshop includes oral presentations by involved program managers, the PIs, a poster session with presentation of all sponsored projects, tutorials, and several keynote speeches. An important aspect of the workshop is program-wide networking and exploration of mechanisms to optimize the results of the overall initiative with respect to research, training, and societal impact."
"1537023","NRI: Collaborative Research: A Dynamic Bayesian Approach to Real-Time Estimation and Filtering in Grasp Acquisition and other Contact Tasks (Continuation)","IIS","National Robotics Initiative","09/01/2015","08/05/2015","Barbara Cutler","NY","Rensselaer Polytechnic Institute","Standard Grant","Ephraim P. Glinert","08/31/2018","$369,239.00","","cutler@cs.rpi.edu","110 8TH ST","Troy","NY","121803522","5182766000","CSE","8013","8086","$0.00","A current weakness of robots is their inability to quickly and reliably perform contact tasks in unstructured environments. The goal of this project, which represents a collaboration between faculty at two partner institutions, is to alleviate this shortcoming by developing techniques that will afford robots accurate real-time perception in tasks exhibiting intermittent contact. Project outcomes will have a strong impact in manipulation tasks, as robots become more capable and autonomous. The PIs also expect successful applications in other areas, for instance to drive real-time haptic displays in augmented reality systems, to extract human manipulation strategies from observed kinesthetic demonstrations, and to identify model parameters to improve simulation accuracy, not to mention in advancing the level of autonomy for space and undersea exploration. Additional applications outside of robotics are anticipated in situations where a system experiences abrupt state transitions and the goal is either state estimation or real-time feedback control (e.g., chemical, financial, and geological systems). The PIs' labs have a track record of supporting women and under-represented minorities, and the research will be integrated into a variety of pedagogical activities at the graduate and undergraduate level on both campuses.<br/><br/>In previous work the team proposed the DBC-SLAM framework, in which continuous states (i.e., poses, velocities and contact impulses), and discrete contact states (i.e., contact-noncontact and stick-slip) of the manipulated objects, are tracked and important model parameters are estimated. In this research, they will extend that work significantly in two directions. First, they will design new parallel, anytime complementarity problem (CP) solvers in order to attain real-time performance. Second, they will enhance the dynamic Bayesian models in DBC-SLAM to allow the use of point-cloud observations and more complex geometric models of the objects, robot links, and environment. The intellectual merit of the project lies in three main activities: first, the creative, yet rigorous, technical process of designing perception algorithms based on fundamental first principles of nonsmooth mechanics and Bayesian estimation in a way that can utilize point-cloud data; second, achieving real-time performance by exploiting the mathematical structure and properties of both the nonsmooth multibody dynamics and CPU/GPU computing systems; and third, pursuing the first two activities in a way that sheds light on the trade-offs between estimation accuracy and speed."
"1526424","NRI: Collaborative Research: Versatile Locomotion: From Walking to Dexterous Climbing With a Human-Scale Robot","IIS","National Robotics Initiative","09/01/2015","08/06/2015","Katie Byl","CA","University of California-Santa Barbara","Standard Grant","Jeffrey Trinkle","08/31/2018","$454,868.00","","katiebyl@ece.ucsb.edu","Office of Research","SANTA BARBARA","CA","931062050","8058934188","CSE","8013","8086","$0.00","The project aims to give legged robots the skills to navigate a wide variety of terrain. This capability is needed to employ robots in applications such as search-and-rescue, construction, and exploration of remote environments on Earth and other planets. The multidisciplinary team, composed of researchers at Duke, Stanford, UC Santa Barbara, JPL, and Motiv Robotics, will develop a robot to climb a variety of surfaces ranging from flat ground to overhanging cliffs. Using an array of sensors, unique hands, and sophisticated algorithms, the robot will dynamically adopt walking, crawling, climbing, and swinging strategies to traverse wildly varied terrain. During the course of this research, the team hopes to achieve the milestone of the first demonstration of a human-scale rock climbing robot. The research is also expected to lead to insights into cognitive and biomechanical processes in human and animal locomotion.<br/><br/>Although rock climbing serves as an ideal proving ground for the work, this project conducts basic research to address more a general-purpose goal; namely, to provide the physical and cognitive skills for robots to adaptively navigate varied terrain. It takes a dexterous climbing approach, which uses non-gaited, coordinated sequences of contact to move the body, much as dexterous manipulation uses contact with the fingers and palm to move an object. It will apply principles from optimization, machine learning, bioinspiration, and control theory to make intellectual contributions in several domains, such as robot hand design, planning algorithms, balance strategies, and locomotion performance measurement. Novel grippers, sensor-based planning strategies, reactive maneuvers, and locomotion metrics will be developed during the course of this research."
"1525006","NRI: Collaborative Research: Unified Feedback Control and Mechanical Design for Robotic, Prosthetic, and Exoskeleton Locomotion","ECCS","National Robotics Initiative","09/01/2015","08/31/2015","Jessy Grizzle","MI","University of Michigan Ann Arbor","Standard Grant","Radhakisan S. Baheti","08/31/2018","$300,022.00","","grizzle@umich.edu","3003 South State St. Room 1062","Ann Arbor","MI","481091274","7347636438","ENG","8013","092E, 8086","$0.00","There is a pressing need for wearable robots, e.g., prostheses and exoskeletons, which improve the quality of life for individuals with limited mobility - devices that work symbiotically with human users to achieve stable, safe and efficient locomotion. At present, approximately 4.7 million people in the United States would benefit from an active lower-limb exoskeleton due to the effects of stroke, polio, multiple sclerosis, spinal cord injury, and cerebral palsy, and by 2050 an estimated 1.5 million people in the United States will be living with a major lower-limb amputation. Yet current wearable robotic devices do not address this growing population's needs since they are bulky, heavy, noisy, and require large batteries for even short duration use, while implementing predominately hierarchical control algorithms. Impeding innovation in this domain is the expensive and slow traditional design-build-test approach that ignores the tight coupling between hardware specifications and control algorithm performance. The vision of this work is to provide a methodology---inspired by advancements in robotic locomotion---that allows lower-limb prostheses and exoskeletons to meet real-world requirements through the co-design of the electromechanical and feedback systems. The transformative nature of this work, therefore, stems from its ability to realize wearable robots that synergize with humans to achieve increased mobility, providing a template for the growing robotic assistive device industry and potentially improving the quality of life of millions. <br/><br/>To realize the vision of this work, the overarching research goal is to create a new unified control and design framework that will allow for the efficient and stable locomotion of robots, prostheses, and exoskeletons. A key aspect of this control methodology is the ability to continuously mediate between different objectives enforcing stability and safety in an efficient manner through force-based interactions among (wearable) robotic devices, their environment and the user. The resulting framework will be utilized via control-in-the-loop mechanical design of prostheses and exoskeletons with stringent design requirements, tested experimentally on a novel humanoid robot, and clinically evaluated through human subject trials. This work is, therefore, guided by the following specific goals: (1) develop a unified online optimization-based control framework for (wearable) robotic locomotion that efficiently mediates stability, safety and force constraints, (2) create a feedback loop between formal control synthesis and the mechanical design of wearable robots that satisfy stringent performance requirements, (3) accelerate clinical testing by translating controllers formally and experimentally from bipedal humanoid robots to prostheses and exoskeletons. As a result of these research goals, this work has the potential to create the next generation of robotic systems that enable stable, safe and efficient human mobility."
"1537257","NRI: Collaborative Research: A Dynamic Bayesian Approach to Real Time Estimation and Filtering in Grasp Acquisition and Other Contact Tasks (Continuation)","IIS","National Robotics Initiative","09/01/2015","08/05/2015","Siwei Lyu","NY","SUNY at Albany","Standard Grant","Ephraim P. Glinert","08/31/2018","$220,997.00","","lsw@cs.albany.edu","1400 WASHINGTON AVE","Albany","NY","122220100","5184374550","CSE","8013","8086","$0.00","A current weakness of robots is their inability to quickly and reliably perform contact tasks in unstructured environments. The goal of this project, which represents a collaboration between faculty at two partner institutions, is to alleviate this shortcoming by developing techniques that will afford robots accurate real-time perception in tasks exhibiting intermittent contact. Project outcomes will have a strong impact in manipulation tasks, as robots become more capable and autonomous. The PIs also expect successful applications in other areas, for instance to drive real-time haptic displays in augmented reality systems, to extract human manipulation strategies from observed kinesthetic demonstrations, and to identify model parameters to improve simulation accuracy, not to mention in advancing the level of autonomy for space and undersea exploration. Additional applications outside of robotics are anticipated in situations where a system experiences abrupt state transitions and the goal is either state estimation or real-time feedback control (e.g., chemical, financial, and geological systems). The PIs' labs have a track record of supporting women and under-represented minorities, and the research will be integrated into a variety of pedagogical activities at the graduate and undergraduate level on both campuses.<br/><br/>In previous work the team proposed the DBC-SLAM framework, in which continuous states (i.e., poses, velocities and contact impulses), and discrete contact states (i.e., contact-noncontact and stick-slip) of the manipulated objects, are tracked and important model parameters are estimated. In this research, they will extend that work significantly in two directions. First, they will design new parallel, anytime complementarity problem (CP) solvers in order to attain real-time performance. Second, they will enhance the dynamic Bayesian models in DBC-SLAM to allow the use of point-cloud observations and more complex geometric models of the objects, robot links, and environment. The intellectual merit of the project lies in three main activities: first, the creative, yet rigorous, technical process of designing perception algorithms based on fundamental first principles of nonsmooth mechanics and Bayesian estimation in a way that can utilize point-cloud data; second, achieving real-time performance by exploiting the mathematical structure and properties of both the nonsmooth multibody dynamics and CPU/GPU computing systems; and third, pursuing the first two activities in a way that sheds light on the trade-offs between estimation accuracy and speed."
"1514882","INSPIRE: Legged Locomotion for Desert Research","IIS","INSPIRE, National Robotics Initiative, INFORMATION TECHNOLOGY RESEARC, GEOMORPHOLOGY & LAND USE DYNAM, SURFACE EARTH PROCESS SECTION","10/01/2015","03/17/2015","Daniel Koditschek","PA","University of Pennsylvania","Standard Grant","Jeffrey Trinkle","09/30/2017","$1,000,000.00","Douglas Jerolmack","kod@ese.upenn.edu","Research Services","Philadelphia","PA","191046205","2158987293","CSE","8078, 8013, 1640, 7458, 7570","8086, 8653","$0.00","This INSPIRE project is supported by the Directorate of Computer and Information Science and Engineering, the Division of Information and Intelligent Systems, the Directorate of Geosciences, Division of Earth Sciences, and the Office of International and Integrative Activities. Sand and dust storms menace the world. They impact large human populations on nearly every continent, damage habitation, disrupt transportation, threaten agriculture, biodiversity, human health and life, and degrade the environment (desertification). A team of scientists and engineers is developing an autonomous legged robot research assistant, designed to operate within harsh desert environments for purposes of gathering heretofore unavailable measurements of wind and sand movement under conditions far too uncomfortable and dangerous for human presence. These data may offer new insights into dust production and its global effects that could significantly impact predictions of environmental degradation by climate change in drylands. At the same time, meeting the formidable mobility and perceptual capabilities arising from scientists' requirements will advance the foundations and practice of robotics. Ultimately, advances in control of dust emissions from soils made possible by this novel collaboration could prolong the sustainability of the agroecosystem and result in improved air quality of downwind population centers.<br/><br/>Dust emission has traditionally been assumed to release sediment previously deposited in soil, but there is growing evidence that sand abrasion may actually produce significant quantities of new dust. Establishing sand seas as dust factories, rather than simply dust reservoirs represents a qualitatively new result that would cascade through aeolian and climate science. But the severe events of interest are hidden from existing conventional instruments and they pose presently insurmountable challenges to flying, wheeled or even tracked vehicles. Aeolian science needs legged machines to negotiate the steep, shifting slopes, and broken ground under the environmental conditions of interest. Steady running over simple terrain is a largely solved problem in robotics, but transitional maneuvers and agile negotiation of geometrically complex, unstable, fragile terrain characteristic of desert substrates pose a daunting next challenge for legged locomotion, requiring new approaches to turning, gait control, and perception."
"1551535","EAGER: Inferring Mechanical Explanations from Manipulation Demonstrations","IIS","National Robotics Initiative","09/01/2015","08/12/2015","Alberto Rodriguez Garcia","MA","Massachusetts Institute of Technology","Standard Grant","Jeffrey Trinkle","08/31/2017","$225,000.00","","albertor@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","7916, 8086","$0.00","Robots are fast, accurate, reliable, and tireless, which make them great assets for factories. However, robots lack intuition - at which humans excel. Current robots have a hard time when tasked with imagining ways to manufacture new products. In an effective human-robot collaboration, humans conceive new products and feasible ways to manufacture them, and robots follow human guidelines with excellent precision and reliability. The scenario of interest is where a human operator demonstrates an execution of a task, such as mating two parts, and a robot understands how to replicate it. This proposal is concerned with improving the current robot understanding of those demonstrations. In particular, it is concerned with recovering information that is not directly observable with cameras, such as contacts between parts, and the forcefulness of the motions, both important for the ability of the robot to replicate the demonstration.<br/><br/>This proposal aims to automate the inference of contact events and contact forces from noisy kinematic observations of the interaction between two known parts. The central idea is to use trajectory optimization and complementarity based models of contact to project noisy kinematic trajectories into dynamically-sound and environment-compatible motions, contacts, and forces. The problem is naturally under-constrained. There are many possible explanations for a given demonstration, and this proposal will investigate different ways to provide the optimizer with prior information to converge to a ""reasonable"" explanation. The first phase will conduct experiments instrumented with motion capture and force sensing to capture ground truth, and evaluate the ability of the algorithms to explain it, and overcome degradations such as noise and occlusions. A second phase will focus on un-instrumented scenarios and free-from demonstrations. The technical merit of the proposal will be demonstrated in the context of two high impact applications: automated assembly and shelf-picking/shelf-restocking in a warehouse scenario."
"1257438","Building a robotics commons","IIS","COLLABORATIVE RESEARCH, IIS SPECIAL PROJECTS, National Robotics Initiative, , ","08/01/2013","05/19/2016","Brian Gerkey","CA","Open Source Robotics Foundation, Inc.","Continuing grant","Reid Simmons","07/31/2016","$2,035,795.00","","gerkey@osrfoundation.org","170 S. Whisman Road","Mountain View","CA","940411512","4085046028","CSE","7298, 7484, 8013, M127, N132","5950, 7925, 8086, 170E","$0.00","This project addresses the lack of commonality which has historically plagued the robotics research community due to the lack of sharable code. Robotics is an experimental endeavor, and code, the software that is written to control the robot, is at the core of any research or development effort. Innovation occurs best when the community builds on each others' accomplishments and robotics has progressed to the point that both researchers and developers can benefit from a shared, high-quality, common software toolbox to build useful and commercially viable applications with a minimum of duplicated engineering effort. This software is part of an open source ""robotics commons"": a shared resource from which all can benefit and to which are can contribute. To build this commons, this proposal begins with ROS (Robot Operating System), an open software platform comprising libraries and tools that facilitate building and running robot applications. Designed for both robotics researchers and application developers, ROS is already widely used around the world in classrooms, labs, and companies. Hence, the intellectual merit of the proposed project revolves around the software engineering methods used to build, improve, test, distribute, and support this high-quality robotics commons. In this project, development will focus on critical, under-served areas within ROS: flexible automation, user interfaces for non-experts, shared autonomy, robot teams, and improved platform support. Development will not focus on the algorithms available to the robotics community; rather, the intent is to provide the community with the best possible libraries and tools that embody state-of-the-art algorithms and techniques. <br/><br/>Just as the availability of the open source LAMP stack (Linux, Apache, MySQL, and Perl/PHP/Python) enabled the internet revolution in the 1990s, ROS and its related tools are expected to have tremendous broader impacts to the engineering community. The OSRF will provide the worldwide robotics community with freely available open source tools that will enable a generation of robotics students, developers, entrepreneurs, and enthusiasts to more quickly and easily realize their aspirations."
"1227184","NRI-Large: Collaborative Research: Multilateral Manipulation by Human-Robot Collaborative Systems","IIS","National Robotics Initiative","10/01/2012","07/31/2013","Jacob Rosen","CA","University of California-Santa Cruz","Continuing grant","Jeffrey Trinkle","03/31/2015","$616,670.00","","rosen@seas.ucla.edu","1156 High Street","Santa Cruz","CA","950641077","8314595278","CSE","8013","7925, 8086","$0.00","This project addresses a large space of manipulation problems that are repetitive, injury-causing, or dangerous for humans to perform, yet are currently impossible to reliably achieve with purely autonomous robots. These problems generally require dexterity, complex perception, and complex physical interaction. Yet, many such problems can be reliably addressed with human/robot collaborative (HRC) systems, where one or more humans provide needed perception and adaptability, working with one or more robot systems that provide speed, precision, accuracy, and dexterity at an appropriate scale, combining these complementary capabilities.<br/><br/>The project focuses on multilateral manipulation, which arises when a human controls one or more robot manipulators in partnership with one or more additional controllers (humans or autonomous agents). Complex operations in surgery and manufacturing can benefit from the extra degrees of freedom provided by more than two hands, and training often depends on hands-on interaction between expert and apprentice. Example applications include surgical operations, which typically involve several physicians and assistants, and other medical tasks such as turning a patient in bed and wrapping a cast to constrain a hand. Multilateral manipulation also applies in manufacturing, for example for threading wires or cables, aligning gaskets to obtain a tight seal, and in many household situations, such as folding tablecloths, wrapping packages, and zipping overfilled suitcases so they will fit inside diabolically-designed overhead airline compartments. Multilateral manipulation often arises with deformable materials or multi-jointed objects with more than six degrees of freedom (DOF). The extra DOFs in materials introduce challenges such as computational complexity, but they also can accommodate minor inconsistencies through redundancy and provide system damping. This project advances the fundamental science of multilateral manipulation guided by specific applications from surgery and manufacturing.<br/><br/>Broader Impacts: Multilateral manipulation systems have the potential to improve healthcare, improve American competitiveness and product quality in manufacturing, and open the door to new service robot applications in the home. The project will be guided by an Advisory Board of experts from industry and medical practice. Project results will be disseminated through yearly conference workshops, open-source software tools integrated into common robotics software environments such as Robot Operating System (ROS), and the investigators' research and course webpages, to encourage integration of our approach into research projects and courses at many institutions. Outreach programs, public lab tours, and mentoring of minority students will broaden participation of underrepresented groups in engineering. These activities will encourage participation in STEM activities and provide student and postdoctoral researchers with mentoring experience."
"1227406","NRI-Large: Collaborative Research: Multilateral Manipulation by Human-Robot Collaborative Systems","IIS","National Robotics Initiative","10/01/2012","08/01/2013","Allison Okamura","CA","Stanford University","Continuing grant","Jeffrey Trinkle","09/30/2016","$1,163,388.00","","aokamura@stanford.edu","3160 Porter Drive","Palo Alto","CA","943041212","6507232300","CSE","8013","7925, 8086","$0.00","This project addresses a large space of manipulation problems that are repetitive, injury-causing, or dangerous for humans to perform, yet are currently impossible to reliably achieve with purely autonomous robots. These problems generally require dexterity, complex perception, and complex physical interaction. Yet, many such problems can be reliably addressed with human/robot collaborative (HRC) systems, where one or more humans provide needed perception and adaptability, working with one or more robot systems that provide speed, precision, accuracy, and dexterity at an appropriate scale, combining these complementary capabilities.<br/><br/>The project focuses on multilateral manipulation, which arises when a human controls one or more robot manipulators in partnership with one or more additional controllers (humans or autonomous agents). Complex operations in surgery and manufacturing can benefit from the extra degrees of freedom provided by more than two hands, and training often depends on hands-on interaction between expert and apprentice. Example applications include surgical operations, which typically involve several physicians and assistants, and other medical tasks such as turning a patient in bed and wrapping a cast to constrain a hand. Multilateral manipulation also applies in manufacturing, for example for threading wires or cables, aligning gaskets to obtain a tight seal, and in many household situations, such as folding tablecloths, wrapping packages, and zipping overfilled suitcases so they will fit inside diabolically-designed overhead airline compartments. Multilateral manipulation often arises with deformable materials or multi-jointed objects with more than six degrees of freedom (DOF). The extra DOFs in materials introduce challenges such as computational complexity, but they also can accommodate minor inconsistencies through redundancy and provide system damping. This project advances the fundamental science of multilateral manipulation guided by specific applications from surgery and manufacturing.<br/><br/>Broader Impacts: Multilateral manipulation systems have the potential to improve healthcare, improve American competitiveness and product quality in manufacturing, and open the door to new service robot applications in the home. The project will be guided by an Advisory Board of experts from industry and medical practice. Project results will be disseminated through yearly conference workshops, open-source software tools integrated into common robotics software environments such as Robot Operating System (ROS), and the investigators' research and course webpages, to encourage integration of our approach into research projects and courses at many institutions. Outreach programs, public lab tours, and mentoring of minority students will broaden participation of underrepresented groups in engineering. These activities will encourage participation in STEM activities and provide student and postdoctoral researchers with mentoring experience."
"1227277","NRI-Large: Collaborative Research: Multilateral Manipulation by Human-Robot Collaborative Systems","IIS","National Robotics Initiative, COLLABORATIVE RESEARCH, IIS SPECIAL PROJECTS","10/01/2012","05/18/2015","Gregory Hager","MD","Johns Hopkins University","Continuing grant","Jeffrey Trinkle","09/30/2017","$639,942.00","","hager@cs.jhu.edu","3400 N CHARLES ST","Baltimore","MD","212182608","4105168668","CSE","8013, 7298, 7484","7925, 8086, 5936, 7484, 9251","$0.00","This project addresses a large space of manipulation problems that are repetitive, injury-causing, or dangerous for humans to perform, yet are currently impossible to reliably achieve with purely autonomous robots. These problems generally require dexterity, complex perception, and complex physical interaction. Yet, many such problems can be reliably addressed with human/robot collaborative (HRC) systems, where one or more humans provide needed perception and adaptability, working with one or more robot systems that provide speed, precision, accuracy, and dexterity at an appropriate scale, combining these complementary capabilities.<br/><br/>The project focuses on multilateral manipulation, which arises when a human controls one or more robot manipulators in partnership with one or more additional controllers (humans or autonomous agents). Complex operations in surgery and manufacturing can benefit from the extra degrees of freedom provided by more than two hands, and training often depends on hands-on interaction between expert and apprentice. Example applications include surgical operations, which typically involve several physicians and assistants, and other medical tasks such as turning a patient in bed and wrapping a cast to constrain a hand. Multilateral manipulation also applies in manufacturing, for example for threading wires or cables, aligning gaskets to obtain a tight seal, and in many household situations, such as folding tablecloths, wrapping packages, and zipping overfilled suitcases so they will fit inside diabolically-designed overhead airline compartments. Multilateral manipulation often arises with deformable materials or multi-jointed objects with more than six degrees of freedom (DOF). The extra DOFs in materials introduce challenges such as computational complexity, but they also can accommodate minor inconsistencies through redundancy and provide system damping. This project advances the fundamental science of multilateral manipulation guided by specific applications from surgery and manufacturing.<br/><br/>Broader Impacts: Multilateral manipulation systems have the potential to improve healthcare, improve American competitiveness and product quality in manufacturing, and open the door to new service robot applications in the home. The project will be guided by an Advisory Board of experts from industry and medical practice. Project results will be disseminated through yearly conference workshops, open-source software tools integrated into common robotics software environments such as Robot Operating System (ROS), and the investigators' research and course webpages, to encourage integration of our approach into research projects and courses at many institutions. Outreach programs, public lab tours, and mentoring of minority students will broaden participation of underrepresented groups in engineering. These activities will encourage participation in STEM activities and provide student and postdoctoral researchers with mentoring experience."
"1208413","NRI-Small: Mixed Human-Robot Teams for Search and Rescue","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","08/01/2012","07/31/2013","Maria Gini","MN","University of Minnesota-Twin Cities","Standard Grant","Hector Munoz-Avila","12/31/2014","$138,001.00","","gini@cs.umn.edu","200 OAK ST SE","Minneapolis","MN","554552070","6126245599","CSE","7495, 8013","7923, 8086, 9251","$0.00","The project aims at increasing the ability to respond to large-scale disasters and manage emergencies by including robots and agents as teammates of humans in search and rescue teams. The project focuses on large teams of humans and robots that have only incomplete knowledge of the disaster situation while they accomplish the mission to rescue people and prevent fires.<br/><br/>The methodology to achieve cooperation within the teams will be based on the development of mental models shared by team members. The shared mental models will facilitate the interactions among robots and humans by providing a suitable level of abstraction enabling them to share beliefs, desires, and intentions as they work to accomplish their tasks.<br/><br/>The performance of teamwork models will be measured by comparing various task performance metrics (such as time to save people), system level metrics (such as computation time or message traffic), and amount of sharedness of the mental models. The experimental work will be conducted using the open source RoboCup Search and Rescue Simulator.<br/><br/>Broader impacts include integration of research results in undergraduate courses, availability of the software produced as open source, outreach activities to expose K-12 students to research issues and to excite them about using computing methods for real-world problems. The long term objective is to improve preparadeness for emergency situations, which will help saving lives and minimizing loss of properties."
"1208287","NRI-Small: Robot Movement for Patient Improvement - Therapeutic Rehabilitation for Children with Disabilities","IIS","National Robotics Initiative","10/01/2012","07/31/2013","Ayanna Howard","GA","Georgia Tech Research Corporation","Standard Grant","Alexander Leonessa","09/30/2016","$644,516.00","Patricio Vela, Yu-ping Chen","ayanna.howard@ece.gatech.edu","Office of Sponsored Programs","Atlanta","GA","303320420","4048944819","CSE","8013","7923, 8086, 9251","$0.00","In the United States, the Individuals with Disabilities Education Act (IDEA) states that children with a physical disability are entitled to a ""free appropriate public education that emphasizes special education and related services designed to meet their unique needs and prepare them for further education, employment, and independent living."" Unfortunately, access to necessary assistive robotic technology remains unequal and children with physical disabilities and movement disorders are largely overlooked. However, recent successes in commercial robots appear to foreshadow an explosion of promising robotic applications for individuals with disabilities. Not only can robots be tasked to assist with Activities of Daily Living (ADL) but they can also help individuals through rehabilitation exercises such that therapists and family members can provide assistance in other arenas. The major barrier is that, to date, most assistive robotic devices are not designed for children. And although robotic systems for rehabilitation can generally be used to record information about motor performance during active movements, these systems are not ideal for rehabilitation with respect to children. This causes a unique challenge for deploying such robotic devcies for this target demographic.<br/><br/>To overcome this barrier, state-of-the-art techniques must be created to facilitate the interaction necessary for robots to be useful for therapeutic rehabilitation with respect to children. Based on the fact that logically, animate playthings naturally engage children, the goal of this project is to fuse play and rehabilitation techniques using a robotic design to induce childrobot interaction that will be entertaining as well as effective for pediatric rehabilitation. Of importance within this proposed work are approaches that allow therapists to provide instruction to robots on rehabilitation tasks that can be remapped to play behaviors specific to the individual child. In addition, robots must have internal perception and inference algorithms that allow them to learn new play behaviors and incorporate them to evoke corresponding behaviors in the child.<br/><br/>Major research questions are (1) How can child play behavior most effectively be assessed and shared with an assistive robot, (2) How can this knowledge be captured and generalized into behaviors useful for rehabilitation and (3) What are the most effective robot interfaces for communicating these behaviors both to the therapist for evaluation and the child for directing movement.<br/><br/>Intellectual Merit: The objective of this research effort is to further rehabilitation techniques for children by developing and validating the core technologies needed to integrate therapy instruction with child-robot play interaction in order to improve pediatric rehabilitation. A principal goal of the research endeavor is to examine how evaluation of upper and/or lower limb body movement can be achieved through robot observation and how different rehabilitation exercises can be recoded to allow the mapping of therapist instruction to play behaviors. The focus is on using human-centered capabilities to enable robots to assist in rehabilitation exercises, an ability that is increasingly needed, especially given the desire to have daily therapeutic activities performed safely and consistently in almost any home environment.<br/><br/>Broader Impact: The successful development of a framework that builds upon proven human-centered observation techniques has the ability to tremendously increase the capabilities of robots that interact with children in a safe and effective manner. The significance of the approach will be emphasized in terms of providing assistance for children with cerebral palsy, but the results of this effort could lay the basis for similar efforts for children with varying disabilities. The PIs plan to incorporate these approaches into courses designed for robotics and software engineering. In addition, the education plan will incorporate many of these ideas into a ""boot-camp"" workshop for underrepresented students. Undergraduate research projects and demonstrations to middle-school students are anticipated to inspire and encourage the next generation of engineers and scientists and acclimate them into a new robot-integrated world."
"1208186","NRI-Small: Collaborative Research: Assistive Robotics for Grasping and Manipulation using Novel Brain Computer Interfaces","IIS","National Robotics Initiative","10/01/2012","09/06/2012","Sanjay Joshi","CA","University of California-Davis","Standard Grant","Alexander Leonessa","09/30/2017","$430,000.00","","maejoshi@ucdavis.edu","OR/Sponsored Programs","Davis","CA","956186134","5307547700","CSE","8013","7923, 8086","$0.00","This is a collaborative proposal (with UC Davis) which is aimed at making concrete some of the major goals of Assistive Robotics. A team of experts has been brought together from the fields of signal processing and control, robotic grasping, and rehabilitative medicine to create a field-deployable assistive robotic system that will allow severely disabled patients to control a robot arm/hand system to perform complex grasping and manipulation tasks using novel Brain Muscle Computer Interfaces (BMCI). Further, the intent of this effort is not just technology-driven, but is also driven by clear and necessary clinical needs, and will be evaluated on how well it meets these clinical requirements. Validation will be performed at the Department of Regenerative and Rehabilitation Medicine at Columbia University on a diverse set of disabled users who will provide important feedback on the technology being developed, and this feedback will be used to iterate on the system design and implementation.<br/><br/>Intellectual Merit: The intellectual merit of this proposal includes:<br/>o Novel research in Human Machine Interfaces that has the potential to be transformative in eliciting rich, multi-degree-of-freedom signal content from simple and non-invasive surface electromyographic (sEMG) sensors.<br/>o Development of smart adaptive software that employs machine learning algorithms that can continually monitor user performance, and then automatically calibrate and tune system parameters based on system performance.<br/>o Data driven methods for real-time grasp planning algorithms that can be used with both known and unknown objects.<br/>o Methods for finding pose-robust grasps that are tolerant of errors in sensing.<br/>o Evaluation of an underactuated hand as a grasping device for certain application tasks.<br/>o Integration of 3D vision with real-time grasp planning.<br/>o Scientific evaluation at the clinical level of the impact of these new technologies on the disabled population.<br/><br/>Broader Impacts: The broader impacts of this proposal include:<br/>o Development of a complete system to aid the severely disabled population with tetraplegia.<br/>o Extensions of this technlogy to others lacking motor control function including multiple sclerosis, stroke, amyotrophic lateral sclerosis (ALS or Lou Gehrig disease), cerebral palsy, and muscular dystrophy.<br/>o New technology that can extend the reach and impact of the field of Assistive Robotics.<br/>o Major extensions to the open-source GraspIt! software system that will allow many other researchers to leverage the results of this project.<br/>o Educational thrusts that will bring together engineering students, clinicians and the disabled population to extend the reach and scope of Assistive Robotics.<br/>o New directions in Human Machine Interfaces that can extend beyond the disabled population and into a variety of other applications."
"1208509","NRI-Small: Core Technologies for MRI-powered Robots","IIS","National Robotics Initiative","09/01/2012","08/31/2012","Pierre Dupont","MA","Children's Hospital Corporation","Standard Grant","Jeffrey Trinkle","08/31/2015","$800,000.00","","pierre.dupont@childrens.harvard.edu","300 LONGWOOD AVENUE","Boston","MA","021155737","6179192729","CSE","8013","7923, 8086","$0.00","The goal of this project is to create a transformative robotic technology that utilizes Magnetic Resonance Imaging (MRI) systems to power, control and image robots under the guidance and control of a clinician. Specifically, the research effort will be organized around three tasks: (1) creation of design principles for MRI-powered actuators, (2) development of motion planning and control algorithms for MRI-powered robots, and (3) design of MRI pulse sequences for closed-loop motor control. This tether-less robot technology addresses the needs for small, low cost medical robots identified in the Roadmap for US Robotics and can be exploited for robots ranging in size from centimeters down to fractions of a millimeter. At the centimeter scale, it could be used for robots designed to crawl inside body cavities to perform interventions and also for robotic prosthetic implants. At the millimeter and sub-millimeter scale, groups of MRI-powered robots can swim inside fluid-filled regions of the body to perform targeted therapies, such as drug and cell delivery, or to assemble as a sensor network. Two testbeds at these different scales will be used to evaluate and demonstrate the technology.<br/><br/>This research addresses a largely unexplored frontier in medical robotics that could revolutionize the standard of care for many serious medical conditions currently associated with both high mortality rates and high societal costs. The location of the PI's lab inside a teaching hospital provides a unique environment to integrate the research and education of the engineering and medical disciplines. To promote an understanding of engineering and medicine along with the value of learning and research to low income and minority school students, the project team will partner with local educational organizations. Furthermore, the project technology will consist of algorithms and software that can be utilized by researchers and educators throughout the country to provide fundamentally new capabilities to existing multi-million dollar equipment. The ultra-minimally invasive medical robots developed using this technology can potentially provide substantial societal benefits in terms of reduced trauma, precise image-based control and lower cost."
"1208412","NRI-Small: Collaborative Research: A Design Methodology for Multi-fingered Robotic Hands with Second-order Kinematic Constraints","IIS","National Robotics Initiative","09/01/2012","05/23/2013","J. Michael McCarthy","CA","University of California-Irvine","Standard Grant","Reid Simmons","05/31/2017","$399,997.00","Nina Robson","jmmccart@uci.edu","5171 California Avenue, Ste 150","Irvine","CA","926173067","9498244768","CSE","8013","7923, 8086","$0.00","This project, developing a systematic methodology for the design of multi-fingered robotic hands and grasping devices for a desired kinematic task, represents a novel formalization of the kinematic synthesis of articulated systems as a tree structure. The kinematic task is to be defined as positions and higher motion derivatives of the fingers, with accelerations related to the contact geometry at the fingertips for grasping actions. This research team aims to develop multi-fingered grasping devices for human-robot and anthropomorphic tasks, however the method will be a general tool for the design of any kind of multiple-finger grasping device. <br/><br/>This research has a number of broader impacts affecting both the academic community and society at large. First, the project will directly result in a design tool for multi-fingered robotic hands to enable the automatic transformation from task specifications to design alternatives ? an important development in its own right. This design tool will increase the ability of industry to design high performance, cost-effective multi-fingered robotic hands and other end effectors. This directly impacts manufacturing by speeding the development of end-of-arm tooling, with secondary benefits to the cost and quality of the final product. This will assist the U.S. to maintain its leadership and encourage the creation of high-quality jobs. The proposed curriculum additions resulting from this project will produce competent engineers for industry with a greater ability of approaching and solving design problems."
"1208626","NRI-Small: Measuring Unconstrained Grasp Forces Using Fingernail Imaging","IIS","National Robotics Initiative","08/01/2012","07/18/2012","Stephen Mascaro","UT","University of Utah","Standard Grant","Jie Yang","07/31/2017","$917,999.00","John Hollerbach","smascaro@mech.utah.edu","75 S 2000 E","SALT LAKE CITY","UT","841128930","8015816903","CSE","8013","7923, 8086, 9150","$0.00","This project develops the technology for unconstrained measurement of human grasp forces. Measurement of multi-fingered grasp forces typically requires a human to grasp an object at predefined sensor locations or to wear instrumented gloves that impede haptic sensations. The objective of this project is to characterize the ability to estimate three-dimensional grasp forces at the fingertips by measuring the color change of the fingernail. This fingernail imaging technique allows the human subject to freely choose where to place the fingers on the object, allowing for completely unconstrained multi-finger grasping. A magnetic levitation device is used to apply a range of 3-D forces to the human fingertip while collecting images of the fingernail. Various image processing techniques are being explored to register the fingernail images to a standard template, and various mathematical models relating pixel intensity to force are being investigated to determine an optimal method. A robotic motion-tracking technique is being implemented to keep the fingers in view of the camera as the hand moves during grasping experiments. The fingernail imaging technique is first validated using constrained grasping experiments, and then applied to unconstrained grasping experiments.<br/><br/>This research enables a co-robot to detect the individual finger forces of a human partner using a technique that does not interfere with the human's haptic sense. A co-robot trained with the appropriate calibration data could recognize and emulate or adapt to a human partner's grasp forces, measured using only vision. Research efforts are being integrated into the Robotics education and outreach at the University of Utah."
"1227504","NRI-Large: Collaborative Research: Purposeful Prediction: Co-robot Interaction via Understanding Intent and Goals","IIS","National Robotics Initiative","10/01/2012","08/05/2014","Joshua Tenenbaum","MA","Massachusetts Institute of Technology","Continuing grant","Ephraim P. Glinert","09/30/2016","$520,000.00","","jbt@mit.edu","77 MASSACHUSETTS AVE","Cambridge","MA","021394301","6172531000","CSE","8013","7925, 8086","$0.00","In order for robots to collaborate with humans, they need to be able to accurately forecast human intent and action. People act with purpose: that is, they make sequences of decisions to achieve long-term objectives. For instance, in driving from home to a store, people carefully plan a sequence of roads that will get them there efficiently. In predicting a person's next decision, algorithms must be developed that reflect these purposeful actions. <br/><br/>Currently, robots are unable to anticipate human needs and goals, and this represents a fundamental barrier to their large-scale deployment in the home and workplace. The aim of this project is to develop a new science of purposeful prediction that can be applied to human-robot interaction across a wide variety of domains. The work draws on recent techniques based on Inverse Optimal Control and Inverse Equilibria Theory that enable statistically sound reasoning about observed deliberate behavior. These new methods provide the foundations of a theoretical framework that integrates traditional decision making techniques like optimal control, search and planning with probabilistic methods that reason about uncertainty and hidden information, particularly about goals, utility and intent. <br/><br/>Intellectual merit: The project will provide a general framework that allows robots to anticipate and adapt to the activities of their human co-workers based on perceptual cues. The investigators will develop the theory, a computational toolbox, and, in collaboration with industrial partners, prototype deployments of these new methods for the prediction of peoples' behavior in a diverse set of robotics domains from computer vision to motor control. The project is transformative in that it combines a novel theoretical/algorithmic framework with extensive support in terms of volume of data and validation infrastructure in the context of many applications. <br/><br/>Broader impacts: A revolution in personal robotics in both the home and workplace depends on the ability to forecast human activities and intents; small- and medium- scale manufacturing will make a leap forward through agile robotic systems intelligent enough to understand and assist their co-workers in flexible assembly tasks; and robust models of pedestrian and vehicular traffic flow will enable more effective driver warning systems and safer autonomous mobile robots. Purposeful prediction technology is an important step towards enabling such understanding of actions and intents in these arenas. The research work will involve the training and mentoring of undergraduate, masters and doctoral students as well as post-doctoral fellows in this emerging multi-disciplinary research area at the intersection of computer and cognitive sciences and robotics."
"1208463","NRI-Small: Collaborative Research: A Dynamic Bayesian Approach to Real-Time Estimation and Filtering in Grasp Acquisition and Other Contact Tasks","IIS","National Robotics Initiative","09/01/2012","08/27/2012","Siwei Lyu","NY","SUNY at Albany","Standard Grant","Ephraim P. Glinert","08/31/2015","$200,284.00","","lsw@cs.albany.edu","1400 WASHINGTON AVE","Albany","NY","122220100","5184374550","CSE","8013","7923, 8086","$0.00","Robots cannot currently grasp objects or perform other contact tasks in unstructured environments with speed or reliability. This project is developing techniques for accurate real-time perception in support of contact tasks. In the proposed method, sensor data tracks the continuous motions of manipulated objects, while models of the objects are simultaneously updated. Particle filtering, a kind of Monte-Carlo simulation, ensures consistency of this tracking and updating.<br/><br/>The strongest impact of this work will be in robotic grasping and manipulation. Because of the synthesis of modeling and probabilistic inference, further impacts can be expected, for example in real-time haptics for telepresence."
"1208540","NRI-Small: Managing Uncertainty in Human-Robot Cooperative Systems","IIS","National Robotics Initiative","10/01/2012","04/28/2016","Peter Kazanzides","MD","Johns Hopkins University","Standard Grant","Jeffrey Trinkle","09/30/2016","$1,015,800.00","Louis Whitcomb, Simon Leonard","pkaz@jhu.edu","3400 N CHARLES ST","Baltimore","MD","212182608","4105168668","CSE","8013","7923, 8086, 9251","$0.00","This project attempts to combine human strengths in reasoning with machine capabilities in information fusion, task planning, and simulation to manage uncertainty and achieve successful human-robot partnerships to perform complex tasks in uncertain environments that were previously considered impractical or infeasible. The approach consists of three objectives: the use of sensing and control to reduce model registration uncertainty; the definition, simulation and implementation of virtual fixtures to allow humans to intuitively constrain the task; and the development of bi-directional task planning and execution with uncertainty to allow humans and robots to request help from one another.<br/><br/>This research has a number of broader impacts affecting both the academic community and society at large. The work is expected to have significant appeal to those in the manufacturing and medical robotics sectors, as testbeds will impact these areas. The PIs will mentor hands-on research by undergraduate, graduate, and post-doc students and guide them in the dissemination of their research to the scientific community. The team will also provide engineering experiences for middle school girls in the Baltimore area through weekend programs on the Johns Hopkins University campus, including the ""Ready, Set, Design!"" program. Finally, the development will be made freely available as all software will be fully integrated with open-source ROS."
"1227536","NRI-Large: Collaborative Research: Multilateral Manipulation by Human-Robot Collaborative Systems","IIS","National Robotics Initiative","10/01/2012","08/02/2013","Pieter Abbeel","CA","University of California-Berkeley","Continuing grant","Jeffrey Trinkle","09/30/2016","$1,168,000.00","Ken Goldberg","pabbeel@cs.berkeley.edu","Sponsored Projects Office","BERKELEY","CA","947045940","5106428109","CSE","8013","7925, 8086","$0.00","This project addresses a large space of manipulation problems that are repetitive, injury-causing, or dangerous for humans to perform, yet are currently impossible to reliably achieve with purely autonomous robots. These problems generally require dexterity, complex perception, and complex physical interaction. Yet, many such problems can be reliably addressed with human/robot collaborative (HRC) systems, where one or more humans provide needed perception and adaptability, working with one or more robot systems that provide speed, precision, accuracy, and dexterity at an appropriate scale, combining these complementary capabilities.<br/><br/>The project focuses on multilateral manipulation, which arises when a human controls one or more robot manipulators in partnership with one or more additional controllers (humans or autonomous agents). Complex operations in surgery and manufacturing can benefit from the extra degrees of freedom provided by more than two hands, and training often depends on hands-on interaction between expert and apprentice. Example applications include surgical operations, which typically involve several physicians and assistants, and other medical tasks such as turning a patient in bed and wrapping a cast to constrain a hand. Multilateral manipulation also applies in manufacturing, for example for threading wires or cables, aligning gaskets to obtain a tight seal, and in many household situations, such as folding tablecloths, wrapping packages, and zipping overfilled suitcases so they will fit inside diabolically-designed overhead airline compartments. Multilateral manipulation often arises with deformable materials or multi-jointed objects with more than six degrees of freedom (DOF). The extra DOFs in materials introduce challenges such as computational complexity, but they also can accommodate minor inconsistencies through redundancy and provide system damping. This project advances the fundamental science of multilateral manipulation guided by specific applications from surgery and manufacturing.<br/><br/>Broader Impacts: Multilateral manipulation systems have the potential to improve healthcare, improve American competitiveness and product quality in manufacturing, and open the door to new service robot applications in the home. The project will be guided by an Advisory Board of experts from industry and medical practice. Project results will be disseminated through yearly conference workshops, open-source software tools integrated into common robotics software environments such as Robot Operating System (ROS), and the investigators' research and course webpages, to encourage integration of our approach into research projects and courses at many institutions. Outreach programs, public lab tours, and mentoring of minority students will broaden participation of underrepresented groups in engineering. These activities will encourage participation in STEM activities and provide student and postdoctoral researchers with mentoring experience."
"1208687","NRI-Small: Improved safety and reliability of robotic systems by faults/anomalies detection from uninterpreted signals of computation graphs","IIS","National Robotics Initiative","10/01/2012","09/30/2013","Andrea Censi","CA","California Institute of Technology","Standard Grant","Ralph Wachter","12/31/2013","$860,000.00","Richard Murray, Andrea Censi","censi@MIT.EDU","1200 E California Blvd","PASADENA","CA","911250600","6263956219","CSE","8013","7923, 8086","$0.00","One of the main challenges to designing robots that can operate around humans is to create systems that can guarantee safety and effectiveness, while being robust to the nuisances of unstructured environments, from hardware faults to software issues, erroneous calibration, and less predictable anomalies, such as tampering and sabotage. However, the fact that the streams of observations and commands possess coherence properties suggests that many of these disturbances could be detected and automatically mitigated with general methods that imply very low design efforts. Currently, robotic systems are developed as a set of components realizing a directed ""computation graph"". This project focuses on theoretical methods, applicable designs, and reference implementation of a faults/anomalies detection mechanism for low-level robotic sensorimotor signals. The system, without any prior information about the robot configuration, should learn a model of the robot and the environment by passive observations of the signals exposed in the computation graph, and, based on this model, instantiate faults/anomalies detection components in an augmented computation graph.<br/><br/>The project engages undergraduate and graduate students in advanced robotics design and development. It is expected the research results will have a significant impact on future robotic systems and machine learning."
"1208519","NRI-Small: Context-Driven Haptic Inquiry of Objects Based on Task Requirements for Artificial Grasp and Manipulation","CBET","National Robotics Initiative","08/15/2012","08/13/2012","Veronica Santos","AZ","Arizona State University","Standard Grant","Alexander Leonessa","11/30/2014","$651,499.00","","vjsantos@ucla.edu","ORSPA","TEMPE","AZ","852816011","4809655479","ENG","8013","7923, 8086","$0.00","PI: Santos, Veronica<br/>Proposal Number: 1208519<br/><br/>Intellectual Merit: Human-like dexterous manipulation is featured prominently as a grand challenge in the 2009 Roadmap for U.S. Robotics' report. Human dexterity relies heavily on tactile sensation and is influenced by proprioceptive and visual feedback. The proposed work aims to advance artificial manipulators by integrating a new class of multimodal tactile sensors with anthropomorphic artificial hands and developing generalizable routines for context-driven haptic inquiry of objects based on task requirements for artificial grasp and manipulation. A primary goal is the development of capabilities for a robot hand to efficiently learn about objects in its unstructured environment through touch, specifically for cases where computer vision would fail to provide critical information about the physical hand-object interactions. While computer vision provides preliminary information about an object and its environment, vision alone cannot provide all essential information necessary for successful physical hand-object interactions. This is especially true when digits are occluded by the grasped object, and when the hand-object interaction is completely out of view. Inspiration for the haptic inquiry framework will be drawn from a suite of human haptic exploration procedures. In contrast to haptic exploration, haptic inquiry will require that the order and time spent on each exploratory procedure depend on task goals. The order and type of questions to be asked haptically will be context-dependent and designed to yield high-level, task-directed information at a low cost of inquiry. The weight given to each mode of tactile sensing (force, vibration, temperature) will also be tuned according to the context of the task.<br/>This proposal aims to strengthen the robustness of co-robot systems by developing a framework for context-driven, task-directed haptic inquiry that integrates multi-digit tactile and proprioception data in a task-appropriate manner. The framework will be developed and deployed on an anthropomorphic robot hand outfitted with a new class of commercially-available multimodal tactile sensors. The work is transformative because it will enable co-robot systems to remain functional even in the absence of visual feedback, which is typically the primary form of feedback for robotic systems. The long-term research objective of this proposal is to reduce the cognitive burden on the user of an artificial manipulator. <br/><br/>Broader Impacts: The proposed translational research could enhance the functional capabilities of co-robot systems in which humans use artificial manipulators to work in unstructured, unsafe, or limited access environments (prosthetic, rehabilitative, assistive, space, underwater, military, rescue, surgery). The proposed work could benefit the human user of a co-robot system by empowering the robot with the ability to control low-level perception-action loops autonomously without burdening the human. The ROS operating system may be used to simulate and control an anthropomorphic robot hand outfitted with commercially-available tactile sensors using commercially-available actuators. Custom source code (C, MATLAB, ROS) and an open source haptic library for a commercially-available tactile sensor (suitable for data mining) will be made publicly available for the benefit and advancement of the robotics community."
"1208497","NRI-Small: Collaborative Research: Multiple Task Learning from Unstructured Demonstrations","IIS","National Robotics Initiative","10/01/2012","09/06/2012","Andrew Barto","MA","University of Massachusetts Amherst","Standard Grant","Reid Simmons","09/30/2016","$499,199.00","","barto@cs.umass.edu","Research Administration Building","AMHERST","MA","010039242","4135450698","CSE","8013","7923, 8086","$0.00","This project develops techniques for the efficient, incremental learning of complex robotic tasks by breaking unstructured demonstrations into reusable component skills. A Bayesian model segments task demonstrations into simpler components and recognizes instances of repeated skills across demonstrations. Established methods from control engineering and reinforcement learning are leveraged and extended to allow for skill improvement from practice, in addition to learning from demonstration. The project aims to unify existing research on each of these ideas into a principled, integrated approach that addresses all of these problems jointly, with the goal of creating a deployment-ready, open-source system that transforms the way experts and novices alike interact with robots.<br/><br/>A simple interface that allows end-users to intuitively program robots is a key step to getting robots out of the laboratory and into human-cooperative settings in the home and workplace. Although it is often possible for an expert to program a robot to perform complex tasks, this programming is often very time-consuming and requires a great deal of knowledge. In response to this, much recent research is focusing on robot learning-from-demonstration, where non-expert users can teach a robot how to perform a task by example. Unfortunately, much of this work is limited to the artificially-structured demonstration of a single task with a well-defined beginning and end. By contrast, human-cooperative robots will be required to efficiently and incrementally learn many different, but often related, tasks from complex, unstructured demonstrations that are easy for non-experts to produce."
"0917266","RI: Small: Scalable Roadmap-Based Methods for Simulating and Controlling Behaviors of Interacting Groups: from Robot Swarms to Crowd Control","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","09/01/2009","08/14/2014","Nancy Amato","TX","Texas A&M Engineering Experiment Station","Continuing grant","Reid Simmons","08/31/2015","$504,000.00","Lawrence Rauchwerger","amato@tamu.edu","TEES State Headquarters Bldg.","College Station","TX","778454645","9798477635","CSE","7495, 8013","7495, 7923, 9102, 9215, HPCC, 9251, 8013","$0.00","Simulating and controlling communities of characters that can <br/>interact with each other and their environment, and dynamically <br/>react to changes, is a challenging problem with many important <br/>applications ranging from homeland security (e.g., simulation <br/>of disaster scenarios and responses), to civil crowd control <br/>(e.g., planning exit strategies for sporting events), to <br/>education and training (e.g., providing immersive museum exhibits <br/>and training systems). While there are existing methods that <br/>attempt to address the simulation aspect, there is a lack of <br/>methods that support interaction of multiple types/groups of <br/>agents and little work has been done on the control or steering <br/>aspect. <br/><br/>This work aims to address these challenges by integrating <br/>roadmap-based planning with agent-based modeling. This hybrid <br/>approach enables the development of methodology for modeling group <br/>interactions which are also influenced by constraints imposed by <br/>the environment (e.g., wide or narrow corridors) and techniques, <br/>including interfaces that enable planning and experimentation, that <br/>can scale to large numbers of agents. The results of this work will <br/>be shared with the community via publications and open source <br/>software. An anticipated outcome of this research is a tool for <br/>simulation and control of large crowds at major events (e.g., <br/>sporting events, political rallies, emergency evacuations of a <br/>building, region, or city). This could allow emergency response <br/>planners to investigate the crowd response when officials are placed <br/>in particular positions, or architects to study how evacuation times <br/>are affected by widening or narrowing corridors."
"0916720","RI: Small: AquaSWARM: Small Wireless Autonomous Robots for Monitoring of Aquatic Environments","IIS","ROBUST INTELLIGENCE, National Robotics Initiative","09/01/2009","08/14/2013","Xiaobo Tan","MI","Michigan State University","Standard Grant","Gregory Chirikjian","08/31/2014","$433,999.00","Elena Litchman","xbtan@msu.edu","Office of Sponsored Programs","East Lansing","MI","488242600","5173555040","CSE","7495, 8013","7495, 7923, 9215, 9251, HPCC, 8086","$0.00","The goal of the AquaSWARM project is to design and develop small, energy-efficient, autonomous underwater robots as sensor-rich platforms for dynamic, long-duration monitoring of aquatic environments. A novel concept of gliding robotic fish is investigated, which merges the energy-efficient design of underwater glider with the high maneuverability of robotic fish. Gliding motion, enabled by pitch and buoyancy control, is exploited to realize dive/ascent and large-distance horizontal travel. Soft actuation materials-based flexible tail fins are used to achieve maneuvers with high hydrodynamic efficiency. The research is focused on understanding gliding design for small robotic fish, and addressing the energy efficiency issue from a systems perspective. Schools of such autonomous robots are deployed in lakes at the Michigan State University Kellogg Biological Station to detect harmful algal blooms (HABs) and validate models for HAB dynamics. <br/><br/>The project is expected to result in cost-effective, underwater robots that can perform uninterrupted, long-duration (several months), long-travel (hundreds of miles) operation in aquatic environments. This will provide a novel, viable, versatile, cyber-physical infrastructure for aquatic environmental monitoring, with applications ranging from understanding the impact of global warming, to environmental protection, drinking water reservoir safety, and seaport security. The project also offers an interdisciplinary training environment for graduate and undergraduate students, and provides outreach opportunities to inspire pre-college students and train highly qualified teachers. Robotic fish-based HAB detection will also be used as a tool to engage communities at local lakes and stimulate their interest in novel technology and environmental issues."
